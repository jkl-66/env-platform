#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡å¯¼å…¥å…¨çƒæç«¯æ°”å€™äº‹ä»¶æ•°æ®é›†
"""

import asyncio
import pandas as pd
import numpy as np
from pathlib import Path
import sys
import os
import shutil
from datetime import datetime
from typing import Dict, List, Tuple, Any
import warnings
import json
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# å°è¯•å¯¼å…¥é¡¹ç›®æ¨¡å—
try:
    from src.data_processing.data_storage import DataStorage
    from src.utils.logger import get_logger
    from src.utils.config import get_settings
    USE_PROJECT_MODULES = True
except ImportError as e:
    print(f"è­¦å‘Š: æ— æ³•å¯¼å…¥é¡¹ç›®æ¨¡å— ({e})ï¼Œå°†ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬")
    USE_PROJECT_MODULES = False
    
    # ç®€åŒ–çš„æ—¥å¿—è®°å½•å™¨
    import logging
    def get_logger(name):
        logger = logging.getLogger(name)
        logger.setLevel(logging.INFO)
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        return logger
    
    # ç®€åŒ–çš„è®¾ç½®
    class SimpleSettings:
        def __init__(self):
            self.database_url = "sqlite:///climate_data.db"
    
    def get_settings():
        return SimpleSettings()

logger = get_logger(__name__)
settings = get_settings()

class ClimateDataBatchImporter:
    """æ°”å€™æ•°æ®æ‰¹é‡å¯¼å…¥å™¨"""
    
    def __init__(self):
        if USE_PROJECT_MODULES:
            self.data_storage = DataStorage()
        else:
            self.data_storage = None
            # åˆ›å»ºè¾“å‡ºç›®å½•
            self.output_dir = Path('outputs') / 'climate_batch_import'
            self.output_dir.mkdir(parents=True, exist_ok=True)
            
            # åˆ›å»ºæœ¬åœ°æ•°æ®ç›®å½•
            self.local_data_dir = Path('data') / 'climate_netcdf'
            self.local_data_dir.mkdir(parents=True, exist_ok=True)
        
        self.import_results = {
            'total_datasets': 0,
            'successful_imports': 0,
            'failed_imports': 0,
            'skipped_imports': 0,
            'details': [],
            'start_time': datetime.now().isoformat(),
            'end_time': None
        }
    
    async def initialize(self):
        """åˆå§‹åŒ–æ•°æ®å­˜å‚¨"""
        if USE_PROJECT_MODULES and self.data_storage:
            await self.data_storage.initialize()
            logger.info("æ•°æ®å­˜å‚¨åˆå§‹åŒ–å®Œæˆ")
        else:
            logger.info("ä½¿ç”¨ç®€åŒ–æ¨¡å¼ï¼Œè·³è¿‡æ•°æ®å­˜å‚¨åˆå§‹åŒ–")
    
    def find_netcdf_files(self, dataset_path: str) -> List[str]:
        """æŸ¥æ‰¾æ•°æ®é›†ç›®å½•ä¸­çš„NetCDFæ–‡ä»¶"""
        logger.info(f"æœç´¢NetCDFæ–‡ä»¶: {dataset_path}")
        
        if not os.path.exists(dataset_path):
            logger.warning(f"æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {dataset_path}")
            return []
        
        if not os.path.isdir(dataset_path):
            logger.warning(f"è·¯å¾„ä¸æ˜¯ç›®å½•: {dataset_path}")
            return []
        
        netcdf_files = []
        try:
            for item in os.listdir(dataset_path):
                if item.endswith('.nc'):
                    file_path = os.path.join(dataset_path, item)
                    if os.path.isfile(file_path):
                        netcdf_files.append(file_path)
            
            logger.info(f"æ‰¾åˆ° {len(netcdf_files)} ä¸ªNetCDFæ–‡ä»¶")
            return netcdf_files
            
        except Exception as e:
            logger.error(f"æœç´¢NetCDFæ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return []
    
    def copy_file_to_local(self, source_path: str) -> str:
        """å¤åˆ¶æ–‡ä»¶åˆ°æœ¬åœ°ç›®å½•"""
        try:
            source_file = Path(source_path)
            local_file = self.local_data_dir / source_file.name
            
            # å¦‚æœæœ¬åœ°æ–‡ä»¶å·²å­˜åœ¨ä¸”å¤§å°ç›¸åŒï¼Œè·³è¿‡å¤åˆ¶
            if local_file.exists():
                if local_file.stat().st_size == source_file.stat().st_size:
                    logger.info(f"æœ¬åœ°æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡å¤åˆ¶: {local_file.name}")
                    return str(local_file)
            
            # å¤åˆ¶æ–‡ä»¶
            shutil.copy2(source_path, local_file)
            logger.info(f"æ–‡ä»¶å¤åˆ¶æˆåŠŸ: {local_file.name}")
            return str(local_file)
            
        except Exception as e:
            logger.error(f"æ–‡ä»¶å¤åˆ¶å¤±è´¥: {e}")
            return None
    
    def load_netcdf_file(self, file_path: str) -> pd.DataFrame:
        """åŠ è½½NetCDFæ–‡ä»¶"""
        logger.info(f"æ­£åœ¨åŠ è½½NetCDFæ–‡ä»¶: {os.path.basename(file_path)}")
        
        try:
            import xarray as xr
            
            # æ‰“å¼€NetCDFæ–‡ä»¶
            ds = xr.open_dataset(file_path)
            
            logger.info(f"NetCDFæ–‡ä»¶ä¿¡æ¯:")
            logger.info(f"  ç»´åº¦: {dict(ds.dims)}")
            logger.info(f"  å˜é‡: {list(ds.data_vars)}")
            logger.info(f"  åæ ‡: {list(ds.coords)}")
            
            # è½¬æ¢ä¸ºDataFrame
            df = ds.to_dataframe().reset_index()
            
            # ç§»é™¤NaNå€¼
            df = df.dropna()
            
            # æ·»åŠ æ–‡ä»¶ä¿¡æ¯
            df['source_file'] = os.path.basename(file_path)
            df['dataset_type'] = self._extract_dataset_type(file_path)
            df['year'] = self._extract_year(file_path)
            
            ds.close()
            
            logger.info(f"NetCDFæ–‡ä»¶åŠ è½½å®Œæˆ: {os.path.basename(file_path)}")
            logger.info(f"æ•°æ®å½¢çŠ¶: {df.shape}")
            
            return df
            
        except ImportError:
            logger.error("éœ€è¦å®‰è£…xarrayåº“æ¥å¤„ç†NetCDFæ–‡ä»¶")
            return pd.DataFrame()
        except Exception as e:
            logger.error(f"åŠ è½½NetCDFæ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return pd.DataFrame()
    
    def _extract_dataset_type(self, filename: str) -> str:
        """ä»æ–‡ä»¶åæå–æ•°æ®é›†ç±»å‹"""
        if 'hot-dry-windy' in filename:
            return 'hot-dry-windy'
        elif 'hot-dry' in filename:
            return 'hot-dry'
        elif 'hot-wet' in filename:
            return 'hot-wet'
        elif 'wet-windy' in filename:
            return 'wet-windy'
        else:
            return 'unknown'
    
    def _extract_year(self, filename: str) -> int:
        """ä»æ–‡ä»¶åæå–å¹´ä»½"""
        import re
        match = re.search(r'_(\d{4})_', filename)
        if match:
            return int(match.group(1))
        return None
    
    async def import_dataset(self, dataset_path: str, dataset_name: str) -> Dict[str, Any]:
        """å¯¼å…¥å•ä¸ªæ•°æ®é›†"""
        logger.info(f"å¼€å§‹å¯¼å…¥æ•°æ®é›†: {dataset_name}")
        
        result = {
            'dataset_name': dataset_name,
            'dataset_path': dataset_path,
            'status': 'failed',
            'files_processed': 0,
            'total_records': 0,
            'error_message': None,
            'record_ids': [],
            'processing_time': 0
        }
        
        start_time = datetime.now()
        
        try:
            # æŸ¥æ‰¾NetCDFæ–‡ä»¶
            netcdf_files = self.find_netcdf_files(dataset_path)
            
            if not netcdf_files:
                result['status'] = 'skipped'
                result['error_message'] = 'æœªæ‰¾åˆ°NetCDFæ–‡ä»¶'
                logger.warning(f"æ•°æ®é›† {dataset_name} ä¸­æœªæ‰¾åˆ°NetCDFæ–‡ä»¶")
                return result
            
            # é™åˆ¶å¤„ç†æ–‡ä»¶æ•°é‡ï¼ˆæµ‹è¯•æ—¶åªå¤„ç†å‰5ä¸ªæ–‡ä»¶ï¼‰
            max_files = 5
            if len(netcdf_files) > max_files:
                logger.info(f"æ•°æ®é›†åŒ…å« {len(netcdf_files)} ä¸ªæ–‡ä»¶ï¼Œé™åˆ¶å¤„ç†å‰ {max_files} ä¸ª")
                netcdf_files = netcdf_files[:max_files]
            
            all_data = []
            processed_files = 0
            
            for file_path in netcdf_files:
                try:
                    # å¤åˆ¶æ–‡ä»¶åˆ°æœ¬åœ°
                    local_file = self.copy_file_to_local(file_path)
                    if not local_file:
                        continue
                    
                    # åŠ è½½NetCDFæ–‡ä»¶
                    data = self.load_netcdf_file(local_file)
                    
                    if not data.empty:
                        all_data.append(data)
                        processed_files += 1
                        logger.info(f"æˆåŠŸå¤„ç†æ–‡ä»¶: {os.path.basename(file_path)} ({data.shape[0]} æ¡è®°å½•)")
                    else:
                        logger.warning(f"æ–‡ä»¶æ•°æ®ä¸ºç©º: {os.path.basename(file_path)}")
                        
                except Exception as e:
                    logger.error(f"å¤„ç†æ–‡ä»¶ {os.path.basename(file_path)} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                    continue
            
            if not all_data:
                result['status'] = 'skipped'
                result['error_message'] = 'æ‰€æœ‰æ–‡ä»¶å¤„ç†å¤±è´¥æˆ–æ•°æ®ä¸ºç©º'
                logger.warning(f"æ•°æ®é›† {dataset_name} ä¸­æ‰€æœ‰æ–‡ä»¶å¤„ç†å¤±è´¥")
                return result
            
            # åˆå¹¶æ‰€æœ‰æ•°æ®
            combined_data = pd.concat(all_data, ignore_index=True)
            logger.info(f"æ•°æ®é›† {dataset_name} åˆå¹¶å®Œæˆï¼Œæ€»è®°å½•æ•°: {combined_data.shape[0]}")
            
            # ä¿å­˜æ•°æ®
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            if USE_PROJECT_MODULES and self.data_storage:
                # ä½¿ç”¨å®Œæ•´çš„æ•°æ®å­˜å‚¨ç³»ç»Ÿ
                filename = f"climate_batch_{dataset_name}_{timestamp}"
                
                # ä¿å­˜ä¸ºparquetæ ¼å¼
                saved_file_path = self.data_storage.save_dataframe(
                    combined_data, 
                    filename, 
                    data_category="processed", 
                    format="parquet"
                )
                
                # åˆ›å»ºæ•°æ®è®°å½•
                record_id = await self.data_storage.save_data_record(
                    source=f"Global Climate NetCDF Batch - {dataset_name}",
                    data_type="extreme_events",
                    location="Global",
                    start_time=datetime(1951, 1, 1),
                    end_time=datetime(2022, 12, 31),
                    file_path=saved_file_path,
                    file_format="parquet",
                    file_size=os.path.getsize(saved_file_path),
                    variables=list(combined_data.columns),
                    data_metadata={
                        "description": f"å…¨çƒ0.5Â°æç«¯æ°”å€™äº‹ä»¶æ•°æ®é›† - {dataset_name}",
                        "resolution": "0.5åº¦",
                        "time_range": "1951-2022",
                        "data_type": "extreme_climate_events",
                        "event_type": dataset_name,
                        "files_processed": processed_files,
                        "total_files": len(netcdf_files),
                        "data_shape": list(combined_data.shape)
                    }
                )
                
                result['record_ids'].append(record_id)
                logger.info(f"æ•°æ®é›† {dataset_name} å·²å¯¼å…¥æ•°æ®åº“ï¼Œè®°å½•ID: {record_id}")
            else:
                # ç®€åŒ–æ¨¡å¼ï¼šç›´æ¥ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶
                filename = f"climate_batch_{dataset_name}_{timestamp}.parquet"
                saved_file_path = self.output_dir / filename
                
                combined_data.to_parquet(saved_file_path)
                
                # ä¿å­˜å…ƒæ•°æ®
                metadata = {
                    "source": f"Global Climate NetCDF Batch - {dataset_name}",
                    "data_type": "extreme_events",
                    "location": "Global",
                    "time_range": "1951-2022",
                    "file_path": str(saved_file_path),
                    "file_format": "parquet",
                    "file_size": os.path.getsize(saved_file_path),
                    "variables": list(combined_data.columns),
                    "description": f"å…¨çƒ0.5Â°æç«¯æ°”å€™äº‹ä»¶æ•°æ®é›† - {dataset_name}",
                    "resolution": "0.5åº¦",
                    "event_type": dataset_name,
                    "files_processed": processed_files,
                    "total_files": len(netcdf_files),
                    "data_shape": list(combined_data.shape)
                }
                
                metadata_path = self.output_dir / f"metadata_{dataset_name}_{timestamp}.json"
                with open(metadata_path, 'w', encoding='utf-8') as f:
                    json.dump(metadata, f, ensure_ascii=False, indent=2)
                
                logger.info(f"æ•°æ®é›† {dataset_name} å·²ä¿å­˜åˆ°æœ¬åœ°: {saved_file_path}")
                logger.info(f"å…ƒæ•°æ®å·²ä¿å­˜: {metadata_path}")
            
            result['status'] = 'success'
            result['files_processed'] = processed_files
            result['total_records'] = combined_data.shape[0]
            
        except Exception as e:
            result['error_message'] = str(e)
            logger.error(f"å¯¼å…¥æ•°æ®é›† {dataset_name} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        
        finally:
            end_time = datetime.now()
            result['processing_time'] = (end_time - start_time).total_seconds()
        
        return result
    
    async def import_all_datasets(self, dataset_paths: Dict[str, str]):
        """å¯¼å…¥æ‰€æœ‰æ•°æ®é›†"""
        logger.info(f"å¼€å§‹æ‰¹é‡å¯¼å…¥ {len(dataset_paths)} ä¸ªæ•°æ®é›†")
        
        self.import_results['total_datasets'] = len(dataset_paths)
        
        for dataset_name, dataset_path in dataset_paths.items():
            logger.info(f"\n{'='*60}")
            logger.info(f"å¤„ç†æ•°æ®é›†: {dataset_name}")
            
            result = await self.import_dataset(dataset_path, dataset_name)
            self.import_results['details'].append(result)
            
            if result['status'] == 'success':
                self.import_results['successful_imports'] += 1
                logger.info(f"âœ… æ•°æ®é›† {dataset_name} å¯¼å…¥æˆåŠŸ")
            elif result['status'] == 'skipped':
                self.import_results['skipped_imports'] += 1
                logger.warning(f"âš ï¸ æ•°æ®é›† {dataset_name} è¢«è·³è¿‡: {result['error_message']}")
            else:
                self.import_results['failed_imports'] += 1
                logger.error(f"âŒ æ•°æ®é›† {dataset_name} å¯¼å…¥å¤±è´¥: {result['error_message']}")
        
        self.import_results['end_time'] = datetime.now().isoformat()
        
        # ä¿å­˜å¯¼å…¥ç»“æœ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = self.output_dir / f"batch_import_results_{timestamp}.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(self.import_results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"\n{'='*80}")
        logger.info("æ‰¹é‡å¯¼å…¥ç»“æœæ‘˜è¦")
        logger.info(f"{'='*80}")
        logger.info(f"ğŸ“Š å¯¼å…¥ç»Ÿè®¡:")
        logger.info(f"  æ€»æ•°æ®é›†æ•°é‡: {self.import_results['total_datasets']}")
        logger.info(f"  æˆåŠŸå¯¼å…¥: {self.import_results['successful_imports']}")
        logger.info(f"  è·³è¿‡: {self.import_results['skipped_imports']}")
        logger.info(f"  å¤±è´¥: {self.import_results['failed_imports']}")
        logger.info(f"")
        logger.info(f"ğŸ“‹ è¯¦ç»†ç»“æœ:")
        for detail in self.import_results['details']:
            status_icon = "âœ…" if detail['status'] == 'success' else "âš ï¸" if detail['status'] == 'skipped' else "âŒ"
            logger.info(f"  {status_icon} {detail['dataset_name']}: {detail['status']} ({detail['files_processed']} æ–‡ä»¶, {detail['total_records']} è®°å½•)")
        logger.info(f"{'='*80}")
        logger.info(f"âœ… æ‰¹é‡å¯¼å…¥å®Œæˆ!")
        logger.info(f"ğŸ“‹ å¯¼å…¥ç»“æœå·²ä¿å­˜: {results_file}")

async def main():
    """ä¸»å‡½æ•°"""
    # æ•°æ®é›†è·¯å¾„
    dataset_paths = {
        "hot-dry": r"D:\ç”¨æˆ·\jin\æ¡Œé¢\å…¨çƒ0.5Â°é€å¹´å¤åˆæç«¯æ°”å€™äº‹ä»¶æ•°æ®é›†ï¼ˆ1951-2022å¹´ï¼‰-æ•°æ®å®ä½“\global_hot-dry_[tasmax_p_95_all_7_1]-[pr_p_5_+0_0_1]_1_1~12",
        "hot-dry-windy": r"D:\ç”¨æˆ·\jin\æ¡Œé¢\å…¨çƒ0.5Â°é€å¹´å¤åˆæç«¯æ°”å€™äº‹ä»¶æ•°æ®é›†ï¼ˆ1951-2022å¹´ï¼‰-æ•°æ®å®ä½“\global_hot-dry-windy_[tasmax_p_95_all_7_1]-[pr_p_5_+0_0_1]-[sfcwind_p_95_+0.5_0_1]_1_1~12",
        "hot-wet": r"D:\ç”¨æˆ·\jin\æ¡Œé¢\å…¨çƒ0.5Â°é€å¹´å¤åˆæç«¯æ°”å€™äº‹ä»¶æ•°æ®é›†ï¼ˆ1951-2022å¹´ï¼‰-æ•°æ®å®ä½“\global_hot-wet_[tasmax_p_95_all_7_1]-[pr_p_95_+1_0_1]_1_1~12",
        "wet-windy": r"D:\ç”¨æˆ·\jin\æ¡Œé¢\å…¨çƒ0.5Â°é€å¹´å¤åˆæç«¯æ°”å€™äº‹ä»¶æ•°æ®é›†ï¼ˆ1951-2022å¹´ï¼‰-æ•°æ®å®ä½“\global_wet-windy_[pr_p_95_+1_0_1]-[sfcwind_p_95_+0.5_0_1]_1_1~12"
    }
    
    print("ğŸš€ å¼€å§‹æ‰¹é‡å¯¼å…¥å…¨çƒæç«¯æ°”å€™äº‹ä»¶æ•°æ®é›†...")
    
    # åˆ›å»ºå¯¼å…¥å™¨å¹¶è¿è¡Œå¯¼å…¥
    importer = ClimateDataBatchImporter()
    
    try:
        await importer.initialize()
        await importer.import_all_datasets(dataset_paths)
        
    except Exception as e:
        print(f"âŒ æ‰¹é‡å¯¼å…¥å¤±è´¥: {e}")
        logger.error(f"æ‰¹é‡å¯¼å…¥å¤±è´¥: {e}", exc_info=True)
    finally:
        if USE_PROJECT_MODULES and importer.data_storage:
            await importer.data_storage.close()

if __name__ == "__main__":
    asyncio.run(main())