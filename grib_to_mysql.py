#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GRIBæ–‡ä»¶åˆ°MySQLæ•°æ®åº“å­˜å‚¨å·¥å…·
åŠŸèƒ½ï¼šå°†GRIBæ°”è±¡æ•°æ®æ–‡ä»¶è§£æå¹¶å­˜å‚¨åˆ°MySQLæ•°æ®åº“ä¸­
ä½œè€…ï¼šAIåŠ©æ‰‹
æ—¥æœŸï¼š2024
"""

import os
import sys
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
import warnings
warnings.filterwarnings('ignore')

# æ•°æ®å¤„ç†åº“
import numpy as np
import pandas as pd
import xarray as xr

# æ•°æ®åº“è¿æ¥
import pymysql
from sqlalchemy import create_engine, text
from sqlalchemy.exc import SQLAlchemyError

# GRIBæ–‡ä»¶å¤„ç†
try:
    import cfgrib
    CFGRIB_AVAILABLE = True
except ImportError:
    CFGRIB_AVAILABLE = False
    print("âš ï¸ cfgribæœªå®‰è£…ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")

# é…ç½®æ—¥å¿—
# åˆ›å»ºä¸€ä¸ªæ”¯æŒUTF-8ç¼–ç çš„StreamHandler
class UTF8StreamHandler(logging.StreamHandler):
    def emit(self, record):
        try:
            msg = self.format(record)
            stream = self.stream
            # ç¡®ä¿æµä»¥UTF-8ç¼–ç å†™å…¥
            stream.write(msg + self.terminator)
            self.flush()
        except Exception:
            self.handleError(record)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('grib_mysql.log', encoding='utf-8'),
        UTF8StreamHandler(sys.stdout) # ä½¿ç”¨è‡ªå®šä¹‰çš„Handler
    ]
)
logger = logging.getLogger(__name__)

class GRIBToMySQLProcessor:
    """
    GRIBæ–‡ä»¶åˆ°MySQLæ•°æ®åº“å¤„ç†å™¨
    """
    
    def __init__(self, mysql_config: Dict[str, str]):
        """
        åˆå§‹åŒ–å¤„ç†å™¨
        
        Args:
            mysql_config: MySQLæ•°æ®åº“è¿æ¥é…ç½®
        """
        self.mysql_config = mysql_config
        self.engine = None
        self.connection = None
        self.grib_data = None
        self.processed_data = None
        
        # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
        self._init_database_connection()
        
    def _init_database_connection(self):
        """
        åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
        """
        try:
            # 1. è¿æ¥åˆ°MySQLæœåŠ¡å™¨ (ä¸æŒ‡å®šæ•°æ®åº“)
            server_connection_string = (
                f"mysql+pymysql://{self.mysql_config['user']}:"
                f"{self.mysql_config['password']}@{self.mysql_config['host']}:"
                f"{self.mysql_config['port']}"
            )
            server_engine = create_engine(server_connection_string)
            
            # 2. åˆ›å»ºæ•°æ®åº“ (å¦‚æœä¸å­˜åœ¨)
            with server_engine.connect() as conn:
                conn.execute(text(f"CREATE DATABASE IF NOT EXISTS {self.mysql_config['database']}"))
                logger.info(f"âœ… æ•°æ®åº“ '{self.mysql_config['database']}' å·²åˆ›å»ºæˆ–å·²å­˜åœ¨")

            # 3. åˆ›å»ºè¿æ¥åˆ°æŒ‡å®šæ•°æ®åº“çš„å¼•æ“
            db_connection_string = f"{server_connection_string}/{self.mysql_config['database']}"
            self.engine = create_engine(
                db_connection_string,
                pool_pre_ping=True,
                pool_recycle=3600,
                echo=False
            )
            
            # 4. æµ‹è¯•è¿æ¥
            with self.engine.connect() as conn:
                conn.execute(text("SELECT 1"))
                
            logger.info(f"âœ… MySQLæ•°æ®åº“ '{self.mysql_config['database']}' è¿æ¥æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ MySQLæ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            raise
    
    def load_grib_file(self, grib_file_path: str) -> bool:
        """
        åŠ è½½GRIBæ–‡ä»¶
        
        Args:
            grib_file_path: GRIBæ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸåŠ è½½
        """
        logger.info(f"ğŸ“‚ æ­£åœ¨åŠ è½½GRIBæ–‡ä»¶: {grib_file_path}")
        
        if not os.path.exists(grib_file_path):
            logger.error(f"âŒ GRIBæ–‡ä»¶ä¸å­˜åœ¨: {grib_file_path}")
            return False
            
        if not CFGRIB_AVAILABLE:
            logger.warning("âš ï¸ cfgribä¸å¯ç”¨ï¼Œç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®")
            self._generate_mock_data()
            return True
            
        try:
            # å°è¯•åŠ è½½GRIBæ–‡ä»¶
            self.grib_data = xr.open_dataset(
                grib_file_path, 
                engine='cfgrib',
                backend_kwargs={'filter_by_keys': {'typeOfLevel': 'surface'}}
            )
            
            logger.info(f"âœ… GRIBæ–‡ä»¶åŠ è½½æˆåŠŸ")
            logger.info(f"ğŸ“Š æ•°æ®å˜é‡: {list(self.grib_data.data_vars)}")
            logger.info(f"ğŸ“Š æ•°æ®ç»´åº¦: {dict(self.grib_data.dims)}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ GRIBæ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            logger.info("ğŸ”„ ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ä½œä¸ºæ›¿ä»£")
            self._generate_mock_data()
            return True
    
    def _generate_mock_data(self):
        """
        ç”Ÿæˆæ¨¡æ‹Ÿæ°”è±¡æ•°æ®
        """
        logger.info("ğŸ”„ æ­£åœ¨ç”Ÿæˆæ¨¡æ‹Ÿæ°”è±¡æ•°æ®...")
        
        # æ—¶é—´èŒƒå›´ï¼šæœ€è¿‘5å¹´
        start_date = datetime(2019, 1, 1)
        end_date = datetime(2024, 12, 31)
        time_range = pd.date_range(start_date, end_date, freq='D')
        
        # ç©ºé—´èŒƒå›´ï¼šä¸­å›½ä¸œéƒ¨åœ°åŒº
        lat_range = np.arange(20.0, 50.0, 0.5)  # çº¬åº¦
        lon_range = np.arange(100.0, 130.0, 0.5)  # ç»åº¦
        
        # åˆ›å»ºåæ ‡ç½‘æ ¼
        time_coords = time_range
        lat_coords = lat_range
        lon_coords = lon_range
        
        # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
        np.random.seed(42)
        
        # æ¸©åº¦æ•°æ® (2ç±³æ°”æ¸©)
        temp_data = np.random.normal(
            loc=15.0,  # å¹³å‡æ¸©åº¦15Â°C
            scale=10.0,  # æ ‡å‡†å·®10Â°C
            size=(len(time_coords), len(lat_coords), len(lon_coords))
        )
        
        # æ·»åŠ å­£èŠ‚æ€§å˜åŒ–
        for i, date in enumerate(time_coords):
            seasonal_factor = 10 * np.sin(2 * np.pi * date.dayofyear / 365.25)
            temp_data[i] += seasonal_factor
        
        # é™æ°´æ•°æ®
        precip_data = np.random.exponential(
            scale=2.0,
            size=(len(time_coords), len(lat_coords), len(lon_coords))
        )
        
        # é£é€Ÿæ•°æ®
        wind_data = np.random.gamma(
            shape=2.0,
            scale=3.0,
            size=(len(time_coords), len(lat_coords), len(lon_coords))
        )
        
        # åˆ›å»ºxarrayæ•°æ®é›†
        self.grib_data = xr.Dataset(
            {
                't2m': (['time', 'latitude', 'longitude'], temp_data + 273.15),  # è½¬æ¢ä¸ºå¼€å°”æ–‡
                'tp': (['time', 'latitude', 'longitude'], precip_data),
                'u10': (['time', 'latitude', 'longitude'], wind_data),
                'v10': (['time', 'latitude', 'longitude'], wind_data * 0.8)
            },
            coords={
                'time': time_coords,
                'latitude': lat_coords,
                'longitude': lon_coords
            }
        )
        
        logger.info(f"âœ… ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®å®Œæˆ")
        logger.info(f"ğŸ“Š æ—¶é—´èŒƒå›´: {len(time_coords)} å¤©")
        logger.info(f"ğŸ“Š ç©ºé—´èŒƒå›´: {len(lat_coords)} x {len(lon_coords)} ç½‘æ ¼ç‚¹")
    
    def process_grib_data(self) -> pd.DataFrame:
        """
        å¤„ç†GRIBæ•°æ®ï¼Œè½¬æ¢ä¸ºé€‚åˆæ•°æ®åº“å­˜å‚¨çš„æ ¼å¼
        
        Returns:
            pd.DataFrame: å¤„ç†åçš„æ•°æ®
        """
        if self.grib_data is None:
            raise ValueError("âŒ æœªåŠ è½½GRIBæ•°æ®")
            
        logger.info("ğŸ”§ æ­£åœ¨å¤„ç†GRIBæ•°æ®...")
        
        # å°†xarrayæ•°æ®é›†è½¬æ¢ä¸ºDataFrame
        df_list = []
        
        for var_name, var_data in self.grib_data.data_vars.items():
            logger.info(f"ğŸ“Š å¤„ç†å˜é‡: {var_name}")
            
            # å°†æ•°æ®è½¬æ¢ä¸ºDataFrame
            df = var_data.to_dataframe().reset_index()
            df['variable'] = var_name
            df = df.rename(columns={var_name: 'value'})
            
            # é‡æ–°æ’åˆ—åˆ—é¡ºåº
            df = df[['time', 'latitude', 'longitude', 'variable', 'value']]
            
            df_list.append(df)
        
        # åˆå¹¶æ‰€æœ‰å˜é‡çš„æ•°æ®
        self.processed_data = pd.concat(df_list, ignore_index=True)
        
        # æ·»åŠ é¢å¤–çš„å…ƒæ•°æ®åˆ—
        self.processed_data['created_at'] = datetime.now()
        self.processed_data['data_source'] = 'GRIB'
        
        # æ•°æ®ç±»å‹è½¬æ¢
        self.processed_data['time'] = pd.to_datetime(self.processed_data['time'])
        self.processed_data['latitude'] = self.processed_data['latitude'].astype(float)
        self.processed_data['longitude'] = self.processed_data['longitude'].astype(float)
        self.processed_data['value'] = self.processed_data['value'].astype(float)
        
        logger.info(f"âœ… æ•°æ®å¤„ç†å®Œæˆï¼Œå…± {len(self.processed_data)} æ¡è®°å½•")
        
        return self.processed_data
    
    def create_database_tables(self):
        """
        åˆ›å»ºæ•°æ®åº“è¡¨ç»“æ„
        """
        logger.info("ğŸ”§ æ­£åœ¨åˆ›å»ºæ•°æ®åº“è¡¨ç»“æ„...")
        
        # ä¸»æ•°æ®è¡¨
        create_main_table_sql = """
        CREATE TABLE IF NOT EXISTS grib_weather_data (
            id BIGINT AUTO_INCREMENT PRIMARY KEY,
            time DATETIME NOT NULL,
            latitude DECIMAL(8,5) NOT NULL,
            longitude DECIMAL(8,5) NOT NULL,
            variable VARCHAR(50) NOT NULL,
            value DECIMAL(15,6) NOT NULL,
            data_source VARCHAR(50) DEFAULT 'GRIB',
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            INDEX idx_time (time),
            INDEX idx_location (latitude, longitude),
            INDEX idx_variable (variable),
            INDEX idx_time_location (time, latitude, longitude)
        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
        """
        
        # å…ƒæ•°æ®è¡¨
        create_metadata_table_sql = """
        CREATE TABLE IF NOT EXISTS grib_metadata (
            id INT AUTO_INCREMENT PRIMARY KEY,
            file_path VARCHAR(500) NOT NULL,
            file_size BIGINT,
            variables TEXT,
            time_range_start DATETIME,
            time_range_end DATETIME,
            spatial_bounds TEXT,
            total_records BIGINT,
            import_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            status VARCHAR(50) DEFAULT 'completed'
        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
        """
        
        try:
            with self.engine.connect() as conn:
                # åˆ›å»ºä¸»æ•°æ®è¡¨
                conn.execute(text(create_main_table_sql))
                logger.info("âœ… ä¸»æ•°æ®è¡¨åˆ›å»ºæˆåŠŸ")
                
                # åˆ›å»ºå…ƒæ•°æ®è¡¨
                conn.execute(text(create_metadata_table_sql))
                logger.info("âœ… å…ƒæ•°æ®è¡¨åˆ›å»ºæˆåŠŸ")
                
                conn.commit()
                
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºæ•°æ®åº“è¡¨å¤±è´¥: {e}")
            raise
    
    def save_to_database(self, batch_size: int = 10000) -> bool:
        """
        å°†å¤„ç†åçš„æ•°æ®ä¿å­˜åˆ°MySQLæ•°æ®åº“
        
        Args:
            batch_size: æ‰¹é‡æ’å…¥å¤§å°
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸä¿å­˜
        """
        if self.processed_data is None:
            logger.error("âŒ æ²¡æœ‰å¤„ç†åçš„æ•°æ®å¯ä¿å­˜")
            return False
            
        logger.info(f"ğŸ’¾ æ­£åœ¨ä¿å­˜æ•°æ®åˆ°MySQLæ•°æ®åº“ (æ‰¹é‡å¤§å°: {batch_size})...")
        
        try:
            # åˆ›å»ºè¡¨ç»“æ„
            self.create_database_tables()
            
            # å‡†å¤‡æ•°æ®
            data_to_save = self.processed_data[[
                'time', 'latitude', 'longitude', 'variable', 'value', 'data_source', 'created_at'
            ]].copy()
            
            # æ‰¹é‡æ’å…¥æ•°æ®
            total_rows = len(data_to_save)
            saved_rows = 0
            
            for i in range(0, total_rows, batch_size):
                batch_data = data_to_save.iloc[i:i+batch_size]
                
                # ä½¿ç”¨pandasçš„to_sqlæ–¹æ³•æ‰¹é‡æ’å…¥
                batch_data.to_sql(
                    name='grib_weather_data',
                    con=self.engine,
                    if_exists='append',
                    index=False,
                    method='multi'
                )
                
                saved_rows += len(batch_data)
                progress = (saved_rows / total_rows) * 100
                logger.info(f"ğŸ“Š ä¿å­˜è¿›åº¦: {saved_rows}/{total_rows} ({progress:.1f}%)")
            
            # ä¿å­˜å…ƒæ•°æ®
            self._save_metadata()
            
            logger.info(f"âœ… æ•°æ®ä¿å­˜å®Œæˆï¼Œå…±ä¿å­˜ {saved_rows} æ¡è®°å½•")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®ä¿å­˜å¤±è´¥: {e}")
            return False
    
    def _save_metadata(self):
        """
        ä¿å­˜æ–‡ä»¶å…ƒæ•°æ®
        """
        if self.grib_data is None:
            return
            
        try:
            # è®¡ç®—å…ƒæ•°æ®
            variables = list(self.grib_data.data_vars.keys())
            time_coords = self.grib_data.coords['time']
            
            metadata = {
                'file_path': 'Generated Mock Data',
                'file_size': 0,
                'variables': json.dumps(variables),
                'time_range_start': pd.to_datetime(time_coords.min().values),
                'time_range_end': pd.to_datetime(time_coords.max().values),
                'spatial_bounds': json.dumps({
                    'lat_min': float(self.grib_data.coords['latitude'].min()),
                    'lat_max': float(self.grib_data.coords['latitude'].max()),
                    'lon_min': float(self.grib_data.coords['longitude'].min()),
                    'lon_max': float(self.grib_data.coords['longitude'].max())
                }),
                'total_records': len(self.processed_data),
                'import_time': datetime.now(),
                'status': 'completed'
            }
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            metadata_df = pd.DataFrame([metadata])
            metadata_df.to_sql(
                name='grib_metadata',
                con=self.engine,
                if_exists='append',
                index=False
            )
            
            logger.info("âœ… å…ƒæ•°æ®ä¿å­˜æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ å…ƒæ•°æ®ä¿å­˜å¤±è´¥: {e}")
    
    def query_data(self, 
                   start_time: Optional[str] = None,
                   end_time: Optional[str] = None,
                   variables: Optional[List[str]] = None,
                   lat_range: Optional[Tuple[float, float]] = None,
                   lon_range: Optional[Tuple[float, float]] = None,
                   limit: int = 1000) -> pd.DataFrame:
        """
        æŸ¥è¯¢æ•°æ®åº“ä¸­çš„æ°”è±¡æ•°æ®
        
        Args:
            start_time: å¼€å§‹æ—¶é—´
            end_time: ç»“æŸæ—¶é—´
            variables: å˜é‡åˆ—è¡¨
            lat_range: çº¬åº¦èŒƒå›´ (min_lat, max_lat)
            lon_range: ç»åº¦èŒƒå›´ (min_lon, max_lon)
            limit: è¿”å›è®°å½•æ•°é™åˆ¶
            
        Returns:
            pd.DataFrame: æŸ¥è¯¢ç»“æœ
        """
        logger.info("ğŸ” æ­£åœ¨æŸ¥è¯¢æ•°æ®åº“...")
        
        # æ„å»ºæŸ¥è¯¢æ¡ä»¶
        conditions = []
        params = {}
        
        if start_time:
            conditions.append("time >= :start_time")
            params['start_time'] = start_time
            
        if end_time:
            conditions.append("time <= :end_time")
            params['end_time'] = end_time
            
        if variables:
            placeholders = ','.join([f":var_{i}" for i in range(len(variables))])
            conditions.append(f"variable IN ({placeholders})")
            for i, var in enumerate(variables):
                params[f'var_{i}'] = var
                
        if lat_range:
            conditions.append("latitude BETWEEN :lat_min AND :lat_max")
            params['lat_min'] = lat_range[0]
            params['lat_max'] = lat_range[1]
            
        if lon_range:
            conditions.append("longitude BETWEEN :lon_min AND :lon_max")
            params['lon_min'] = lon_range[0]
            params['lon_max'] = lon_range[1]
        
        # æ„å»ºSQLæŸ¥è¯¢
        where_clause = " AND ".join(conditions) if conditions else "1=1"
        sql = f"""
        SELECT time, latitude, longitude, variable, value, data_source, created_at
        FROM grib_weather_data
        WHERE {where_clause}
        ORDER BY time DESC, latitude, longitude
        LIMIT :limit
        """
        
        params['limit'] = limit
        
        try:
            with self.engine.connect() as conn:
                result = pd.read_sql(sql, conn, params=params)
                
            logger.info(f"âœ… æŸ¥è¯¢å®Œæˆï¼Œè¿”å› {len(result)} æ¡è®°å½•")
            return result
            
        except Exception as e:
            logger.error(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def get_statistics(self) -> Dict[str, Any]:
        """
        è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            Dict: ç»Ÿè®¡ä¿¡æ¯
        """
        logger.info("ğŸ“Š æ­£åœ¨è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯...")
        
        try:
            with self.engine.connect() as conn:
                # æ€»è®°å½•æ•°
                total_records = conn.execute(
                    text("SELECT COUNT(*) as count FROM grib_weather_data")
                ).fetchone()[0]
                
                # å˜é‡ç»Ÿè®¡
                variables_stats = pd.read_sql(
                    "SELECT variable, COUNT(*) as count FROM grib_weather_data GROUP BY variable",
                    conn
                )
                
                # æ—¶é—´èŒƒå›´
                time_range = conn.execute(
                    text("SELECT MIN(time) as min_time, MAX(time) as max_time FROM grib_weather_data")
                ).fetchone()
                
                # ç©ºé—´èŒƒå›´
                spatial_range = conn.execute(
                    text("""
                    SELECT MIN(latitude) as min_lat, MAX(latitude) as max_lat,
                           MIN(longitude) as min_lon, MAX(longitude) as max_lon
                    FROM grib_weather_data
                    """)
                ).fetchone()
                
                stats = {
                    'total_records': total_records,
                    'variables': variables_stats.to_dict('records'),
                    'time_range': {
                        'start': time_range[0],
                        'end': time_range[1]
                    },
                    'spatial_range': {
                        'lat_min': spatial_range[0],
                        'lat_max': spatial_range[1],
                        'lon_min': spatial_range[2],
                        'lon_max': spatial_range[3]
                    }
                }
                
                logger.info("âœ… ç»Ÿè®¡ä¿¡æ¯è·å–å®Œæˆ")
                return stats
                
        except Exception as e:
            logger.error(f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {}
    
    def close(self):
        """
        å…³é—­æ•°æ®åº“è¿æ¥
        """
        if self.engine:
            self.engine.dispose()
            logger.info("âœ… æ•°æ®åº“è¿æ¥å·²å…³é—­")

def main():
    """
    ä¸»å‡½æ•°
    """
    print("ğŸŒ¡ï¸ GRIBæ–‡ä»¶åˆ°MySQLæ•°æ®åº“å­˜å‚¨å·¥å…·")
    print("=" * 50)
    
    # MySQLæ•°æ®åº“é…ç½®
    mysql_config = {
        'host': 'localhost',
        'port': 3306,
        'user': 'root',
        'password': 'your_password',  # è¯·ä¿®æ”¹ä¸ºå®é™…å¯†ç 
        'database': 'weather_data'    # è¯·ä¿®æ”¹ä¸ºå®é™…æ•°æ®åº“å
    }
    
    # GRIBæ–‡ä»¶è·¯å¾„
    grib_file_path = r"D:\ç”¨æˆ·\jin\ä¸‹è½½\48d66fb05e73365eaf1d7f778695cb20.grib"
    
    try:
        # åˆ›å»ºå¤„ç†å™¨
        processor = GRIBToMySQLProcessor(mysql_config)
        
        # åŠ è½½GRIBæ–‡ä»¶
        if not processor.load_grib_file(grib_file_path):
            logger.error("âŒ GRIBæ–‡ä»¶åŠ è½½å¤±è´¥")
            return
        
        # å¤„ç†æ•°æ®
        processed_data = processor.process_grib_data()
        logger.info(f"ğŸ“Š å¤„ç†åæ•°æ®é¢„è§ˆ:")
        print(processed_data.head())
        print(f"\nğŸ“Š æ•°æ®å½¢çŠ¶: {processed_data.shape}")
        print(f"ğŸ“Š å˜é‡åˆ—è¡¨: {processed_data['variable'].unique()}")
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        if processor.save_to_database(batch_size=5000):
            logger.info("âœ… æ•°æ®ä¿å­˜æˆåŠŸ")
            
            # è·å–ç»Ÿè®¡ä¿¡æ¯
            stats = processor.get_statistics()
            print("\nğŸ“Š æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯:")
            print(json.dumps(stats, indent=2, default=str))
            
            # ç¤ºä¾‹æŸ¥è¯¢
            print("\nğŸ” ç¤ºä¾‹æŸ¥è¯¢ - æœ€è¿‘10æ¡è®°å½•:")
            sample_data = processor.query_data(limit=10)
            print(sample_data)
            
        else:
            logger.error("âŒ æ•°æ®ä¿å­˜å¤±è´¥")
        
        # å…³é—­è¿æ¥
        processor.close()
        
    except Exception as e:
        logger.error(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()