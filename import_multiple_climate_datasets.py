#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šè·¯å¾„å…¨çƒæç«¯æ°”å€™äº‹ä»¶æ•°æ®é›†å¯¼å…¥è„šæœ¬

è¯¥è„šæœ¬ç”¨äºå¯¼å…¥å¤šä¸ªè·¯å¾„ä¸‹çš„å…¨çƒ0.5Â°é€å¹´å¤åˆæç«¯æ°”å€™äº‹ä»¶æ•°æ®é›†ï¼ˆ1951-2022å¹´ï¼‰ï¼Œ
åŒ…æ‹¬hot-dryã€hot-dry-windyã€hot-wetã€wet-windyç­‰ä¸åŒç±»å‹çš„æç«¯æ°”å€™äº‹ä»¶æ•°æ®ã€‚

ä½¿ç”¨æ–¹æ³•:
    python import_multiple_climate_datasets.py
"""

import asyncio
import pandas as pd
import numpy as np
from pathlib import Path
import sys
import os
from datetime import datetime
from typing import Dict, List, Tuple, Any
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# å°è¯•å¯¼å…¥é¡¹ç›®æ¨¡å—ï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
try:
    from src.data_processing.data_storage import DataStorage
    from src.utils.logger import get_logger
    from src.utils.config import get_settings
    USE_PROJECT_MODULES = True
except ImportError as e:
    print(f"è­¦å‘Š: æ— æ³•å¯¼å…¥é¡¹ç›®æ¨¡å— ({e})ï¼Œå°†ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬")
    USE_PROJECT_MODULES = False
    
    # ç®€åŒ–çš„æ—¥å¿—è®°å½•å™¨
    import logging
    def get_logger(name):
        logger = logging.getLogger(name)
        logger.setLevel(logging.INFO)
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        return logger
    
    # ç®€åŒ–çš„è®¾ç½®
    class SimpleSettings:
        def __init__(self):
            self.database_url = "sqlite:///climate_data.db"
    
    def get_settings():
        return SimpleSettings()

logger = get_logger(__name__)
settings = get_settings()

class MultipleClimateDatasetImporter:
    """å¤šè·¯å¾„å…¨çƒæç«¯æ°”å€™äº‹ä»¶æ•°æ®é›†å¯¼å…¥å™¨"""
    
    def __init__(self, dataset_paths: List[str]):
        self.dataset_paths = [Path(path) for path in dataset_paths]
        self.imported_records = []
        
        if USE_PROJECT_MODULES:
            self.data_storage = DataStorage()
        else:
            self.data_storage = None
            # åˆ›å»ºè¾“å‡ºç›®å½•
            self.output_dir = Path('outputs') / 'climate_data_import'
            self.output_dir.mkdir(parents=True, exist_ok=True)
        
    async def initialize(self):
        """åˆå§‹åŒ–æ•°æ®å­˜å‚¨"""
        if USE_PROJECT_MODULES and self.data_storage:
            await self.data_storage.initialize()
            logger.info("æ•°æ®å­˜å‚¨åˆå§‹åŒ–å®Œæˆ")
        else:
            logger.info("ä½¿ç”¨ç®€åŒ–æ¨¡å¼ï¼Œè·³è¿‡æ•°æ®å­˜å‚¨åˆå§‹åŒ–")
    
    def identify_dataset_type(self, path: Path) -> str:
        """æ ¹æ®è·¯å¾„è¯†åˆ«æ•°æ®é›†ç±»å‹"""
        path_str = str(path).lower()
        
        if 'hot-dry-windy' in path_str:
            return 'hot-dry-windy'
        elif 'hot-dry' in path_str:
            return 'hot-dry'
        elif 'hot-wet' in path_str:
            return 'hot-wet'
        elif 'wet-windy' in path_str:
            return 'wet-windy'
        else:
            return 'unknown'
    
    def load_dataset_from_path(self, dataset_path: Path) -> pd.DataFrame:
        """ä»æŒ‡å®šè·¯å¾„åŠ è½½æ•°æ®é›†"""
        logger.info(f"æ­£åœ¨åŠ è½½æ•°æ®é›†: {dataset_path}")
        
        if not dataset_path.exists():
            raise FileNotFoundError(f"æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {dataset_path}")
        
        # å¦‚æœæ˜¯ç›®å½•ï¼ŒæŸ¥æ‰¾æ•°æ®æ–‡ä»¶
        if dataset_path.is_dir():
            # æ”¯æŒå¤šç§æ–‡ä»¶æ ¼å¼
            extensions = ['*.csv', '*.xlsx', '*.xls', '*.nc', '*.txt', '*.dat', '*.json', '*.tif', '*.tiff', '*.hdf', '*.h5']
            data_files = []
            
            try:
                # åˆ—å‡ºç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶ä»¥ä¾¿è°ƒè¯•
                all_files = list(dataset_path.iterdir())
                logger.info(f"ç›®å½• {dataset_path.name} ä¸­åŒ…å« {len(all_files)} ä¸ªé¡¹ç›®")
                
                # æ˜¾ç¤ºå‰10ä¸ªæ–‡ä»¶/ç›®å½•çš„ä¿¡æ¯
                for i, item in enumerate(all_files[:10]):
                    if item.is_file():
                        logger.info(f"  æ–‡ä»¶: {item.name} (å¤§å°: {item.stat().st_size} å­—èŠ‚, æ‰©å±•å: {item.suffix})")
                    else:
                        logger.info(f"  ç›®å½•: {item.name}/")
                
                if len(all_files) > 10:
                    logger.info(f"  ... è¿˜æœ‰ {len(all_files) - 10} ä¸ªé¡¹ç›®")
                
                # æœç´¢æ”¯æŒçš„æ–‡ä»¶æ ¼å¼
                for ext in extensions:
                    found_files = list(dataset_path.glob(ext))
                    if found_files:
                        logger.info(f"æ‰¾åˆ° {len(found_files)} ä¸ª {ext} æ–‡ä»¶")
                        data_files.extend(found_files)
                    
                    # ä¹Ÿæœç´¢å­ç›®å½•
                    found_files_recursive = list(dataset_path.rglob(ext))
                    if found_files_recursive:
                        logger.info(f"åœ¨å­ç›®å½•ä¸­æ‰¾åˆ° {len(found_files_recursive)} ä¸ª {ext} æ–‡ä»¶")
                        data_files.extend(found_files_recursive)
                
                # å»é‡
                data_files = list(set(data_files))
                logger.info(f"æ€»å…±æ‰¾åˆ° {len(data_files)} ä¸ªæ”¯æŒçš„æ•°æ®æ–‡ä»¶")
                
            except Exception as e:
                logger.error(f"æœç´¢æ•°æ®æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                return pd.DataFrame()
            
            if not data_files:
                logger.warning(f"åœ¨ç›®å½• {dataset_path} ä¸­æœªæ‰¾åˆ°æ”¯æŒçš„æ•°æ®æ–‡ä»¶")
                logger.info(f"æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {', '.join(extensions)}")
                return pd.DataFrame()  # è¿”å›ç©ºDataFrame
            
            # å¦‚æœæœ‰å¤šä¸ªæ–‡ä»¶ï¼Œå°è¯•åˆå¹¶
            if len(data_files) == 1:
                file_path = data_files[0]
            else:
                logger.info(f"æ‰¾åˆ° {len(data_files)} ä¸ªæ•°æ®æ–‡ä»¶ï¼Œå°†å°è¯•åˆå¹¶")
                # åŠ è½½æ‰€æœ‰æ–‡ä»¶å¹¶åˆå¹¶
                dfs = []
                for file_path in data_files[:10]:  # é™åˆ¶æœ€å¤šå¤„ç†10ä¸ªæ–‡ä»¶
                    try:
                        df = self._load_single_file(file_path)
                        if not df.empty:
                            df['source_file'] = file_path.name
                            dfs.append(df)
                    except Exception as e:
                        logger.warning(f"æ— æ³•åŠ è½½æ–‡ä»¶ {file_path}: {e}")
                
                if dfs:
                    data = pd.concat(dfs, ignore_index=True)
                    logger.info(f"æˆåŠŸåˆå¹¶ {len(dfs)} ä¸ªæ–‡ä»¶ï¼Œæ€»æ•°æ®å½¢çŠ¶: {data.shape}")
                    return data
                else:
                    logger.warning(f"æ— æ³•ä»ç›®å½• {dataset_path} åŠ è½½ä»»ä½•æ•°æ®")
                    return pd.DataFrame()
        else:
            file_path = dataset_path
        
        # åŠ è½½å•ä¸ªæ–‡ä»¶
        return self._load_single_file(file_path)
    
    def _load_single_file(self, file_path: Path) -> pd.DataFrame:
        """åŠ è½½å•ä¸ªæ•°æ®æ–‡ä»¶"""
        try:
            # æ ¹æ®æ–‡ä»¶æ‰©å±•åé€‰æ‹©åŠ è½½æ–¹å¼
            if file_path.suffix.lower() == '.csv':
                data = pd.read_csv(file_path, encoding='utf-8')
            elif file_path.suffix.lower() in ['.xlsx', '.xls']:
                data = pd.read_excel(file_path)
            elif file_path.suffix.lower() == '.nc':
                try:
                    import xarray as xr
                    ds = xr.open_dataset(file_path)
                    data = ds.to_dataframe().reset_index()
                except ImportError:
                    logger.warning("éœ€è¦å®‰è£…xarrayåº“æ¥å¤„ç†NetCDFæ–‡ä»¶")
                    return pd.DataFrame()
            elif file_path.suffix.lower() in ['.tif', '.tiff']:
                try:
                    # å¤„ç†GeoTIFFæ–‡ä»¶
                    import rasterio
                    import rasterio.features
                    import rasterio.warp
                    
                    with rasterio.open(file_path) as src:
                        # è¯»å–æ•°æ®
                        data_array = src.read(1)  # è¯»å–ç¬¬ä¸€ä¸ªæ³¢æ®µ
                        
                        # è·å–åœ°ç†åæ ‡
                        height, width = data_array.shape
                        cols, rows = np.meshgrid(np.arange(width), np.arange(height))
                        
                        # è½¬æ¢åƒç´ åæ ‡åˆ°åœ°ç†åæ ‡
                        xs, ys = rasterio.transform.xy(src.transform, rows, cols)
                        
                        # åˆ›å»ºDataFrame
                        data = pd.DataFrame({
                            'longitude': np.array(xs).flatten(),
                            'latitude': np.array(ys).flatten(),
                            'value': data_array.flatten()
                        })
                        
                        # ç§»é™¤æ— æ•ˆå€¼
                        if hasattr(src, 'nodata') and src.nodata is not None:
                            data = data[data['value'] != src.nodata]
                        
                        # æ·»åŠ æ–‡ä»¶ä¿¡æ¯
                        data['file_name'] = file_path.stem
                        
                        logger.info(f"GeoTIFFæ–‡ä»¶åŠ è½½å®Œæˆ: {file_path.name}, å½¢çŠ¶: {data.shape}")
                        logger.info(f"åæ ‡èŒƒå›´: ç»åº¦ [{data['longitude'].min():.3f}, {data['longitude'].max():.3f}], çº¬åº¦ [{data['latitude'].min():.3f}, {data['latitude'].max():.3f}]")
                        logger.info(f"æ•°å€¼èŒƒå›´: [{data['value'].min():.3f}, {data['value'].max():.3f}]")
                        
                except ImportError:
                    logger.warning("éœ€è¦å®‰è£…rasterioåº“æ¥å¤„ç†GeoTIFFæ–‡ä»¶")
                    logger.info("è¯·è¿è¡Œ: pip install rasterio")
                    return pd.DataFrame()
            elif file_path.suffix.lower() in ['.hdf', '.h5']:
                try:
                    import h5py
                    # è¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“çš„HDF5æ–‡ä»¶ç»“æ„æ¥å¤„ç†
                    # æš‚æ—¶è¿”å›ç©ºDataFrameï¼Œéœ€è¦ç”¨æˆ·æä¾›å…·ä½“çš„æ•°æ®ç»“æ„ä¿¡æ¯
                    logger.warning(f"HDF5æ–‡ä»¶éœ€è¦ç‰¹å®šçš„å¤„ç†é€»è¾‘: {file_path.name}")
                    return pd.DataFrame()
                except ImportError:
                    logger.warning("éœ€è¦å®‰è£…h5pyåº“æ¥å¤„ç†HDF5æ–‡ä»¶")
                    return pd.DataFrame()
            elif file_path.suffix.lower() in ['.txt', '.dat']:
                # å°è¯•ä»¥åˆ¶è¡¨ç¬¦æˆ–é€—å·åˆ†éš”çš„æ–‡æœ¬æ–‡ä»¶
                try:
                    data = pd.read_csv(file_path, sep='\t', encoding='utf-8')
                except:
                    data = pd.read_csv(file_path, sep=',', encoding='utf-8')
            elif file_path.suffix.lower() == '.json':
                data = pd.read_json(file_path)
            else:
                logger.warning(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path.suffix}")
                return pd.DataFrame()
            
            logger.info(f"æ–‡ä»¶åŠ è½½å®Œæˆ: {file_path.name}, å½¢çŠ¶: {data.shape}")
            return data
            
        except Exception as e:
            logger.error(f"åŠ è½½æ–‡ä»¶ {file_path} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return pd.DataFrame()
    
    async def import_single_dataset(self, dataset_path: Path) -> str:
        """å¯¼å…¥å•ä¸ªæ•°æ®é›†åˆ°æ•°æ®åº“"""
        dataset_type = self.identify_dataset_type(dataset_path)
        logger.info(f"æ­£åœ¨å¯¼å…¥æ•°æ®é›†ç±»å‹: {dataset_type}")
        
        # åŠ è½½æ•°æ®
        data = self.load_dataset_from_path(dataset_path)
        
        if data.empty:
            logger.warning(f"æ•°æ®é›†ä¸ºç©ºï¼Œè·³è¿‡å¯¼å…¥: {dataset_path}")
            return None
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        if USE_PROJECT_MODULES and self.data_storage:
            # ä½¿ç”¨å®Œæ•´çš„æ•°æ®å­˜å‚¨ç³»ç»Ÿ
            filename = f"climate_extreme_events_{dataset_type}_{timestamp}"
            
            # ä¿å­˜ä¸ºparquetæ ¼å¼ä»¥æé«˜æ€§èƒ½
            file_path = self.data_storage.save_dataframe(
                data, 
                filename, 
                data_category="processed", 
                format="parquet"
            )
            
            # åˆ›å»ºæ•°æ®è®°å½•
            record_id = await self.data_storage.save_data_record(
                source=f"Global Climate Dataset - {dataset_type}",
                data_type="extreme_events",
                location="Global",
                start_time=datetime(1951, 1, 1),
                end_time=datetime(2022, 12, 31),
                file_path=file_path,
                file_format="parquet",
                file_size=os.path.getsize(file_path),
                variables=list(data.columns),
                data_metadata={
                    "description": f"å…¨çƒ0.5Â°é€å¹´å¤åˆæç«¯æ°”å€™äº‹ä»¶æ•°æ®é›†ï¼ˆ1951-2022å¹´ï¼‰- {dataset_type}",
                    "resolution": "0.5åº¦",
                    "temporal_coverage": "1951-2022",
                    "data_type": "extreme_climate_events",
                    "event_type": dataset_type,
                    "original_path": str(dataset_path)
                }
            )
            
            logger.info(f"æ•°æ®é›†å·²å¯¼å…¥æ•°æ®åº“ï¼Œè®°å½•ID: {record_id}")
            return record_id
        else:
            # ç®€åŒ–æ¨¡å¼ï¼šç›´æ¥ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶
            filename = f"climate_extreme_events_{dataset_type}_{timestamp}.parquet"
            file_path = self.output_dir / filename
            
            data.to_parquet(file_path)
            
            # ä¿å­˜å…ƒæ•°æ®
            metadata = {
                "source": f"Global Climate Dataset - {dataset_type}",
                "data_type": "extreme_events",
                "location": "Global",
                "start_time": "1951-01-01",
                "end_time": "2022-12-31",
                "file_path": str(file_path),
                "file_format": "parquet",
                "file_size": os.path.getsize(file_path),
                "variables": list(data.columns),
                "description": f"å…¨çƒ0.5Â°é€å¹´å¤åˆæç«¯æ°”å€™äº‹ä»¶æ•°æ®é›†ï¼ˆ1951-2022å¹´ï¼‰- {dataset_type}",
                "resolution": "0.5åº¦",
                "temporal_coverage": "1951-2022",
                "event_type": dataset_type,
                "original_path": str(dataset_path),
                "data_shape": data.shape
            }
            
            metadata_path = self.output_dir / f"metadata_{dataset_type}_{timestamp}.json"
            import json
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            logger.info(f"æ•°æ®é›†å·²ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶: {file_path}")
            logger.info(f"å…ƒæ•°æ®å·²ä¿å­˜: {metadata_path}")
            return f"local_file_{dataset_type}_{timestamp}"
    
    async def import_all_datasets(self) -> List[str]:
        """å¯¼å…¥æ‰€æœ‰æ•°æ®é›†"""
        logger.info(f"å¼€å§‹å¯¼å…¥ {len(self.dataset_paths)} ä¸ªæ•°æ®é›†...")
        
        imported_records = []
        
        for i, dataset_path in enumerate(self.dataset_paths, 1):
            logger.info(f"\nå¤„ç†æ•°æ®é›† {i}/{len(self.dataset_paths)}: {dataset_path}")
            
            try:
                record_id = await self.import_single_dataset(dataset_path)
                if record_id:
                    imported_records.append({
                        'path': str(dataset_path),
                        'type': self.identify_dataset_type(dataset_path),
                        'record_id': record_id,
                        'status': 'success'
                    })
                else:
                    imported_records.append({
                        'path': str(dataset_path),
                        'type': self.identify_dataset_type(dataset_path),
                        'record_id': None,
                        'status': 'skipped'
                    })
            except Exception as e:
                logger.error(f"å¯¼å…¥æ•°æ®é›†å¤±è´¥ {dataset_path}: {e}")
                imported_records.append({
                    'path': str(dataset_path),
                    'type': self.identify_dataset_type(dataset_path),
                    'record_id': None,
                    'status': 'failed',
                    'error': str(e)
                })
        
        self.imported_records = imported_records
        return imported_records
    
    def print_import_summary(self):
        """æ‰“å°å¯¼å…¥æ‘˜è¦"""
        print("\n" + "="*80)
        print("å¤šè·¯å¾„æ°”å€™æ•°æ®é›†å¯¼å…¥ç»“æœæ‘˜è¦")
        print("="*80)
        
        success_count = sum(1 for record in self.imported_records if record['status'] == 'success')
        skipped_count = sum(1 for record in self.imported_records if record['status'] == 'skipped')
        failed_count = sum(1 for record in self.imported_records if record['status'] == 'failed')
        
        print(f"\nğŸ“Š å¯¼å…¥ç»Ÿè®¡:")
        print(f"  æ€»æ•°æ®é›†æ•°é‡: {len(self.imported_records)}")
        print(f"  æˆåŠŸå¯¼å…¥: {success_count}")
        print(f"  è·³è¿‡: {skipped_count}")
        print(f"  å¤±è´¥: {failed_count}")
        
        print(f"\nğŸ“‹ è¯¦ç»†ç»“æœ:")
        for record in self.imported_records:
            status_icon = "âœ…" if record['status'] == 'success' else "âš ï¸" if record['status'] == 'skipped' else "âŒ"
            print(f"  {status_icon} {record['type']}: {record['status']}")
            if record['record_id']:
                print(f"      è®°å½•ID: {record['record_id']}")
            if record.get('error'):
                print(f"      é”™è¯¯: {record['error']}")
        
        print("="*80)
    
    async def run_import(self):
        """è¿è¡Œå®Œæ•´å¯¼å…¥æµç¨‹"""
        try:
            # åˆå§‹åŒ–
            await self.initialize()
            
            # å¯¼å…¥æ‰€æœ‰æ•°æ®é›†
            imported_records = await self.import_all_datasets()
            
            # æ‰“å°æ‘˜è¦
            self.print_import_summary()
            
            return imported_records
            
        except Exception as e:
            logger.error(f"å¯¼å…¥è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            raise
        finally:
            if USE_PROJECT_MODULES and self.data_storage:
                await self.data_storage.close()

async def main():
    """ä¸»å‡½æ•°"""
    # ç”¨æˆ·æŒ‡å®šçš„4ä¸ªæ•°æ®é›†è·¯å¾„
    dataset_paths = [
        r"D:\ç”¨æˆ·\jin\æ¡Œé¢\å…¨çƒ0.5Â°é€å¹´å¤åˆæç«¯æ°”å€™äº‹ä»¶æ•°æ®é›†ï¼ˆ1951-2022å¹´ï¼‰-æ•°æ®å®ä½“\global_hot-dry_[tasmax_p_95_all_7_1]-[pr_p_5_+0_0_1]_1_1~12",
        r"D:\ç”¨æˆ·\jin\æ¡Œé¢\å…¨çƒ0.5Â°é€å¹´å¤åˆæç«¯æ°”å€™äº‹ä»¶æ•°æ®é›†ï¼ˆ1951-2022å¹´ï¼‰-æ•°æ®å®ä½“\global_hot-dry-windy_[tasmax_p_95_all_7_1]-[pr_p_5_+0_0_1]-[sfcwind_p_95_+0.5_0_1]_1_1~12",
        r"D:\ç”¨æˆ·\jin\æ¡Œé¢\å…¨çƒ0.5Â°é€å¹´å¤åˆæç«¯æ°”å€™äº‹ä»¶æ•°æ®é›†ï¼ˆ1951-2022å¹´ï¼‰-æ•°æ®å®ä½“\global_hot-wet_[tasmax_p_95_all_7_1]-[pr_p_95_+1_0_1]_1_1~12",
        r"D:\ç”¨æˆ·\jin\æ¡Œé¢\å…¨çƒ0.5Â°é€å¹´å¤åˆæç«¯æ°”å€™äº‹ä»¶æ•°æ®é›†ï¼ˆ1951-2022å¹´ï¼‰-æ•°æ®å®ä½“\global_wet-windy_[pr_p_95_+1_0_1]-[sfcwind_p_95_+0.5_0_1]_1_1~12"
    ]
    
    print("ğŸš€ å¼€å§‹å¤šè·¯å¾„å…¨çƒæç«¯æ°”å€™äº‹ä»¶æ•°æ®é›†å¯¼å…¥...")
    print(f"ğŸ“‚ å°†å¤„ç† {len(dataset_paths)} ä¸ªæ•°æ®é›†è·¯å¾„")
    
    # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
    existing_paths = []
    for path in dataset_paths:
        if Path(path).exists():
            existing_paths.append(path)
            print(f"âœ… è·¯å¾„å­˜åœ¨: {Path(path).name}")
        else:
            print(f"âŒ è·¯å¾„ä¸å­˜åœ¨: {path}")
    
    if not existing_paths:
        print("\nâŒ é”™è¯¯: æ‰€æœ‰æŒ‡å®šçš„æ•°æ®é›†è·¯å¾„éƒ½ä¸å­˜åœ¨")
        print("è¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼Œæˆ–å°†æ•°æ®é›†æ–‡ä»¶æ”¾ç½®åœ¨æŒ‡å®šä½ç½®ã€‚")
        return
    
    if len(existing_paths) < len(dataset_paths):
        print(f"\nâš ï¸ è­¦å‘Š: åªæœ‰ {len(existing_paths)}/{len(dataset_paths)} ä¸ªè·¯å¾„å­˜åœ¨ï¼Œå°†åªå¤„ç†å­˜åœ¨çš„è·¯å¾„")
    
    # åˆ›å»ºå¯¼å…¥å™¨å¹¶è¿è¡Œå¯¼å…¥
    importer = MultipleClimateDatasetImporter(existing_paths)
    
    try:
        results = await importer.run_import()
        
        print("\nâœ… å¯¼å…¥å®Œæˆ!")
        
        # ä¿å­˜å¯¼å…¥ç»“æœ
        import json
        results_file = Path('outputs') / 'climate_data_import' / f'import_results_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
        results_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“‹ å¯¼å…¥ç»“æœå·²ä¿å­˜: {results_file}")
        
    except Exception as e:
        print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        logger.error(f"å¯¼å…¥å¤±è´¥: {e}", exc_info=True)

if __name__ == "__main__":
    asyncio.run(main())