#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å…¨çƒæç«¯æ°”å€™äº‹ä»¶æ•°æ®é›†å¯¼å…¥ä¸åˆ†æè„šæœ¬

è¯¥è„šæœ¬ç”¨äºå¯¼å…¥å…¨çƒ0.5Â°é€å¹´å¤åˆæç«¯æ°”å€™äº‹ä»¶æ•°æ®é›†ï¼ˆ1951-2022å¹´ï¼‰ï¼Œ
å¹¶ä½¿ç”¨æ°”è±¡ç ”ç©¶äººå‘˜çš„ä¼ ç»Ÿæ–¹æ³•å’ŒAIæ–¹æ³•åˆ†æå¬å›ç‡å’Œå‡†ç¡®ç‡ï¼Œæœ€åç”Ÿæˆå¯¹æ¯”å›¾è¡¨ã€‚

ä½¿ç”¨æ–¹æ³•:
    python import_climate_dataset.py
"""

import asyncio
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import sys
import os
from datetime import datetime
from typing import Dict, List, Tuple, Any
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# å°è¯•å¯¼å…¥é¡¹ç›®æ¨¡å—ï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
try:
    from src.data_processing.data_storage import DataStorage
    from src.ml.model_manager import ModelManager
    from src.utils.logger import get_logger
    from src.utils.config import get_settings
    USE_PROJECT_MODULES = True
except ImportError as e:
    print(f"è­¦å‘Š: æ— æ³•å¯¼å…¥é¡¹ç›®æ¨¡å— ({e})ï¼Œå°†ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬")
    USE_PROJECT_MODULES = False
    
    # ç®€åŒ–çš„æ—¥å¿—è®°å½•å™¨
    import logging
    def get_logger(name):
        logger = logging.getLogger(name)
        logger.setLevel(logging.INFO)
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        return logger
    
    # ç®€åŒ–çš„è®¾ç½®
    class SimpleSettings:
        def __init__(self):
            self.database_url = "sqlite:///climate_data.db"
    
    def get_settings():
        return SimpleSettings()

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False

logger = get_logger(__name__)
settings = get_settings()

class ClimateDatasetAnalyzer:
    """å…¨çƒæç«¯æ°”å€™äº‹ä»¶æ•°æ®é›†åˆ†æå™¨"""
    
    def __init__(self, dataset_path: str):
        self.dataset_path = Path(dataset_path)
        self.data = None
        self.results = {}
        
        if USE_PROJECT_MODULES:
            self.data_storage = DataStorage()
            self.model_manager = ModelManager()
        else:
            self.data_storage = None
            self.model_manager = None
            # åˆ›å»ºè¾“å‡ºç›®å½•
            self.output_dir = Path('outputs') / 'climate_analysis'
            self.output_dir.mkdir(parents=True, exist_ok=True)
        
    async def initialize(self):
        """åˆå§‹åŒ–æ•°æ®å­˜å‚¨"""
        if USE_PROJECT_MODULES and self.data_storage:
            await self.data_storage.initialize()
            logger.info("æ•°æ®å­˜å‚¨åˆå§‹åŒ–å®Œæˆ")
        else:
            logger.info("ä½¿ç”¨ç®€åŒ–æ¨¡å¼ï¼Œè·³è¿‡æ•°æ®å­˜å‚¨åˆå§‹åŒ–")
    
    def load_dataset(self) -> pd.DataFrame:
        """åŠ è½½æ•°æ®é›†"""
        logger.info(f"æ­£åœ¨åŠ è½½æ•°æ®é›†: {self.dataset_path}")
        
        if not self.dataset_path.exists():
            raise FileNotFoundError(f"æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {self.dataset_path}")
        
        # æ ¹æ®æ–‡ä»¶æ‰©å±•åé€‰æ‹©åŠ è½½æ–¹å¼
        if self.dataset_path.suffix.lower() == '.csv':
            self.data = pd.read_csv(self.dataset_path, encoding='utf-8')
        elif self.dataset_path.suffix.lower() in ['.xlsx', '.xls']:
            self.data = pd.read_excel(self.dataset_path)
        elif self.dataset_path.suffix.lower() == '.nc':
            import xarray as xr
            ds = xr.open_dataset(self.dataset_path)
            self.data = ds.to_dataframe().reset_index()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {self.dataset_path.suffix}")
        
        logger.info(f"æ•°æ®é›†åŠ è½½å®Œæˆï¼Œå½¢çŠ¶: {self.data.shape}")
        logger.info(f"åˆ—å: {list(self.data.columns)}")
        
        return self.data
    
    async def import_to_database(self) -> str:
        """å°†æ•°æ®é›†å¯¼å…¥æ•°æ®åº“"""
        logger.info("æ­£åœ¨å°†æ•°æ®é›†å¯¼å…¥æ•°æ®åº“...")
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        if USE_PROJECT_MODULES and self.data_storage:
            # ä½¿ç”¨å®Œæ•´çš„æ•°æ®å­˜å‚¨ç³»ç»Ÿ
            filename = f"climate_extreme_events_{timestamp}"
            
            # ä¿å­˜ä¸ºparquetæ ¼å¼ä»¥æé«˜æ€§èƒ½
            file_path = self.data_storage.save_dataframe(
                self.data, 
                filename, 
                data_category="processed", 
                format="parquet"
            )
            
            # åˆ›å»ºæ•°æ®è®°å½•
            record_id = await self.data_storage.save_data_record(
                source="Global Climate Dataset",
                data_type="extreme_events",
                location="Global",
                start_time=datetime(1951, 1, 1),
                end_time=datetime(2022, 12, 31),
                file_path=file_path,
                file_format="parquet",
                file_size=os.path.getsize(file_path),
                variables=list(self.data.columns),
                data_metadata={
                    "description": "å…¨çƒ0.5Â°é€å¹´å¤åˆæç«¯æ°”å€™äº‹ä»¶æ•°æ®é›†ï¼ˆ1951-2022å¹´ï¼‰",
                    "resolution": "0.5åº¦",
                    "temporal_coverage": "1951-2022",
                    "data_type": "extreme_climate_events"
                }
            )
            
            logger.info(f"æ•°æ®é›†å·²å¯¼å…¥æ•°æ®åº“ï¼Œè®°å½•ID: {record_id}")
            return record_id
        else:
            # ç®€åŒ–æ¨¡å¼ï¼šç›´æ¥ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶
            filename = f"climate_extreme_events_{timestamp}.parquet"
            file_path = self.output_dir / filename
            
            self.data.to_parquet(file_path)
            
            # ä¿å­˜å…ƒæ•°æ®
            metadata = {
                "source": "Global Climate Dataset",
                "data_type": "extreme_events",
                "location": "Global",
                "start_time": "1951-01-01",
                "end_time": "2022-12-31",
                "file_path": str(file_path),
                "file_format": "parquet",
                "file_size": os.path.getsize(file_path),
                "variables": list(self.data.columns),
                "description": "å…¨çƒ0.5Â°é€å¹´å¤åˆæç«¯æ°”å€™äº‹ä»¶æ•°æ®é›†ï¼ˆ1951-2022å¹´ï¼‰",
                "resolution": "0.5åº¦",
                "temporal_coverage": "1951-2022"
            }
            
            metadata_path = self.output_dir / f"metadata_{timestamp}.json"
            import json
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            logger.info(f"æ•°æ®é›†å·²ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶: {file_path}")
            logger.info(f"å…ƒæ•°æ®å·²ä¿å­˜: {metadata_path}")
            return f"local_file_{timestamp}"
    
    def preprocess_data(self) -> Tuple[pd.DataFrame, pd.Series]:
        """æ•°æ®é¢„å¤„ç†"""
        logger.info("æ­£åœ¨è¿›è¡Œæ•°æ®é¢„å¤„ç†...")
        
        # å‡è®¾æ•°æ®é›†åŒ…å«æç«¯äº‹ä»¶æ ‡è¯†åˆ—
        # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…æ•°æ®é›†ç»“æ„è°ƒæ•´
        if 'extreme_event' in self.data.columns:
            target_col = 'extreme_event'
        elif 'is_extreme' in self.data.columns:
            target_col = 'is_extreme'
        else:
            # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„æç«¯äº‹ä»¶æ ‡è¯†ï¼ŒåŸºäºé˜ˆå€¼åˆ›å»º
            # è¿™é‡Œä»¥æ¸©åº¦ä¸ºä¾‹ï¼Œå¯ä»¥æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
            temp_cols = [col for col in self.data.columns if 'temp' in col.lower() or 'temperature' in col.lower()]
            if temp_cols:
                temp_col = temp_cols[0]
                threshold = self.data[temp_col].quantile(0.95)  # 95åˆ†ä½æ•°ä½œä¸ºæç«¯äº‹ä»¶é˜ˆå€¼
                self.data['extreme_event'] = (self.data[temp_col] > threshold).astype(int)
                target_col = 'extreme_event'
            else:
                raise ValueError("æ— æ³•è¯†åˆ«æç«¯äº‹ä»¶æ ‡è¯†åˆ—ï¼Œè¯·æ£€æŸ¥æ•°æ®é›†ç»“æ„")
        
        # åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡å˜é‡
        feature_cols = [col for col in self.data.columns if col != target_col]
        X = self.data[feature_cols].select_dtypes(include=[np.number])
        y = self.data[target_col]
        
        # å¤„ç†ç¼ºå¤±å€¼
        X = X.fillna(X.mean())
        
        logger.info(f"é¢„å¤„ç†å®Œæˆï¼Œç‰¹å¾æ•°é‡: {X.shape[1]}, æ ·æœ¬æ•°é‡: {X.shape[0]}")
        logger.info(f"æç«¯äº‹ä»¶æ¯”ä¾‹: {y.mean():.3f}")
        
        return X, y
    
    def meteorological_method_analysis(self, X: pd.DataFrame, y: pd.Series) -> Dict[str, float]:
        """æ°”è±¡ç ”ç©¶äººå‘˜çš„ä¼ ç»Ÿæ–¹æ³•åˆ†æ"""
        logger.info("æ­£åœ¨ä½¿ç”¨æ°”è±¡ç ”ç©¶äººå‘˜çš„ä¼ ç»Ÿæ–¹æ³•è¿›è¡Œåˆ†æ...")
        
        # ä¼ ç»Ÿæ°”è±¡æ–¹æ³•ï¼šåŸºäºé˜ˆå€¼çš„ç®€å•åˆ†ç±»
        results = {}
        
        # æ–¹æ³•1ï¼šåŸºäºæ¸©åº¦é˜ˆå€¼
        temp_cols = [col for col in X.columns if 'temp' in col.lower()]
        if temp_cols:
            temp_col = temp_cols[0]
            temp_threshold = X[temp_col].quantile(0.9)
            temp_pred = (X[temp_col] > temp_threshold).astype(int)
            
            results['temp_threshold'] = {
                'accuracy': accuracy_score(y, temp_pred),
                'precision': precision_score(y, temp_pred, zero_division=0),
                'recall': recall_score(y, temp_pred, zero_division=0),
                'f1': f1_score(y, temp_pred, zero_division=0)
            }
        
        # æ–¹æ³•2ï¼šåŸºäºé™æ°´é˜ˆå€¼
        precip_cols = [col for col in X.columns if 'precip' in col.lower() or 'rain' in col.lower()]
        if precip_cols:
            precip_col = precip_cols[0]
            precip_threshold = X[precip_col].quantile(0.95)
            precip_pred = (X[precip_col] > precip_threshold).astype(int)
            
            results['precip_threshold'] = {
                'accuracy': accuracy_score(y, precip_pred),
                'precision': precision_score(y, precip_pred, zero_division=0),
                'recall': recall_score(y, precip_pred, zero_division=0),
                'f1': f1_score(y, precip_pred, zero_division=0)
            }
        
        # æ–¹æ³•3ï¼šç»¼åˆæŒ‡æ•°æ–¹æ³•
        # æ ‡å‡†åŒ–ç‰¹å¾å¹¶è®¡ç®—ç»¼åˆæŒ‡æ•°
        X_normalized = (X - X.mean()) / X.std()
        composite_index = X_normalized.mean(axis=1)
        composite_threshold = composite_index.quantile(0.9)
        composite_pred = (composite_index > composite_threshold).astype(int)
        
        results['composite_index'] = {
            'accuracy': accuracy_score(y, composite_pred),
            'precision': precision_score(y, composite_pred, zero_division=0),
            'recall': recall_score(y, composite_pred, zero_division=0),
            'f1': f1_score(y, composite_pred, zero_division=0)
        }
        
        logger.info("ä¼ ç»Ÿæ°”è±¡æ–¹æ³•åˆ†æå®Œæˆ")
        return results
    
    def ai_method_analysis(self, X: pd.DataFrame, y: pd.Series) -> Dict[str, float]:
        """AIæ–¹æ³•åˆ†æ"""
        logger.info("æ­£åœ¨ä½¿ç”¨AIæ–¹æ³•è¿›è¡Œåˆ†æ...")
        
        # åˆ†å‰²è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42, stratify=y
        )
        
        results = {}
        
        # æ–¹æ³•1ï¼šéšæœºæ£®æ—
        rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
        rf_model.fit(X_train, y_train)
        rf_pred = rf_model.predict(X_test)
        
        results['random_forest'] = {
            'accuracy': accuracy_score(y_test, rf_pred),
            'precision': precision_score(y_test, rf_pred, zero_division=0),
            'recall': recall_score(y_test, rf_pred, zero_division=0),
            'f1': f1_score(y_test, rf_pred, zero_division=0)
        }
        
        # æ–¹æ³•2ï¼šé€»è¾‘å›å½’
        lr_model = LogisticRegression(random_state=42, max_iter=1000)
        lr_model.fit(X_train, y_train)
        lr_pred = lr_model.predict(X_test)
        
        results['logistic_regression'] = {
            'accuracy': accuracy_score(y_test, lr_pred),
            'precision': precision_score(y_test, lr_pred, zero_division=0),
            'recall': recall_score(y_test, lr_pred, zero_division=0),
            'f1': f1_score(y_test, lr_pred, zero_division=0)
        }
        
        # ä¿å­˜æ¨¡å‹
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        if USE_PROJECT_MODULES and self.model_manager:
            # ä¿å­˜åˆ°æ¨¡å‹ç®¡ç†å™¨
            rf_model_path = self.model_manager.models_path / f"climate_rf_{timestamp}.joblib"
        else:
            # ç®€åŒ–æ¨¡å¼ï¼šä¿å­˜åˆ°æœ¬åœ°ç›®å½•
            models_dir = self.output_dir / 'models'
            models_dir.mkdir(exist_ok=True)
            rf_model_path = models_dir / f"climate_rf_{timestamp}.joblib"
        
        import joblib
        joblib.dump(rf_model, rf_model_path)
        logger.info(f"éšæœºæ£®æ—æ¨¡å‹å·²ä¿å­˜: {rf_model_path}")
        
        logger.info("AIæ–¹æ³•åˆ†æå®Œæˆ")
        return results
    
    def create_comparison_chart(self, meteorological_results: Dict, ai_results: Dict):
        """åˆ›å»ºå¯¹æ¯”å›¾è¡¨"""
        logger.info("æ­£åœ¨åˆ›å»ºå¯¹æ¯”å›¾è¡¨...")
        
        # å‡†å¤‡æ•°æ®
        methods = []
        accuracies = []
        precisions = []
        recalls = []
        f1_scores = []
        method_types = []
        
        # æ°”è±¡æ–¹æ³•ç»“æœ
        for method, metrics in meteorological_results.items():
            methods.append(f"æ°”è±¡-{method}")
            accuracies.append(metrics['accuracy'])
            precisions.append(metrics['precision'])
            recalls.append(metrics['recall'])
            f1_scores.append(metrics['f1'])
            method_types.append('ä¼ ç»Ÿæ°”è±¡æ–¹æ³•')
        
        # AIæ–¹æ³•ç»“æœ
        for method, metrics in ai_results.items():
            methods.append(f"AI-{method}")
            accuracies.append(metrics['accuracy'])
            precisions.append(metrics['precision'])
            recalls.append(metrics['recall'])
            f1_scores.append(metrics['f1'])
            method_types.append('AIæ–¹æ³•')
        
        # åˆ›å»ºDataFrame
        df_results = pd.DataFrame({
            'æ–¹æ³•': methods,
            'å‡†ç¡®ç‡': accuracies,
            'ç²¾ç¡®ç‡': precisions,
            'å¬å›ç‡': recalls,
            'F1åˆ†æ•°': f1_scores,
            'æ–¹æ³•ç±»å‹': method_types
        })
        
        # åˆ›å»ºå›¾è¡¨
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('æ°”è±¡ç ”ç©¶æ–¹æ³• vs AIæ–¹æ³• - æç«¯æ°”å€™äº‹ä»¶æ£€æµ‹æ€§èƒ½å¯¹æ¯”', fontsize=16, fontweight='bold')
        
        # å‡†ç¡®ç‡å¯¹æ¯”
        sns.barplot(data=df_results, x='æ–¹æ³•', y='å‡†ç¡®ç‡', hue='æ–¹æ³•ç±»å‹', ax=axes[0, 0])
        axes[0, 0].set_title('å‡†ç¡®ç‡å¯¹æ¯”')
        axes[0, 0].tick_params(axis='x', rotation=45)
        
        # ç²¾ç¡®ç‡å¯¹æ¯”
        sns.barplot(data=df_results, x='æ–¹æ³•', y='ç²¾ç¡®ç‡', hue='æ–¹æ³•ç±»å‹', ax=axes[0, 1])
        axes[0, 1].set_title('ç²¾ç¡®ç‡å¯¹æ¯”')
        axes[0, 1].tick_params(axis='x', rotation=45)
        
        # å¬å›ç‡å¯¹æ¯”
        sns.barplot(data=df_results, x='æ–¹æ³•', y='å¬å›ç‡', hue='æ–¹æ³•ç±»å‹', ax=axes[1, 0])
        axes[1, 0].set_title('å¬å›ç‡å¯¹æ¯”')
        axes[1, 0].tick_params(axis='x', rotation=45)
        
        # F1åˆ†æ•°å¯¹æ¯”
        sns.barplot(data=df_results, x='æ–¹æ³•', y='F1åˆ†æ•°', hue='æ–¹æ³•ç±»å‹', ax=axes[1, 1])
        axes[1, 1].set_title('F1åˆ†æ•°å¯¹æ¯”')
        axes[1, 1].tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        chart_path = Path('outputs') / 'climate_analysis' / f'method_comparison_{timestamp}.png'
        chart_path.parent.mkdir(parents=True, exist_ok=True)
        plt.savefig(chart_path, dpi=300, bbox_inches='tight')
        
        logger.info(f"å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜: {chart_path}")
        
        # æ˜¾ç¤ºå›¾è¡¨
        plt.show()
        
        # ä¿å­˜ç»“æœæ•°æ®
        results_path = chart_path.parent / f'method_comparison_data_{timestamp}.csv'
        df_results.to_csv(results_path, index=False, encoding='utf-8-sig')
        
        logger.info(f"ç»“æœæ•°æ®å·²ä¿å­˜: {results_path}")
        
        return chart_path, results_path
    
    def print_summary(self, meteorological_results: Dict, ai_results: Dict):
        """æ‰“å°åˆ†ææ‘˜è¦"""
        print("\n" + "="*80)
        print("å…¨çƒæç«¯æ°”å€™äº‹ä»¶æ•°æ®é›†åˆ†æç»“æœæ‘˜è¦")
        print("="*80)
        
        print("\nğŸ“Š ä¼ ç»Ÿæ°”è±¡ç ”ç©¶æ–¹æ³•ç»“æœ:")
        for method, metrics in meteorological_results.items():
            print(f"\n  {method}:")
            print(f"    å‡†ç¡®ç‡: {metrics['accuracy']:.3f}")
            print(f"    ç²¾ç¡®ç‡: {metrics['precision']:.3f}")
            print(f"    å¬å›ç‡: {metrics['recall']:.3f}")
            print(f"    F1åˆ†æ•°: {metrics['f1']:.3f}")
        
        print("\nğŸ¤– AIæ–¹æ³•ç»“æœ:")
        for method, metrics in ai_results.items():
            print(f"\n  {method}:")
            print(f"    å‡†ç¡®ç‡: {metrics['accuracy']:.3f}")
            print(f"    ç²¾ç¡®ç‡: {metrics['precision']:.3f}")
            print(f"    å¬å›ç‡: {metrics['recall']:.3f}")
            print(f"    F1åˆ†æ•°: {metrics['f1']:.3f}")
        
        # æ‰¾å‡ºæœ€ä½³æ–¹æ³•
        all_results = {**meteorological_results, **ai_results}
        best_method = max(all_results.keys(), key=lambda x: all_results[x]['f1'])
        best_f1 = all_results[best_method]['f1']
        
        print(f"\nğŸ† æœ€ä½³æ–¹æ³•: {best_method} (F1åˆ†æ•°: {best_f1:.3f})")
        print("="*80)
    
    async def run_analysis(self):
        """è¿è¡Œå®Œæ•´åˆ†ææµç¨‹"""
        try:
            # åˆå§‹åŒ–
            await self.initialize()
            
            # åŠ è½½æ•°æ®é›†
            self.load_dataset()
            
            # å¯¼å…¥æ•°æ®åº“
            record_id = await self.import_to_database()
            
            # æ•°æ®é¢„å¤„ç†
            X, y = self.preprocess_data()
            
            # ä¼ ç»Ÿæ°”è±¡æ–¹æ³•åˆ†æ
            meteorological_results = self.meteorological_method_analysis(X, y)
            
            # AIæ–¹æ³•åˆ†æ
            ai_results = self.ai_method_analysis(X, y)
            
            # åˆ›å»ºå¯¹æ¯”å›¾è¡¨
            chart_path, results_path = self.create_comparison_chart(meteorological_results, ai_results)
            
            # æ‰“å°æ‘˜è¦
            self.print_summary(meteorological_results, ai_results)
            
            return {
                'record_id': record_id,
                'meteorological_results': meteorological_results,
                'ai_results': ai_results,
                'chart_path': str(chart_path),
                'results_path': str(results_path)
            }
            
        except Exception as e:
            logger.error(f"åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            raise
        finally:
            if USE_PROJECT_MODULES and self.data_storage:
                await self.data_storage.close()

def create_sample_data():
    """åˆ›å»ºç¤ºä¾‹æç«¯æ°”å€™äº‹ä»¶æ•°æ®é›†ç”¨äºæ¼”ç¤º"""
    print("ğŸ“Š æ­£åœ¨ç”Ÿæˆç¤ºä¾‹æç«¯æ°”å€™äº‹ä»¶æ•°æ®é›†...")
    
    # åˆ›å»ºdataç›®å½•
    data_dir = Path('data')
    data_dir.mkdir(exist_ok=True)
    
    # ç”Ÿæˆç¤ºä¾‹æ•°æ®
    np.random.seed(42)
    n_samples = 1000
    
    # ç”Ÿæˆæ—¶é—´åºåˆ—ï¼ˆ1951-2022å¹´ï¼‰
    years = np.random.randint(1951, 2023, n_samples)
    months = np.random.randint(1, 13, n_samples)
    
    # ç”Ÿæˆåœ°ç†åæ ‡ï¼ˆå…¨çƒ0.5åº¦ç½‘æ ¼ï¼‰
    latitudes = np.random.uniform(-90, 90, n_samples)
    longitudes = np.random.uniform(-180, 180, n_samples)
    
    # ç”Ÿæˆæ°”è±¡å˜é‡
    temperatures = np.random.normal(15, 10, n_samples)  # æ¸©åº¦ (Â°C)
    precipitation = np.random.exponential(50, n_samples)  # é™æ°´é‡ (mm)
    wind_speed = np.random.gamma(2, 5, n_samples)  # é£é€Ÿ (m/s)
    humidity = np.random.uniform(30, 100, n_samples)  # æ¹¿åº¦ (%)
    pressure = np.random.normal(1013, 20, n_samples)  # æ°”å‹ (hPa)
    
    # ç”Ÿæˆæç«¯äº‹ä»¶æ ‡è¯†ï¼ˆåŸºäºå¤šä¸ªæ¡ä»¶ï¼‰
    extreme_temp = (temperatures > np.percentile(temperatures, 95)) | (temperatures < np.percentile(temperatures, 5))
    extreme_precip = precipitation > np.percentile(precipitation, 95)
    extreme_wind = wind_speed > np.percentile(wind_speed, 90)
    
    # ç»¼åˆæç«¯äº‹ä»¶åˆ¤æ–­
    extreme_event = (extreme_temp | extreme_precip | extreme_wind).astype(int)
    
    # åˆ›å»ºDataFrame
    sample_data = pd.DataFrame({
        'year': years,
        'month': months,
        'latitude': latitudes,
        'longitude': longitudes,
        'temperature': temperatures,
        'precipitation': precipitation,
        'wind_speed': wind_speed,
        'humidity': humidity,
        'pressure': pressure,
        'extreme_event': extreme_event
    })
    
    # ä¿å­˜ç¤ºä¾‹æ•°æ®
    sample_file = data_dir / 'sample_climate_extreme_events.csv'
    sample_data.to_csv(sample_file, index=False, encoding='utf-8')
    
    print(f"âœ… ç¤ºä¾‹æ•°æ®å·²åˆ›å»º: {sample_file}")
    print(f"ğŸ“ˆ æ•°æ®å½¢çŠ¶: {sample_data.shape}")
    print(f"ğŸ”¥ æç«¯äº‹ä»¶æ¯”ä¾‹: {extreme_event.mean():.3f}")
    
    return str(sample_file)

async def main():
    """ä¸»å‡½æ•°"""
    # æ•°æ®é›†è·¯å¾„
    dataset_path = r"D:\ç”¨æˆ·\jin\æ¡Œé¢\å…¨çƒ0.5Â°é€å¹´å¤åˆæç«¯æ°”å€™äº‹ä»¶æ•°æ®é›†ï¼ˆ1951-2022å¹´ï¼‰-æ•°æ®å®ä½“"
    
    # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
    if not Path(dataset_path).exists():
        print(f"âŒ é”™è¯¯: æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {dataset_path}")
        print("è¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼Œæˆ–å°†æ•°æ®é›†æ–‡ä»¶æ”¾ç½®åœ¨æŒ‡å®šä½ç½®ã€‚")
        print("\nğŸ”§ æ­£åœ¨åˆ›å»ºç¤ºä¾‹æ•°æ®ç”¨äºæ¼”ç¤º...")
        dataset_path = create_sample_data()
    else:
        # å¦‚æœæ˜¯ç›®å½•ï¼ŒæŸ¥æ‰¾æ•°æ®æ–‡ä»¶
        if Path(dataset_path).is_dir():
            # æ”¯æŒæ›´å¤šæ–‡ä»¶æ ¼å¼
            extensions = ['*.csv', '*.xlsx', '*.xls', '*.nc', '*.txt', '*.dat', '*.json']
            data_files = []
            
            for ext in extensions:
                data_files.extend(list(Path(dataset_path).glob(ext)))
            
            if not data_files:
                print(f"âŒ é”™è¯¯: åœ¨ç›®å½• {dataset_path} ä¸­æœªæ‰¾åˆ°æ”¯æŒçš„æ•°æ®æ–‡ä»¶")
                print("æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: .csv, .xlsx, .xls, .nc, .txt, .dat, .json")
                print("\nğŸ’¡ å»ºè®®:")
                print("1. å°†æ•°æ®æ–‡ä»¶å¤åˆ¶åˆ°é¡¹ç›®ç›®å½• 'data' æ–‡ä»¶å¤¹ä¸­")
                print("2. æˆ–è€…ç›´æ¥æŒ‡å®šæ•°æ®æ–‡ä»¶çš„å®Œæ•´è·¯å¾„")
                
                # åˆ›å»ºç¤ºä¾‹æ•°æ®ç”¨äºæ¼”ç¤º
                print("\nğŸ”§ æ­£åœ¨åˆ›å»ºç¤ºä¾‹æ•°æ®ç”¨äºæ¼”ç¤º...")
                dataset_path = create_sample_data()
            else:
                # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„æ•°æ®æ–‡ä»¶
                dataset_path = str(data_files[0])
                print(f"ğŸ“ æ‰¾åˆ°æ•°æ®æ–‡ä»¶: {dataset_path}")
    
    print("\nğŸš€ å¼€å§‹å…¨çƒæç«¯æ°”å€™äº‹ä»¶æ•°æ®é›†åˆ†æ...")
    print(f"ğŸ“‚ æ•°æ®é›†è·¯å¾„: {dataset_path}")
    
    # åˆ›å»ºåˆ†æå™¨å¹¶è¿è¡Œåˆ†æ
    analyzer = ClimateDatasetAnalyzer(dataset_path)
    
    try:
        results = await analyzer.run_analysis()
        
        print("\nâœ… åˆ†æå®Œæˆ!")
        print(f"ğŸ“Š å¯¹æ¯”å›¾è¡¨: {results['chart_path']}")
        print(f"ğŸ“‹ ç»“æœæ•°æ®: {results['results_path']}")
        print(f"ğŸ—„ï¸ æ•°æ®åº“è®°å½•ID: {results['record_id']}")
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        logger.error(f"åˆ†æå¤±è´¥: {e}", exc_info=True)

if __name__ == "__main__":
    asyncio.run(main())