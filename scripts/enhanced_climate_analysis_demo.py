#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆå†å²æ°”å€™æ¨¡å¼è¯†åˆ«æ¼”ç¤º

æœ¬è„šæœ¬åˆ›å»ºæ›´çœŸå®çš„æ¨¡æ‹Ÿæ°”å€™æ•°æ®ï¼Œå±•ç¤ºAIæŠ€æœ¯åœ¨è¯†åˆ«å†å²æ°”å€™æ¨¡å¼ã€è¶‹åŠ¿å’Œå¼‚å¸¸äº‹ä»¶æ–¹é¢çš„èƒ½åŠ›ã€‚
åŒ…å«ï¼š
1. æ˜æ˜¾çš„å…¨çƒå˜æš–è¶‹åŠ¿
2. å­£èŠ‚æ€§å˜åŒ–æ¨¡å¼
3. æç«¯å¤©æ°”äº‹ä»¶ï¼ˆçƒ­æµªã€å¯’æ½®ã€å¹²æ—±ã€æ´ªæ¶ï¼‰
4. æ°”å€™æŒ¯è¡æ¨¡å¼ï¼ˆå¦‚ENSOï¼‰
5. çªå˜ç‚¹æ£€æµ‹
"""

import sys
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.models.historical_climate_analyzer import (
    HistoricalClimateAnalyzer,
    AnomalyMethod,
    TrendMethod
)
from src.utils.logger import setup_logger

# è®¾ç½®æ—¥å¿—
logger = setup_logger(__name__)

# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œå›¾è¡¨æ ·å¼
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
plt.style.use('seaborn-v0_8')

def create_realistic_climate_data():
    """
    åˆ›å»ºæ›´çœŸå®çš„å†å²æ°”å€™æ•°æ®ï¼ŒåŒ…å«å¤šç§æ°”å€™æ¨¡å¼å’Œäº‹ä»¶
    """
    logger.info("åˆ›å»ºçœŸå®çš„å†å²æ°”å€™æ•°æ®...")
    
    # åˆ›å»º50å¹´çš„æœˆåº¦æ•°æ®ï¼ˆ1970-2023ï¼‰
    start_date = datetime(1970, 1, 1)
    end_date = datetime(2023, 12, 31)
    dates = pd.date_range(start_date, end_date, freq='M')
    
    np.random.seed(42)
    n_points = len(dates)
    time_index = np.arange(n_points)
    
    # === æ¸©åº¦æ•°æ® ===
    # 1. å…¨çƒå˜æš–è¶‹åŠ¿ï¼ˆ1970-2023å¹´å‡æ¸©çº¦1.5åº¦ï¼‰
    warming_trend = 1.5 * time_index / n_points
    
    # 2. å­£èŠ‚æ€§å˜åŒ–ï¼ˆåŒ—åŠçƒï¼‰
    seasonal_temp = 15 * np.sin(2 * np.pi * time_index / 12 - np.pi/2)
    
    # 3. å¹´é™…å˜åŒ–ï¼ˆENSOç­‰ï¼‰
    enso_cycle = 2 * np.sin(2 * np.pi * time_index / (3.5 * 12))  # 3.5å¹´å‘¨æœŸ
    
    # 4. åå¹´é™…å˜åŒ–ï¼ˆPDOç­‰ï¼‰
    decadal_cycle = 1.5 * np.sin(2 * np.pi * time_index / (20 * 12))  # 20å¹´å‘¨æœŸ
    
    # 5. éšæœºå™ªå£°
    temp_noise = np.random.normal(0, 1.5, n_points)
    
    # åŸºç¡€æ¸©åº¦
    base_temp = 14.0
    temperature = base_temp + warming_trend + seasonal_temp + enso_cycle + decadal_cycle + temp_noise
    
    # æ·»åŠ æç«¯äº‹ä»¶
    # çƒ­æµªäº‹ä»¶ï¼ˆå¤å­£æ›´é¢‘ç¹ï¼‰
    summer_months = np.where((time_index % 12 >= 5) & (time_index % 12 <= 7))[0]
    heatwave_indices = np.random.choice(summer_months, min(30, len(summer_months)), replace=False)
    temperature[heatwave_indices] += np.random.uniform(5, 12, len(heatwave_indices))
    
    # å¯’æ½®äº‹ä»¶ï¼ˆå†¬å­£æ›´é¢‘ç¹ï¼‰
    winter_months = np.where((time_index % 12 <= 1) | (time_index % 12 >= 11))[0]
    coldwave_indices = np.random.choice(winter_months, min(20, len(winter_months)), replace=False)
    temperature[coldwave_indices] -= np.random.uniform(8, 15, len(coldwave_indices))
    
    # æ°”å€™çªå˜ç‚¹ï¼ˆ1980å¹´ä»£å’Œ2000å¹´ä»£ï¼‰
    shift_1980s = np.where((time_index >= 10*12) & (time_index <= 15*12))[0]
    temperature[shift_1980s] += 0.8  # 1980å¹´ä»£å‡æ¸©
    
    shift_2000s = np.where(time_index >= 30*12)[0]
    temperature[shift_2000s] += 0.5  # 2000å¹´ä»£åè¿›ä¸€æ­¥å‡æ¸©
    
    # === é™æ°´æ•°æ® ===
    # 1. å­£èŠ‚æ€§é™æ°´ï¼ˆå¤å­£å¤šé›¨ï¼‰
    seasonal_precip = 50 + 40 * np.sin(2 * np.pi * time_index / 12)
    
    # 2. ENSOå¯¹é™æ°´çš„å½±å“
    enso_precip_effect = 20 * np.sin(2 * np.pi * time_index / (3.5 * 12) + np.pi/4)
    
    # 3. é•¿æœŸè¶‹åŠ¿ï¼ˆæŸäº›åœ°åŒºé™æ°´å‡å°‘ï¼‰
    precip_trend = -10 * time_index / n_points
    
    # 4. éšæœºå˜åŒ–
    precip_noise = np.random.gamma(2, 15, n_points)
    
    precipitation = seasonal_precip + enso_precip_effect + precip_trend + precip_noise
    precipitation = np.maximum(precipitation, 0)  # é™æ°´ä¸èƒ½ä¸ºè´Ÿ
    
    # æ·»åŠ æç«¯é™æ°´äº‹ä»¶
    # å¹²æ—±äº‹ä»¶ï¼ˆè¿ç»­ä½é™æ°´ï¼‰
    drought_periods = [
        (15*12, 15*12+18),  # 1985å¹´å¹²æ—±
        (25*12, 25*12+24),  # 1995å¹´å¹²æ—±
        (40*12, 40*12+15)   # 2010å¹´å¹²æ—±
    ]
    
    for start, end in drought_periods:
        if end < n_points:
            precipitation[start:end] *= 0.3
    
    # æ´ªæ¶äº‹ä»¶ï¼ˆæç«¯é™æ°´ï¼‰
    flood_indices = np.random.choice(n_points, 25, replace=False)
    precipitation[flood_indices] += np.random.uniform(150, 400, 25)
    
    # === å…¶ä»–æ°”è±¡å˜é‡ ===
    
    # æ¹¿åº¦ï¼ˆä¸æ¸©åº¦å’Œé™æ°´ç›¸å…³ï¼‰
    humidity = 65 + 15 * np.sin(2 * np.pi * time_index / 12) - 0.3 * warming_trend + \
               0.1 * (precipitation - np.mean(precipitation)) + np.random.normal(0, 3, n_points)
    humidity = np.clip(humidity, 20, 95)
    
    # é£é€Ÿï¼ˆå­£èŠ‚æ€§å˜åŒ– + æç«¯äº‹ä»¶ï¼‰
    wind_speed = 12 + 4 * np.sin(2 * np.pi * time_index / 12 + np.pi) + np.random.exponential(2, n_points)
    
    # æ·»åŠ é£æš´äº‹ä»¶
    storm_indices = np.random.choice(n_points, 15, replace=False)
    wind_speed[storm_indices] += np.random.uniform(20, 40, 15)
    
    # æ°”å‹ï¼ˆä¸æ¸©åº¦åç›¸å…³ï¼‰
    pressure = 1013 - 0.5 * warming_trend + 8 * np.sin(2 * np.pi * time_index / 12 + np.pi) + \
               np.random.normal(0, 5, n_points)
    
    # æµ·è¡¨æ¸©åº¦ï¼ˆä¸é™†åœ°æ¸©åº¦ç›¸å…³ä½†å˜åŒ–è¾ƒå°ï¼‰
    sst = temperature * 0.8 + 2 + np.random.normal(0, 0.8, n_points)
    
    # åˆ›å»ºDataFrame
    climate_data = pd.DataFrame({
        'time': dates,
        'temperature': temperature,
        'precipitation': precipitation,
        'humidity': humidity,
        'wind_speed': wind_speed,
        'pressure': pressure,
        'sea_surface_temp': sst
    })
    
    logger.info(f"åˆ›å»ºäº†åŒ…å« {len(climate_data)} æ¡è®°å½•çš„çœŸå®æ°”å€™æ•°æ®")
    logger.info(f"æ—¶é—´èŒƒå›´: {dates[0]} è‡³ {dates[-1]}")
    logger.info(f"å˜é‡: {list(climate_data.columns[1:])}")
    
    return climate_data

def analyze_climate_patterns(data):
    """
    æ‰§è¡Œå…¨é¢çš„æ°”å€™æ¨¡å¼åˆ†æ
    """
    logger.info("=== å¼€å§‹å…¨é¢æ°”å€™æ¨¡å¼åˆ†æ ===")
    
    analyzer = HistoricalClimateAnalyzer()
    
    print("\nğŸŒ å†å²æ°”å€™æ¨¡å¼è¯†åˆ«ä¸åˆ†æ")
    print("=" * 60)
    print(f"ğŸ“Š æ•°æ®æ¦‚å†µ: {len(data)} ä¸ªæœˆçš„æ°”å€™æ•°æ® ({data['time'].min().year}-{data['time'].max().year})")
    print(f"ğŸŒ¡ï¸ åˆ†æå˜é‡: {', '.join(data.columns[1:])}")
    print()
    
    # 1. è¶‹åŠ¿åˆ†æ
    print("ğŸ“ˆ 1. é•¿æœŸè¶‹åŠ¿åˆ†æ")
    print("-" * 30)
    
    trend_results = analyzer.analyze_trends(data, method=TrendMethod.LINEAR_REGRESSION)
    
    for var, result in trend_results.items():
        if result.significance == 'significant':
            direction_emoji = {
                'increasing': 'ğŸ”º',
                'decreasing': 'ğŸ”»',
                'stable': 'â¡ï¸'
            }.get(result.trend_direction, 'â“')
            
            print(f"  {direction_emoji} {var}:")
            print(f"    â€¢ è¶‹åŠ¿: {result.trend_direction} (p={result.p_value:.4f})")
            print(f"    â€¢ å¹´é™…å˜åŒ–: {result.annual_change:.4f}")
            print(f"    â€¢ 50å¹´æ€»å˜åŒ–: {result.annual_change * 50:.2f}")
            print(f"    â€¢ RÂ²: {result.r_squared:.4f}")
            print()
    
    # 2. å­£èŠ‚æ€§åˆ†æ
    print("ğŸ”„ 2. å­£èŠ‚æ€§æ¨¡å¼åˆ†æ")
    print("-" * 30)
    
    seasonality_results = analyzer.analyze_seasonality(data)
    
    for var, result in seasonality_results.items():
        if result.has_seasonality:
            print(f"  ğŸ”„ {var}:")
            print(f"    â€¢ å­£èŠ‚æ€§å¼ºåº¦: {result.seasonal_strength:.4f}")
            if result.seasonal_peaks:
                months = ['1æœˆ', '2æœˆ', '3æœˆ', '4æœˆ', '5æœˆ', '6æœˆ', 
                         '7æœˆ', '8æœˆ', '9æœˆ', '10æœˆ', '11æœˆ', '12æœˆ']
                peak_months = [months[int(peak)-1] for peak in result.seasonal_peaks if 1 <= int(peak) <= 12]
                print(f"    â€¢ å³°å€¼æœˆä»½: {', '.join(peak_months)}")
            print()
    
    # 3. å¼‚å¸¸äº‹ä»¶æ£€æµ‹
    print("ğŸš¨ 3. å¼‚å¸¸äº‹ä»¶æ£€æµ‹")
    print("-" * 30)
    
    # ä½¿ç”¨å¤šç§æ–¹æ³•æ£€æµ‹å¼‚å¸¸
    methods = [AnomalyMethod.ISOLATION_FOREST, AnomalyMethod.STATISTICAL]
    
    for method in methods:
        print(f"\n  æ–¹æ³•: {method.value}")
        anomaly_results = analyzer.detect_anomalies(data, method=method, contamination=0.05)
        
        for var, result in anomaly_results.items():
            if result.total_anomalies > 0:
                print(f"    ğŸš¨ {var}: {result.total_anomalies} ä¸ªå¼‚å¸¸ç‚¹ ({result.anomaly_rate:.1%})")
                
                # æ˜¾ç¤ºæœ€æç«¯çš„å¼‚å¸¸å€¼
                if result.anomaly_values:
                    extreme_values = sorted(result.anomaly_values, key=abs, reverse=True)[:3]
                    print(f"      æœ€æç«¯å€¼: {[f'{v:.2f}' for v in extreme_values]}")
    
    # 4. æç«¯äº‹ä»¶åˆ†æ
    print("\nâš¡ 4. æç«¯äº‹ä»¶åˆ†æ")
    print("-" * 30)
    
    extreme_results = analyzer.analyze_extreme_events(data)
    
    event_emojis = {
        'heatwave': 'ğŸ”¥',
        'coldwave': 'ğŸ§Š',
        'drought': 'ğŸœï¸',
        'flood': 'ğŸŒŠ'
    }
    
    for event_key, result in extreme_results.items():
        if len(result.events) > 0:
            emoji = event_emojis.get(result.event_type, 'âš¡')
            print(f"  {emoji} {result.variable} - {result.event_type}:")
            print(f"    â€¢ äº‹ä»¶æ€»æ•°: {len(result.events)}")
            print(f"    â€¢ å¹´å‡é¢‘ç‡: {result.frequency:.2f} æ¬¡/å¹´")
            
            if result.events:
                intensities = [event['intensity'] for event in result.events]
                durations = [event['duration'] for event in result.events]
                
                print(f"    â€¢ å¹³å‡å¼ºåº¦: {np.mean(intensities):.2f}")
                print(f"    â€¢ æœ€å¤§å¼ºåº¦: {np.max(intensities):.2f}")
                print(f"    â€¢ å¹³å‡æŒç»­: {np.mean(durations):.1f} ä¸ªæœˆ")
                
                # æ£€æŸ¥è¶‹åŠ¿
                if result.intensity_trend.significance == 'significant':
                    trend_dir = 'å¢å¼º' if result.intensity_trend.trend_direction == 'increasing' else 'å‡å¼±'
                    print(f"    â€¢ å¼ºåº¦è¶‹åŠ¿: {trend_dir} âš ï¸")
            print()
    
    # 5. æ¨¡å¼è¯†åˆ«
    print("ğŸ” 5. æ°”å€™æ¨¡å¼è¯†åˆ«")
    print("-" * 30)
    
    pattern_results = analyzer.identify_patterns(data, n_clusters=4)
    
    for var, result in pattern_results.items():
        if result.pattern_strength > 0.3:  # åªæ˜¾ç¤ºè¾ƒå¼ºçš„æ¨¡å¼
            print(f"  ğŸ” {var}:")
            print(f"    â€¢ æ¨¡å¼å¼ºåº¦: {result.pattern_strength:.4f}")
            print(f"    â€¢ è¯†åˆ«æ¨¡å¼æ•°: {len(result.dominant_patterns)}")
            
            for i, pattern in enumerate(result.dominant_patterns[:2]):  # æ˜¾ç¤ºå‰2ä¸ªä¸»è¦æ¨¡å¼
                print(f"      æ¨¡å¼ {i+1}: {pattern['characteristics']} ({pattern['percentage']:.1f}%)")
            print()
    
    return {
        'trends': trend_results,
        'seasonality': seasonality_results,
        'anomalies': anomaly_results,
        'extremes': extreme_results,
        'patterns': pattern_results
    }

def create_comprehensive_visualizations(data, analysis_results):
    """
    åˆ›å»ºç»¼åˆçš„å¯è§†åŒ–åˆ†æå›¾è¡¨
    """
    logger.info("åˆ›å»ºç»¼åˆå¯è§†åŒ–åˆ†æ...")
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“å’Œå›¾è¡¨æ ·å¼
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS', 'DejaVu Sans', 'sans-serif']
    plt.rcParams['axes.unicode_minus'] = False
    plt.rcParams['font.size'] = 10
    plt.rcParams['figure.titlesize'] = 14
    plt.rcParams['axes.titlesize'] = 12
    plt.rcParams['axes.labelsize'] = 10
    plt.rcParams['xtick.labelsize'] = 9
    plt.rcParams['ytick.labelsize'] = 9
    plt.rcParams['legend.fontsize'] = 9
    
    # åˆ›å»ºå¤§å‹å›¾è¡¨
    fig = plt.figure(figsize=(20, 16))
    gs = fig.add_gridspec(4, 3, hspace=0.3, wspace=0.3)
    
    # 1. æ¸©åº¦é•¿æœŸè¶‹åŠ¿ (å¤§å›¾)
    ax1 = fig.add_subplot(gs[0, :])
    ax1.plot(data['time'], data['temperature'], alpha=0.6, linewidth=0.8, color='steelblue', label='æœˆåº¦æ¸©åº¦')
    
    # æ·»åŠ è¶‹åŠ¿çº¿
    if 'temperature' in analysis_results['trends']:
        trend = analysis_results['trends']['temperature']
        x_numeric = np.arange(len(data))
        trend_line = trend.slope * x_numeric + trend.intercept
        # ç¡®ä¿æ•°å­—æ ¼å¼åŒ–æ­£ç¡®æ˜¾ç¤º
        trend_str = f"{trend.annual_change:.4f}".replace('.', '.')
        ax1.plot(data['time'], trend_line, 'r-', linewidth=3, 
                label=f'è¶‹åŠ¿çº¿ ({trend_str}Â°C/å¹´)')
    
    # æ·»åŠ 5å¹´ç§»åŠ¨å¹³å‡
    temp_5yr = data['temperature'].rolling(window=60, center=True).mean()
    ax1.plot(data['time'], temp_5yr, 'orange', linewidth=2, label='5å¹´ç§»åŠ¨å¹³å‡')
    
    ax1.set_title('å…¨çƒæ¸©åº¦é•¿æœŸå˜åŒ–è¶‹åŠ¿ (1970-2023)', fontsize=14, fontweight='bold')
    ax1.set_ylabel('æ¸©åº¦ (Â°C)')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. é™æ°´å¼‚å¸¸æ£€æµ‹
    ax2 = fig.add_subplot(gs[1, 0])
    ax2.plot(data['time'], data['precipitation'], alpha=0.7, linewidth=1, color='blue')
    
    # æ ‡è®°å¼‚å¸¸ç‚¹
    if 'precipitation' in analysis_results['anomalies']:
        anomaly_result = analysis_results['anomalies']['precipitation']
        if anomaly_result.anomaly_indices:
            anomaly_times = data['time'].iloc[anomaly_result.anomaly_indices]
            anomaly_values = [data['precipitation'].iloc[i] for i in anomaly_result.anomaly_indices]
            # ç¡®ä¿æ•°å­—æ ¼å¼åŒ–æ­£ç¡®æ˜¾ç¤º
            anomaly_count = len(anomaly_result.anomaly_indices)
            ax2.scatter(anomaly_times, anomaly_values, color='red', s=30, 
                       label=f'å¼‚å¸¸ç‚¹ ({anomaly_count}ä¸ª)', zorder=5)
    
    ax2.set_title('é™æ°´å¼‚å¸¸æ£€æµ‹')
    ax2.set_ylabel('é™æ°´é‡ (mm)')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. å­£èŠ‚æ€§æ¨¡å¼
    ax3 = fig.add_subplot(gs[1, 1])
    
    # è®¡ç®—æœˆåº¦å¹³å‡å€¼
    monthly_temp = data.groupby(data['time'].dt.month)['temperature'].mean()
    monthly_precip = data.groupby(data['time'].dt.month)['precipitation'].mean()
    
    months = ['1æœˆ', '2æœˆ', '3æœˆ', '4æœˆ', '5æœˆ', '6æœˆ', 
             '7æœˆ', '8æœˆ', '9æœˆ', '10æœˆ', '11æœˆ', '12æœˆ']
    
    ax3_twin = ax3.twinx()
    
    line1 = ax3.plot(range(1, 13), monthly_temp, 'ro-', linewidth=2, label='æ¸©åº¦')
    line2 = ax3_twin.plot(range(1, 13), monthly_precip, 'bs-', linewidth=2, label='é™æ°´')
    
    ax3.set_xlabel('æœˆä»½')
    ax3.set_ylabel('æ¸©åº¦ (Â°C)', color='red')
    ax3_twin.set_ylabel('é™æ°´é‡ (mm)', color='blue')
    ax3.set_title('å­£èŠ‚æ€§å˜åŒ–æ¨¡å¼')
    ax3.set_xticks(range(1, 13))
    ax3.set_xticklabels([m[:2] for m in months])
    ax3.grid(True, alpha=0.3)
    
    # åˆå¹¶å›¾ä¾‹
    lines = line1 + line2
    labels = [l.get_label() for l in lines]
    ax3.legend(lines, labels, loc='upper left')
    
    # 4. å˜é‡ç›¸å…³æ€§çƒ­å›¾
    ax4 = fig.add_subplot(gs[1, 2])
    numeric_cols = data.select_dtypes(include=[np.number]).columns
    corr_matrix = data[numeric_cols].corr()
    
    im = ax4.imshow(corr_matrix, cmap='RdBu_r', aspect='auto', vmin=-1, vmax=1)
    ax4.set_xticks(range(len(corr_matrix.columns)))
    ax4.set_yticks(range(len(corr_matrix.columns)))
    ax4.set_xticklabels([col[:8] for col in corr_matrix.columns], rotation=45, ha='right')
    ax4.set_yticklabels([col[:8] for col in corr_matrix.columns])
    ax4.set_title('å˜é‡ç›¸å…³æ€§çŸ©é˜µ')
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i in range(len(corr_matrix.columns)):
        for j in range(len(corr_matrix.columns)):
            text = ax4.text(j, i, f'{corr_matrix.iloc[i, j]:.2f}',
                           ha="center", va="center", color="black", fontsize=8)
    
    plt.colorbar(im, ax=ax4, shrink=0.8)
    
    # 5. æç«¯äº‹ä»¶æ—¶é—´åºåˆ—
    ax5 = fig.add_subplot(gs[2, :])
    
    # ç»˜åˆ¶æ¸©åº¦å’Œæç«¯äº‹ä»¶
    ax5.plot(data['time'], data['temperature'], alpha=0.5, color='gray', linewidth=0.8)
    
    # æ ‡è®°æç«¯äº‹ä»¶
    colors = {'heatwave': 'red', 'coldwave': 'blue', 'drought': 'brown', 'flood': 'cyan'}
    
    for event_key, result in analysis_results['extremes'].items():
        if len(result.events) > 0 and 'temperature' in result.variable:
            color = colors.get(result.event_type, 'black')
            for event in result.events:
                start_idx = event['start_index']
                end_idx = event['end_index']
                if start_idx < len(data) and end_idx < len(data):
                    event_time = data['time'].iloc[start_idx:end_idx+1]
                    event_temp = data['temperature'].iloc[start_idx:end_idx+1]
                    ax5.plot(event_time, event_temp, color=color, linewidth=3, alpha=0.8)
    
    ax5.set_title('æç«¯æ¸©åº¦äº‹ä»¶è¯†åˆ«')
    ax5.set_ylabel('æ¸©åº¦ (Â°C)')
    ax5.grid(True, alpha=0.3)
    
    # æ·»åŠ å›¾ä¾‹
    legend_elements = [plt.Line2D([0], [0], color=color, lw=3, label=event_type) 
                      for event_type, color in colors.items()]
    ax5.legend(handles=legend_elements, loc='upper left')
    
    # 6. å¹´ä»£é™…å˜åŒ–å¯¹æ¯”
    ax6 = fig.add_subplot(gs[3, 0])
    
    # æŒ‰å¹´ä»£åˆ†ç»„
    data['decade'] = (data['time'].dt.year // 10) * 10
    decade_temp = data.groupby('decade')['temperature'].mean()
    
    bars = ax6.bar(decade_temp.index, decade_temp.values, 
                   color=['lightblue', 'skyblue', 'steelblue', 'darkblue', 'navy', 'midnightblue'])
    ax6.set_title('å¹´ä»£é™…æ¸©åº¦å˜åŒ–')
    ax6.set_xlabel('å¹´ä»£')
    ax6.set_ylabel('å¹³å‡æ¸©åº¦ (Â°C)')
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, temp in zip(bars, decade_temp.values):
        height = bar.get_height()
        ax6.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                f'{temp:.1f}Â°C', ha='center', va='bottom')
    
    # 7. å¼‚å¸¸äº‹ä»¶é¢‘ç‡ç»Ÿè®¡
    ax7 = fig.add_subplot(gs[3, 1])
    
    anomaly_counts = {}
    for var, result in analysis_results['anomalies'].items():
        if result.total_anomalies > 0:
            anomaly_counts[var[:8]] = result.total_anomalies
    
    if anomaly_counts:
        vars_list = list(anomaly_counts.keys())
        counts = list(anomaly_counts.values())
        
        bars = ax7.bar(vars_list, counts, color='orange', alpha=0.7)
        ax7.set_title('å¼‚å¸¸äº‹ä»¶ç»Ÿè®¡')
        ax7.set_ylabel('å¼‚å¸¸ç‚¹æ•°é‡')
        ax7.tick_params(axis='x', rotation=45)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, count in zip(bars, counts):
            height = bar.get_height()
            ax7.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                    f'{count}', ha='center', va='bottom')
    
    # 8. æ¨¡å¼å¼ºåº¦å¯¹æ¯”
    ax8 = fig.add_subplot(gs[3, 2])
    
    pattern_strengths = {}
    for var, result in analysis_results['patterns'].items():
        pattern_strengths[var[:8]] = result.pattern_strength
    
    if pattern_strengths:
        vars_list = list(pattern_strengths.keys())
        strengths = list(pattern_strengths.values())
        
        bars = ax8.bar(vars_list, strengths, color='green', alpha=0.7)
        ax8.set_title('æ°”å€™æ¨¡å¼å¼ºåº¦')
        ax8.set_ylabel('æ¨¡å¼å¼ºåº¦')
        ax8.tick_params(axis='x', rotation=45)
        ax8.set_ylim(0, 1)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, strength in zip(bars, strengths):
            height = bar.get_height()
            ax8.text(bar.get_x() + bar.get_width()/2., height + 0.02,
                    f'{strength:.3f}', ha='center', va='bottom')
    
    plt.suptitle('å†å²æ°”å€™æ¨¡å¼è¯†åˆ«ä¸åˆ†æç»¼åˆæŠ¥å‘Š', fontsize=18, fontweight='bold', y=0.98)
    
    # ä¿å­˜å›¾è¡¨
    output_dir = "outputs/climate_analysis"
    os.makedirs(output_dir, exist_ok=True)
    
    plt.savefig(f"{output_dir}/enhanced_climate_analysis.png", dpi=300, bbox_inches='tight')
    logger.info(f"ç»¼åˆåˆ†æå›¾è¡¨å·²ä¿å­˜åˆ° {output_dir}/enhanced_climate_analysis.png")
    
    plt.show()

def generate_analysis_summary(data, analysis_results):
    """
    ç”Ÿæˆåˆ†ææ‘˜è¦æŠ¥å‘Š
    """
    print("\nğŸ“‹ åˆ†ææ‘˜è¦æŠ¥å‘Š")
    print("=" * 60)
    
    # æ•°æ®æ¦‚å†µ
    print(f"ğŸ“Š æ•°æ®æ—¶é—´è·¨åº¦: {data['time'].min().year}-{data['time'].max().year} ({len(data)} ä¸ªæœˆ)")
    print(f"ğŸŒ¡ï¸ æ¸©åº¦èŒƒå›´: {data['temperature'].min():.1f}Â°C è‡³ {data['temperature'].max():.1f}Â°C")
    print(f"ğŸŒ§ï¸ é™æ°´èŒƒå›´: {data['precipitation'].min():.1f}mm è‡³ {data['precipitation'].max():.1f}mm")
    print()
    
    # è¶‹åŠ¿å‘ç°
    significant_trends = {var: result for var, result in analysis_results['trends'].items() 
                         if result.significance == 'significant'}
    
    print(f"ğŸ“ˆ æ˜¾è‘—è¶‹åŠ¿å‘ç° ({len(significant_trends)} ä¸ª):")
    for var, result in significant_trends.items():
        direction = 'ä¸Šå‡' if result.trend_direction == 'increasing' else 'ä¸‹é™'
        print(f"  â€¢ {var}: {direction}è¶‹åŠ¿ï¼Œå¹´å˜åŒ–ç‡ {result.annual_change:.4f}")
    print()
    
    # å­£èŠ‚æ€§å‘ç°
    seasonal_vars = {var: result for var, result in analysis_results['seasonality'].items() 
                    if result.has_seasonality}
    
    print(f"ğŸ”„ å­£èŠ‚æ€§æ¨¡å¼ ({len(seasonal_vars)} ä¸ªå˜é‡):")
    for var, result in seasonal_vars.items():
        print(f"  â€¢ {var}: å­£èŠ‚æ€§å¼ºåº¦ {result.seasonal_strength:.3f}")
    print()
    
    # å¼‚å¸¸äº‹ä»¶ç»Ÿè®¡
    total_anomalies = sum(result.total_anomalies for result in analysis_results['anomalies'].values())
    print(f"ğŸš¨ å¼‚å¸¸äº‹ä»¶: å…±æ£€æµ‹åˆ° {total_anomalies} ä¸ªå¼‚å¸¸ç‚¹")
    
    high_anomaly_vars = {var: result for var, result in analysis_results['anomalies'].items() 
                        if result.anomaly_rate > 0.05}
    
    for var, result in high_anomaly_vars.items():
        print(f"  â€¢ {var}: {result.total_anomalies} ä¸ªå¼‚å¸¸ç‚¹ ({result.anomaly_rate:.1%})")
    print()
    
    # æç«¯äº‹ä»¶ç»Ÿè®¡
    total_extreme_events = sum(len(result.events) for result in analysis_results['extremes'].values())
    print(f"âš¡ æç«¯äº‹ä»¶: å…±è¯†åˆ« {total_extreme_events} ä¸ªæç«¯äº‹ä»¶")
    
    for event_key, result in analysis_results['extremes'].items():
        if len(result.events) > 0:
            print(f"  â€¢ {result.variable} {result.event_type}: {len(result.events)} æ¬¡ ({result.frequency:.2f}/å¹´)")
    print()
    
    # æ¨¡å¼è¯†åˆ«ç»“æœ
    strong_patterns = {var: result for var, result in analysis_results['patterns'].items() 
                      if result.pattern_strength > 0.3}
    
    print(f"ğŸ” æ°”å€™æ¨¡å¼: è¯†åˆ«å‡º {len(strong_patterns)} ä¸ªå¼ºæ¨¡å¼")
    for var, result in strong_patterns.items():
        print(f"  â€¢ {var}: æ¨¡å¼å¼ºåº¦ {result.pattern_strength:.3f}")
    print()
    
    # å…³é”®å‘ç°
    print("ğŸ¯ å…³é”®å‘ç°:")
    
    # æ¸©åº¦è¶‹åŠ¿
    if 'temperature' in significant_trends:
        temp_trend = significant_trends['temperature']
        total_change = temp_trend.annual_change * (data['time'].max().year - data['time'].min().year)
        print(f"  ğŸ”¥ æ¸©åº¦æ˜¾è‘—ä¸Šå‡: {total_change:.2f}Â°C ({data['time'].min().year}-{data['time'].max().year})")
    
    # æç«¯äº‹ä»¶è¶‹åŠ¿
    extreme_trends = []
    for event_key, result in analysis_results['extremes'].items():
        if (result.intensity_trend.significance == 'significant' or 
            result.duration_trend.significance == 'significant'):
            extreme_trends.append(f"{result.event_type}äº‹ä»¶å¼ºåº¦æˆ–æŒç»­æ—¶é—´æœ‰æ˜¾è‘—å˜åŒ–")
    
    if extreme_trends:
        print(f"  âš ï¸ æç«¯äº‹ä»¶å˜åŒ–: {'; '.join(extreme_trends)}")
    
    # å¼‚å¸¸ç‡é«˜çš„å˜é‡
    high_anomaly_list = [var for var, result in analysis_results['anomalies'].items() 
                        if result.anomaly_rate > 0.08]
    if high_anomaly_list:
        print(f"  ğŸš¨ é«˜å¼‚å¸¸ç‡å˜é‡: {', '.join(high_anomaly_list)}")
    
    print()
    print("âœ… åˆ†æå®Œæˆï¼AIæŠ€æœ¯æˆåŠŸè¯†åˆ«äº†å†å²æ°”å€™æ•°æ®ä¸­çš„å¤šç§æ¨¡å¼ã€è¶‹åŠ¿å’Œå¼‚å¸¸äº‹ä»¶ã€‚")

def main():
    """
    ä¸»å‡½æ•°ï¼šæ‰§è¡Œå¢å¼ºç‰ˆå†å²æ°”å€™æ¨¡å¼åˆ†æ
    """
    print("ğŸŒ å¢å¼ºç‰ˆå†å²æ°”å€™æ¨¡å¼è¯†åˆ«ä¸åˆ†æç³»ç»Ÿ")
    print("=" * 70)
    print("åˆ©ç”¨AIæŠ€æœ¯æ·±åº¦åˆ†æå†å²æ°”å€™æ•°æ®ï¼Œè¯†åˆ«æ¨¡å¼ã€è¶‹åŠ¿å’Œå¼‚å¸¸äº‹ä»¶")
    print()
    
    try:
        # 1. åˆ›å»ºçœŸå®çš„æ°”å€™æ•°æ®
        data = create_realistic_climate_data()
        
        # 2. æ‰§è¡Œå…¨é¢åˆ†æ
        analysis_results = analyze_climate_patterns(data)
        
        # 3. åˆ›å»ºå¯è§†åŒ–
        create_comprehensive_visualizations(data, analysis_results)
        
        # 4. ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
        generate_analysis_summary(data, analysis_results)
        
        # 5. ä¿å­˜è¯¦ç»†ç»“æœ
        output_dir = "outputs/climate_analysis"
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜æ•°æ®
        data.to_csv(f"{output_dir}/simulated_climate_data.csv", index=False, encoding='utf-8-sig')
        
        # ä¿å­˜åˆ†æç»“æœæ‘˜è¦
        with open(f"{output_dir}/enhanced_analysis_summary.txt", 'w', encoding='utf-8') as f:
            f.write("å¢å¼ºç‰ˆå†å²æ°”å€™æ¨¡å¼åˆ†ææ‘˜è¦\n")
            f.write("=" * 50 + "\n\n")
            
            # å†™å…¥å…³é”®ç»Ÿè®¡ä¿¡æ¯
            f.write(f"æ•°æ®æ—¶é—´è·¨åº¦: {data['time'].min().year}-{data['time'].max().year}\n")
            f.write(f"æ•°æ®ç‚¹æ•°é‡: {len(data)} ä¸ªæœˆ\n")
            f.write(f"åˆ†æå˜é‡: {', '.join(data.columns[1:])}\n\n")
            
            # è¶‹åŠ¿åˆ†æç»“æœ
            f.write("æ˜¾è‘—è¶‹åŠ¿:\n")
            for var, result in analysis_results['trends'].items():
                if result.significance == 'significant':
                    f.write(f"  {var}: {result.trend_direction}, å¹´å˜åŒ–ç‡ {result.annual_change:.6f}\n")
            
            # å¼‚å¸¸æ£€æµ‹ç»“æœ
            f.write("\nå¼‚å¸¸æ£€æµ‹:\n")
            for var, result in analysis_results['anomalies'].items():
                if result.total_anomalies > 0:
                    f.write(f"  {var}: {result.total_anomalies} ä¸ªå¼‚å¸¸ç‚¹ ({result.anomaly_rate:.2%})\n")
            
            # æç«¯äº‹ä»¶
            f.write("\næç«¯äº‹ä»¶:\n")
            for event_key, result in analysis_results['extremes'].items():
                if len(result.events) > 0:
                    f.write(f"  {result.variable} {result.event_type}: {len(result.events)} æ¬¡\n")
        
        logger.info(f"è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ° {output_dir} ç›®å½•")
        
        print("\nğŸ‰ å¢å¼ºç‰ˆå†å²æ°”å€™æ¨¡å¼åˆ†æå®Œæˆï¼")
        print("ğŸ“ æ‰€æœ‰ç»“æœæ–‡ä»¶å·²ä¿å­˜åˆ° outputs/climate_analysis/ ç›®å½•")
        print("ğŸ“Š ç»¼åˆå¯è§†åŒ–å›¾è¡¨å·²ç”Ÿæˆå¹¶æ˜¾ç¤º")
        
    except Exception as e:
        logger.error(f"åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()