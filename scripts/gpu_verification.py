#!/usr/bin/env python3
"""
GPUé…ç½®éªŒè¯è„šæœ¬

éªŒè¯PyTorchå’Œç›¸å…³AIåº“çš„GPUé…ç½®æ˜¯å¦æ­£ç¡®ã€‚
"""

import torch
import sys
import warnings
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

def check_pytorch_gpu():
    """æ£€æŸ¥PyTorch GPUé…ç½®"""
    print("=== PyTorch GPU é…ç½®æ£€æŸ¥ ===")
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
        
        for i in range(torch.cuda.device_count()):
            gpu_name = torch.cuda.get_device_name(i)
            gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
            print(f"GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)")
        
        # æµ‹è¯•GPUå¼ é‡æ“ä½œ
        try:
            test_tensor = torch.randn(1000, 1000).cuda()
            result = torch.mm(test_tensor, test_tensor.t())
            print(f"GPUå¼ é‡æµ‹è¯•: æˆåŠŸ (ç»“æœå½¢çŠ¶: {result.shape})")
            print(f"GPUå†…å­˜ä½¿ç”¨: {torch.cuda.memory_allocated()/1024**2:.1f}MB")
            
            # æ¸…ç†GPUå†…å­˜
            del test_tensor, result
            torch.cuda.empty_cache()
            
        except Exception as e:
            print(f"GPUå¼ é‡æµ‹è¯•: å¤±è´¥ - {e}")
    else:
        print("CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPU")
    
    print()

def check_ai_libraries():
    """æ£€æŸ¥AIåº“çš„GPUæ”¯æŒ"""
    print("=== AIåº“GPUæ”¯æŒæ£€æŸ¥ ===")
    
    # æ£€æŸ¥transformers
    try:
        import transformers
        print(f"Transformersç‰ˆæœ¬: {transformers.__version__}")
        
        # æµ‹è¯•ç®€å•çš„GPUæ“ä½œ
        if torch.cuda.is_available():
            from transformers import AutoTokenizer
            # è¿™é‡Œåªæ˜¯éªŒè¯åº“èƒ½æ­£å¸¸å¯¼å…¥ï¼Œä¸åŠ è½½å¤§æ¨¡å‹
            print("Transformers GPUæ”¯æŒ: å¯ç”¨")
        else:
            print("Transformers GPUæ”¯æŒ: CUDAä¸å¯ç”¨")
            
    except ImportError:
        print("Transformers: æœªå®‰è£…")
    except Exception as e:
        print(f"Transformersæ£€æŸ¥å¤±è´¥: {e}")
    
    # æ£€æŸ¥diffusers
    try:
        import diffusers
        print(f"Diffusersç‰ˆæœ¬: {diffusers.__version__}")
        print("Diffusers GPUæ”¯æŒ: å¯ç”¨" if torch.cuda.is_available() else "Diffusers GPUæ”¯æŒ: CUDAä¸å¯ç”¨")
    except ImportError:
        print("Diffusers: æœªå®‰è£…")
    except Exception as e:
        print(f"Diffusersæ£€æŸ¥å¤±è´¥: {e}")
    
    # æ£€æŸ¥torchvision
    try:
        import torchvision
        print(f"Torchvisionç‰ˆæœ¬: {torchvision.__version__}")
        print("Torchvision GPUæ”¯æŒ: å¯ç”¨" if torch.cuda.is_available() else "Torchvision GPUæ”¯æŒ: CUDAä¸å¯ç”¨")
    except ImportError:
        print("Torchvision: æœªå®‰è£…")
    except Exception as e:
        print(f"Torchvisionæ£€æŸ¥å¤±è´¥: {e}")
    
    print()

def check_project_models():
    """æ£€æŸ¥é¡¹ç›®æ¨¡å‹çš„GPUé…ç½®"""
    print("=== é¡¹ç›®æ¨¡å‹GPUé…ç½®æ£€æŸ¥ ===")
    
    try:
        from src.models.ecology_image_generator import EcologyImageGenerator
        
        # åˆ›å»ºç”Ÿæˆå™¨å®ä¾‹
        generator = EcologyImageGenerator()
        print(f"ç”Ÿæ€å›¾åƒç”Ÿæˆå™¨è®¾å¤‡: {generator.device}")
        print(f"ç”Ÿæˆå™¨ä½¿ç”¨GPU: {generator.device == 'cuda'}")
        
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦èƒ½æ­£ç¡®æ„å»º
        try:
            generator.build_model()
            print("æ¨¡å‹æ„å»º: æˆåŠŸ")
            
            # æ£€æŸ¥æ¨¡å‹å‚æ•°æ˜¯å¦åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
            if hasattr(generator, 'generator') and generator.generator is not None:
                model_device = next(generator.generator.parameters()).device
                print(f"æ¨¡å‹å‚æ•°è®¾å¤‡: {model_device}")
                print(f"è®¾å¤‡ä¸€è‡´æ€§: {model_device.type == generator.device}")
            
        except Exception as e:
            print(f"æ¨¡å‹æ„å»ºå¤±è´¥: {e}")
            
    except ImportError as e:
        print(f"æ— æ³•å¯¼å…¥é¡¹ç›®æ¨¡å‹: {e}")
    except Exception as e:
        print(f"é¡¹ç›®æ¨¡å‹æ£€æŸ¥å¤±è´¥: {e}")
    
    print()

def check_gpu_memory():
    """æ£€æŸ¥GPUå†…å­˜ä½¿ç”¨æƒ…å†µ"""
    if not torch.cuda.is_available():
        return
        
    print("=== GPUå†…å­˜ä½¿ç”¨æƒ…å†µ ===")
    
    for i in range(torch.cuda.device_count()):
        total_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
        allocated_memory = torch.cuda.memory_allocated(i) / 1024**3
        cached_memory = torch.cuda.memory_reserved(i) / 1024**3
        
        print(f"GPU {i}:")
        print(f"  æ€»å†…å­˜: {total_memory:.1f}GB")
        print(f"  å·²åˆ†é…: {allocated_memory:.3f}GB")
        print(f"  å·²ç¼“å­˜: {cached_memory:.3f}GB")
        print(f"  å¯ç”¨å†…å­˜: {total_memory - cached_memory:.1f}GB")
    
    print()

def main():
    """ä¸»å‡½æ•°"""
    print("GPUé…ç½®éªŒè¯è„šæœ¬")
    print("=" * 50)
    
    # æŠ‘åˆ¶ä¸€äº›è­¦å‘Š
    warnings.filterwarnings("ignore", category=UserWarning)
    
    check_pytorch_gpu()
    check_ai_libraries()
    check_project_models()
    check_gpu_memory()
    
    print("éªŒè¯å®Œæˆ!")
    
    # æä¾›å»ºè®®
    if torch.cuda.is_available():
        print("\nâœ… GPUé…ç½®æ­£å¸¸ï¼Œå¯ä»¥ä½¿ç”¨GPUåŠ é€Ÿè®­ç»ƒå’Œæ¨ç†ã€‚")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ¶æ„å…¼å®¹æ€§è­¦å‘Š
        try:
            test_tensor = torch.randn(10, 10).cuda()
            del test_tensor
            torch.cuda.empty_cache()
        except Exception:
            pass
            
        print("\nğŸ’¡ å»ºè®®:")
        print("- ç¡®ä¿åœ¨è®­ç»ƒæ—¶è®¾ç½®é€‚å½“çš„batch_sizeä»¥å……åˆ†åˆ©ç”¨GPUå†…å­˜")
        print("- ä½¿ç”¨mixed precisionè®­ç»ƒå¯ä»¥è¿›ä¸€æ­¥æå‡æ€§èƒ½")
        print("- å®šæœŸæ¸…ç†GPUå†…å­˜: torch.cuda.empty_cache()")
    else:
        print("\nâš ï¸  CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUè¿›è¡Œè®¡ç®—ã€‚")
        print("\nğŸ’¡ å¦‚éœ€GPUåŠ é€Ÿï¼Œè¯·æ£€æŸ¥:")
        print("- NVIDIAé©±åŠ¨æ˜¯å¦æ­£ç¡®å®‰è£…")
        print("- CUDAå·¥å…·åŒ…æ˜¯å¦å®‰è£…")
        print("- PyTorchæ˜¯å¦ä¸ºGPUç‰ˆæœ¬")

if __name__ == "__main__":
    main()