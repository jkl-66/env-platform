#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å†å²æ°”å€™æ¨¡å¼è¯†åˆ«ä¸åˆ†æç¤ºä¾‹

æœ¬è„šæœ¬æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨AIæŠ€æœ¯è¾…åŠ©ç ”ç©¶äººå‘˜è¯†åˆ«å†å²æ°”å€™æ¨¡å¼ã€è¶‹åŠ¿å’Œå¼‚å¸¸äº‹ä»¶ã€‚
å®ç°äº†ä»¥ä¸‹åŠŸèƒ½ï¼š
1. è¶‹åŠ¿åˆ†æä¸å‘¨æœŸæ€§æ£€æµ‹
2. å¼‚å¸¸äº‹ä»¶æ£€æµ‹
3. æ¨¡å¼è¯†åˆ«
4. æç«¯äº‹ä»¶åˆ†æ
5. ç»¼åˆæŠ¥å‘Šç”Ÿæˆ
"""

import sys
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.models.historical_climate_analyzer import (
    HistoricalClimateAnalyzer,
    AnomalyMethod,
    TrendMethod,
    analyze_climate_data,
    detect_climate_anomalies
)
from src.data_processing.grib_processor import GRIBProcessor
from src.utils.logger import setup_logger

# è®¾ç½®æ—¥å¿—
logger = setup_logger(__name__)

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

def load_climate_data():
    """
    åŠ è½½å†å²æ°”å€™æ•°æ®
    """
    logger.info("æ­£åœ¨åŠ è½½å†å²æ°”å€™æ•°æ®...")
    
    # å°è¯•ä»GRIBæ–‡ä»¶åŠ è½½æ•°æ®
    grib_file = "data/raw/6cd7cc57755a5204a65bc7db615cd36b.grib"
    
    if os.path.exists(grib_file):
        try:
            processor = GRIBProcessor()
            df = processor.process_grib_to_dataframe(grib_file, sample_size=1000)
            
            # ç¡®ä¿æœ‰æ—¶é—´åˆ—
            if 'time' not in df.columns and 'valid_time' in df.columns:
                df['time'] = df['valid_time']
            elif 'time' not in df.columns:
                # åˆ›å»ºè™šæ‹Ÿæ—¶é—´åºåˆ—
                start_date = datetime(2020, 1, 1)
                df['time'] = [start_date + timedelta(hours=6*i) for i in range(len(df))]
            
            logger.info(f"ä»GRIBæ–‡ä»¶åŠ è½½äº† {len(df)} æ¡è®°å½•")
            return df
            
        except Exception as e:
            logger.warning(f"æ— æ³•åŠ è½½GRIBæ–‡ä»¶: {e}ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
    
    # åˆ›å»ºæ¨¡æ‹Ÿçš„å†å²æ°”å€™æ•°æ®
    logger.info("åˆ›å»ºæ¨¡æ‹Ÿå†å²æ°”å€™æ•°æ®...")
    
    # åˆ›å»º30å¹´çš„æœˆåº¦æ•°æ®
    start_date = datetime(1990, 1, 1)
    end_date = datetime(2023, 12, 31)
    dates = pd.date_range(start_date, end_date, freq='M')
    
    np.random.seed(42)
    n_points = len(dates)
    
    # æ¨¡æ‹Ÿæ¸©åº¦æ•°æ®ï¼ˆå…¨çƒå˜æš–è¶‹åŠ¿ + å­£èŠ‚æ€§ + å™ªå£°ï¼‰
    years = np.arange(n_points) / 12
    warming_trend = 0.02 * years  # æ¯å¹´0.02åº¦çš„å‡æ¸©
    seasonal_temp = 10 * np.sin(2 * np.pi * np.arange(n_points) / 12)  # å­£èŠ‚æ€§å˜åŒ–
    temp_noise = np.random.normal(0, 2, n_points)
    temperature = 15 + warming_trend + seasonal_temp + temp_noise
    
    # æ·»åŠ æç«¯äº‹ä»¶
    # çƒ­æµªäº‹ä»¶
    heatwave_indices = np.random.choice(n_points, 20, replace=False)
    temperature[heatwave_indices] += np.random.uniform(8, 15, 20)
    
    # å¯’æ½®äº‹ä»¶
    coldwave_indices = np.random.choice(n_points, 15, replace=False)
    temperature[coldwave_indices] -= np.random.uniform(8, 12, 15)
    
    # æ¨¡æ‹Ÿé™æ°´æ•°æ®ï¼ˆå¸¦å¹²æ—±å’Œæ´ªæ¶ï¼‰
    base_precip = 80 + 30 * np.sin(2 * np.pi * np.arange(n_points) / 12)  # å­£èŠ‚æ€§é™æ°´
    precip_noise = np.random.gamma(2, 20, n_points)
    precipitation = base_precip + precip_noise
    
    # å¹²æ—±äº‹ä»¶ï¼ˆè¿ç»­ä½é™æ°´ï¼‰
    drought_start = np.random.choice(n_points - 6, 5)
    for start in drought_start:
        precipitation[start:start+6] *= 0.2
    
    # æ´ªæ¶äº‹ä»¶ï¼ˆæç«¯é™æ°´ï¼‰
    flood_indices = np.random.choice(n_points, 10, replace=False)
    precipitation[flood_indices] += np.random.uniform(200, 400, 10)
    
    # æ¨¡æ‹Ÿæ¹¿åº¦æ•°æ®
    humidity = 60 + 20 * np.sin(2 * np.pi * np.arange(n_points) / 12) + np.random.normal(0, 5, n_points)
    humidity = np.clip(humidity, 0, 100)
    
    # æ¨¡æ‹Ÿé£é€Ÿæ•°æ®
    wind_speed = 8 + 3 * np.sin(2 * np.pi * np.arange(n_points) / 6) + np.random.exponential(2, n_points)
    
    # æ¨¡æ‹Ÿæ°”å‹æ•°æ®
    pressure = 1013 + 10 * np.sin(2 * np.pi * np.arange(n_points) / 12) + np.random.normal(0, 8, n_points)
    
    # åˆ›å»ºDataFrame
    climate_data = pd.DataFrame({
        'time': dates,
        'temperature': temperature,
        'precipitation': precipitation,
        'humidity': humidity,
        'wind_speed': wind_speed,
        'pressure': pressure
    })
    
    logger.info(f"åˆ›å»ºäº†åŒ…å« {len(climate_data)} æ¡è®°å½•çš„æ¨¡æ‹Ÿæ°”å€™æ•°æ®")
    return climate_data

def analyze_trends_and_seasonality(analyzer, data):
    """
    åˆ†æè¶‹åŠ¿å’Œå­£èŠ‚æ€§
    """
    logger.info("=== å¼€å§‹è¶‹åŠ¿åˆ†æä¸å‘¨æœŸæ€§æ£€æµ‹ ===")
    
    # è¶‹åŠ¿åˆ†æ
    trend_results = analyzer.analyze_trends(data)
    
    print("\nğŸ“ˆ è¶‹åŠ¿åˆ†æç»“æœ:")
    for var, result in trend_results.items():
        direction_emoji = {
            'increasing': 'ğŸ“ˆ',
            'decreasing': 'ğŸ“‰',
            'stable': 'â¡ï¸'
        }.get(result.trend_direction, 'â“')
        
        significance_emoji = 'âœ…' if result.significance == 'significant' else 'âŒ'
        
        print(f"  {direction_emoji} {var}:")
        print(f"    è¶‹åŠ¿æ–¹å‘: {result.trend_direction}")
        print(f"    å¹´é™…å˜åŒ–: {result.annual_change:.4f}")
        print(f"    åå¹´å˜åŒ–: {result.decadal_change:.4f}")
        print(f"    æ˜¾è‘—æ€§: {result.significance} {significance_emoji}")
        print(f"    RÂ²: {result.r_squared:.4f}")
        print()
    
    # å­£èŠ‚æ€§åˆ†æ
    seasonality_results = analyzer.analyze_seasonality(data)
    
    print("\nğŸ”„ å­£èŠ‚æ€§åˆ†æç»“æœ:")
    for var, result in seasonality_results.items():
        seasonality_emoji = 'âœ…' if result.has_seasonality else 'âŒ'
        
        print(f"  ğŸ”„ {var}:")
        print(f"    å­£èŠ‚æ€§: {'æ˜¯' if result.has_seasonality else 'å¦'} {seasonality_emoji}")
        print(f"    å­£èŠ‚æ€§å¼ºåº¦: {result.seasonal_strength:.4f}")
        if result.dominant_periods:
            print(f"    ä¸»å¯¼å‘¨æœŸ: {result.dominant_periods}")
        print()
    
    return trend_results, seasonality_results

def detect_anomalies_and_patterns(analyzer, data):
    """
    æ£€æµ‹å¼‚å¸¸å’Œè¯†åˆ«æ¨¡å¼
    """
    logger.info("=== å¼€å§‹å¼‚å¸¸äº‹ä»¶æ£€æµ‹ ===")
    
    # å¼‚å¸¸æ£€æµ‹ - ä½¿ç”¨å¤šç§æ–¹æ³•
    methods = [AnomalyMethod.ISOLATION_FOREST, AnomalyMethod.STATISTICAL, AnomalyMethod.ONE_CLASS_SVM]
    
    all_anomaly_results = {}
    for method in methods:
        try:
            anomaly_results = analyzer.detect_anomalies(data, method=method)
            all_anomaly_results[method.value] = anomaly_results
        except Exception as e:
            logger.warning(f"å¼‚å¸¸æ£€æµ‹æ–¹æ³• {method.value} å¤±è´¥: {e}")
    
    # æ˜¾ç¤ºå¼‚å¸¸æ£€æµ‹ç»“æœ
    print("\nğŸš¨ å¼‚å¸¸äº‹ä»¶æ£€æµ‹ç»“æœ:")
    for method_name, results in all_anomaly_results.items():
        print(f"\n  æ–¹æ³•: {method_name}")
        for var, result in results.items():
            anomaly_emoji = 'ğŸš¨' if result.total_anomalies > 0 else 'âœ…'
            print(f"    {anomaly_emoji} {var}:")
            print(f"      å¼‚å¸¸ç‚¹æ•°é‡: {result.total_anomalies}")
            print(f"      å¼‚å¸¸ç‡: {result.anomaly_rate:.2%}")
            if result.total_anomalies > 0:
                print(f"      å¼‚å¸¸å€¼èŒƒå›´: {min(result.anomaly_values):.2f} ~ {max(result.anomaly_values):.2f}")
    
    # æ¨¡å¼è¯†åˆ«
    logger.info("=== å¼€å§‹æ¨¡å¼è¯†åˆ« ===")
    
    pattern_results = analyzer.identify_patterns(data)
    
    print("\nğŸ” æ¨¡å¼è¯†åˆ«ç»“æœ:")
    for var, result in pattern_results.items():
        print(f"  ğŸ” {var}:")
        print(f"    æ¨¡å¼ç±»å‹: {result.pattern_type}")
        print(f"    æ¨¡å¼å¼ºåº¦: {result.pattern_strength:.4f}")
        print(f"    ä¸»è¦æ¨¡å¼æ•°é‡: {len(result.dominant_patterns)}")
        
        for i, pattern in enumerate(result.dominant_patterns[:3]):  # æ˜¾ç¤ºå‰3ä¸ªä¸»è¦æ¨¡å¼
            print(f"      æ¨¡å¼ {i+1}: {pattern['characteristics']} ({pattern['percentage']:.1f}%)")
        print()
    
    return all_anomaly_results, pattern_results

def analyze_extreme_events(analyzer, data):
    """
    åˆ†ææç«¯äº‹ä»¶
    """
    logger.info("=== å¼€å§‹æç«¯äº‹ä»¶åˆ†æ ===")
    
    extreme_results = analyzer.analyze_extreme_events(data)
    
    print("\nâš¡ æç«¯äº‹ä»¶åˆ†æç»“æœ:")
    for event_key, result in extreme_results.items():
        event_emoji = {
            'heatwave': 'ğŸ”¥',
            'coldwave': 'ğŸ§Š',
            'drought': 'ğŸœï¸',
            'flood': 'ğŸŒŠ'
        }
        
        event_type = result.event_type
        emoji = event_emoji.get(event_type, 'âš¡')
        
        print(f"  {emoji} {result.variable} - {event_type}:")
        print(f"    äº‹ä»¶æ•°é‡: {len(result.events)}")
        print(f"    å¹´å‡é¢‘ç‡: {result.frequency:.2f} æ¬¡/å¹´")
        
        if result.events:
            intensities = [event['intensity'] for event in result.events]
            durations = [event['duration'] for event in result.events]
            
            print(f"    å¹³å‡å¼ºåº¦: {np.mean(intensities):.2f}")
            print(f"    æœ€å¤§å¼ºåº¦: {np.max(intensities):.2f}")
            print(f"    å¹³å‡æŒç»­æ—¶é—´: {np.mean(durations):.1f} ä¸ªæ—¶é—´æ­¥")
            print(f"    æœ€é•¿æŒç»­æ—¶é—´: {np.max(durations)} ä¸ªæ—¶é—´æ­¥")
            
            # æ˜¾ç¤ºå¼ºåº¦å’ŒæŒç»­æ—¶é—´è¶‹åŠ¿
            if result.intensity_trend.significance == 'significant':
                trend_emoji = 'ğŸ“ˆ' if result.intensity_trend.trend_direction == 'increasing' else 'ğŸ“‰'
                print(f"    å¼ºåº¦è¶‹åŠ¿: {result.intensity_trend.trend_direction} {trend_emoji}")
            
            if result.duration_trend.significance == 'significant':
                trend_emoji = 'ğŸ“ˆ' if result.duration_trend.trend_direction == 'increasing' else 'ğŸ“‰'
                print(f"    æŒç»­æ—¶é—´è¶‹åŠ¿: {result.duration_trend.trend_direction} {trend_emoji}")
        
        print()
    
    return extreme_results

def generate_comprehensive_report(analyzer, data):
    """
    ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š
    """
    logger.info("=== ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š ===")
    
    report = analyzer.generate_comprehensive_report(data, "å†å²æ°”å€™æ•°æ®åˆ†æ")
    
    print("\nğŸ“Š ç»¼åˆåˆ†ææŠ¥å‘Š:")
    print(f"  ğŸ“… æ•°æ®é›†: {report.dataset_name}")
    print(f"  ğŸ“… åˆ†ææ—¶é—´: {report.analysis_date.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"  ğŸ“… æ•°æ®æ—¶é—´èŒƒå›´: {report.time_period[0]} è‡³ {report.time_period[1]}")
    print(f"  ğŸ“Š åˆ†æå˜é‡æ•°: {report.summary['total_variables']}")
    print(f"  ğŸ“ˆ æ˜¾è‘—è¶‹åŠ¿æ•°: {report.summary['significant_trends']}")
    print(f"  ğŸ”„ å­£èŠ‚æ€§å˜é‡æ•°: {report.summary['variables_with_seasonality']}")
    print(f"  ğŸš¨ æ€»å¼‚å¸¸ç‚¹æ•°: {report.summary['total_anomalies']}")
    print(f"  âš¡ æ€»æç«¯äº‹ä»¶æ•°: {report.summary['total_extreme_events']}")
    print(f"  ğŸ”— å¼ºç›¸å…³æ€§æ•°: {report.summary['strong_correlations']}")
    
    print("\nğŸ” å…³é”®å‘ç°:")
    for finding in report.summary['key_findings']:
        print(f"    â€¢ {finding}")
    
    # æ˜¾ç¤ºå˜é‡é—´ç›¸å…³æ€§
    if report.correlations:
        print("\nğŸ”— å˜é‡é—´ç›¸å…³æ€§ (|r| > 0.5):")
        strong_corrs = {k: v for k, v in report.correlations.items() if abs(v) > 0.5}
        for pair, corr in sorted(strong_corrs.items(), key=lambda x: abs(x[1]), reverse=True):
            corr_emoji = 'ğŸ”´' if abs(corr) > 0.8 else 'ğŸŸ¡' if abs(corr) > 0.6 else 'ğŸŸ¢'
            print(f"    {corr_emoji} {pair}: {corr:.3f}")
    
    return report

def create_visualizations(data, trend_results, anomaly_results, extreme_results):
    """
    åˆ›å»ºå¯è§†åŒ–å›¾è¡¨
    """
    logger.info("åˆ›å»ºå¯è§†åŒ–å›¾è¡¨...")
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“å’Œå›¾è¡¨æ ·å¼
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS', 'DejaVu Sans', 'sans-serif']
    plt.rcParams['axes.unicode_minus'] = False
    plt.rcParams['font.size'] = 10
    plt.rcParams['figure.titlesize'] = 14
    plt.rcParams['axes.titlesize'] = 12
    plt.rcParams['axes.labelsize'] = 10
    plt.rcParams['xtick.labelsize'] = 9
    plt.rcParams['ytick.labelsize'] = 9
    plt.rcParams['legend.fontsize'] = 9
    plt.style.use('seaborn-v0_8')
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('å†å²æ°”å€™æ¨¡å¼åˆ†æå¯è§†åŒ–', fontsize=16, fontweight='bold')
    
    # 1. æ¸©åº¦è¶‹åŠ¿å›¾
    ax1 = axes[0, 0]
    if 'temperature' in data.columns:
        ax1.plot(data['time'], data['temperature'], alpha=0.7, linewidth=1)
        
        # æ·»åŠ è¶‹åŠ¿çº¿
        if 'temperature' in trend_results:
            trend = trend_results['temperature']
            x_numeric = np.arange(len(data))
            trend_line = trend.slope * x_numeric + trend.intercept
            ax1.plot(data['time'], trend_line, 'r--', linewidth=2, 
                    # ç¡®ä¿æ•°å­—æ ¼å¼åŒ–æ­£ç¡®æ˜¾ç¤º
                slope_str = f"{trend.slope:.4f}".replace('.', '.')
                label=f'è¶‹åŠ¿çº¿ (æ–œç‡: {slope_str})')
        
        ax1.set_title('æ¸©åº¦å˜åŒ–è¶‹åŠ¿')
        ax1.set_ylabel('æ¸©åº¦ (Â°C)')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
    
    # 2. é™æ°´å¼‚å¸¸æ£€æµ‹
    ax2 = axes[0, 1]
    if 'precipitation' in data.columns:
        ax2.plot(data['time'], data['precipitation'], alpha=0.7, linewidth=1, label='é™æ°´é‡')
        
        # æ ‡è®°å¼‚å¸¸ç‚¹
        if 'isolation_forest' in anomaly_results and 'precipitation' in anomaly_results['isolation_forest']:
            anomaly_result = anomaly_results['isolation_forest']['precipitation']
            if anomaly_result.anomaly_indices:
                anomaly_times = data['time'].iloc[anomaly_result.anomaly_indices]
                anomaly_values = [data['precipitation'].iloc[i] for i in anomaly_result.anomaly_indices]
                ax2.scatter(anomaly_times, anomaly_values, color='red', s=50, 
                           # ç¡®ä¿æ•°å­—æ ¼å¼åŒ–æ­£ç¡®æ˜¾ç¤º
                anomaly_count = len(anomaly_result.anomaly_indices)
                label=f'å¼‚å¸¸ç‚¹ ({anomaly_count}ä¸ª)', zorder=5)
        
        ax2.set_title('é™æ°´é‡å¼‚å¸¸æ£€æµ‹')
        ax2.set_ylabel('é™æ°´é‡ (mm)')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
    
    # 3. å¤šå˜é‡ç›¸å…³æ€§çƒ­å›¾
    ax3 = axes[1, 0]
    numeric_cols = data.select_dtypes(include=[np.number]).columns
    if len(numeric_cols) > 1:
        corr_matrix = data[numeric_cols].corr()
        im = ax3.imshow(corr_matrix, cmap='RdBu_r', aspect='auto', vmin=-1, vmax=1)
        ax3.set_xticks(range(len(corr_matrix.columns)))
        ax3.set_yticks(range(len(corr_matrix.columns)))
        ax3.set_xticklabels(corr_matrix.columns, rotation=45, ha='right')
        ax3.set_yticklabels(corr_matrix.columns)
        ax3.set_title('å˜é‡ç›¸å…³æ€§çŸ©é˜µ')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i in range(len(corr_matrix.columns)):
            for j in range(len(corr_matrix.columns)):
                text = ax3.text(j, i, f'{corr_matrix.iloc[i, j]:.2f}',
                               ha="center", va="center", color="black", fontsize=8)
        
        plt.colorbar(im, ax=ax3, shrink=0.8)
    
    # 4. æç«¯äº‹ä»¶ç»Ÿè®¡
    ax4 = axes[1, 1]
    if extreme_results:
        event_counts = {}
        for key, result in extreme_results.items():
            event_type = result.event_type
            if event_type not in event_counts:
                event_counts[event_type] = 0
            event_counts[event_type] += len(result.events)
        
        if event_counts:
            events = list(event_counts.keys())
            counts = list(event_counts.values())
            colors = ['#ff6b6b', '#4ecdc4', '#45b7d1', '#96ceb4']
            
            bars = ax4.bar(events, counts, color=colors[:len(events)])
            ax4.set_title('æç«¯äº‹ä»¶ç»Ÿè®¡')
            ax4.set_ylabel('äº‹ä»¶æ•°é‡')
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, count in zip(bars, counts):
                height = bar.get_height()
                ax4.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                        f'{count}', ha='center', va='bottom')
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾è¡¨
    output_dir = "outputs/climate_analysis"
    os.makedirs(output_dir, exist_ok=True)
    
    plt.savefig(f"{output_dir}/climate_pattern_analysis.png", dpi=300, bbox_inches='tight')
    logger.info(f"å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ° {output_dir}/climate_pattern_analysis.png")
    
    plt.show()

def save_results_to_files(report, output_dir="outputs/climate_analysis"):
    """
    ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶
    """
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜ç»¼åˆæŠ¥å‘Š
    report_file = f"{output_dir}/comprehensive_climate_report.txt"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(f"å†å²æ°”å€™æ¨¡å¼åˆ†æç»¼åˆæŠ¥å‘Š\n")
        f.write(f"{'='*50}\n\n")
        f.write(f"æ•°æ®é›†: {report.dataset_name}\n")
        f.write(f"åˆ†ææ—¶é—´: {report.analysis_date}\n")
        f.write(f"æ•°æ®æ—¶é—´èŒƒå›´: {report.time_period[0]} è‡³ {report.time_period[1]}\n\n")
        
        f.write(f"åˆ†ææ‘˜è¦:\n")
        f.write(f"  åˆ†æå˜é‡æ•°: {report.summary['total_variables']}\n")
        f.write(f"  æ˜¾è‘—è¶‹åŠ¿æ•°: {report.summary['significant_trends']}\n")
        f.write(f"  å­£èŠ‚æ€§å˜é‡æ•°: {report.summary['variables_with_seasonality']}\n")
        f.write(f"  æ€»å¼‚å¸¸ç‚¹æ•°: {report.summary['total_anomalies']}\n")
        f.write(f"  æ€»æç«¯äº‹ä»¶æ•°: {report.summary['total_extreme_events']}\n")
        f.write(f"  å¼ºç›¸å…³æ€§æ•°: {report.summary['strong_correlations']}\n\n")
        
        f.write(f"å…³é”®å‘ç°:\n")
        for finding in report.summary['key_findings']:
            f.write(f"  â€¢ {finding}\n")
    
    # ä¿å­˜è¯¦ç»†çš„è¶‹åŠ¿åˆ†æç»“æœ
    trend_file = f"{output_dir}/trend_analysis_results.csv"
    trend_data = []
    for var, result in report.trend_results.items():
        trend_data.append({
            'variable': var,
            'trend_direction': result.trend_direction,
            'slope': result.slope,
            'r_squared': result.r_squared,
            'p_value': result.p_value,
            'significance': result.significance,
            'annual_change': result.annual_change,
            'decadal_change': result.decadal_change
        })
    
    pd.DataFrame(trend_data).to_csv(trend_file, index=False, encoding='utf-8-sig')
    
    # ä¿å­˜å¼‚å¸¸æ£€æµ‹ç»“æœ
    anomaly_file = f"{output_dir}/anomaly_detection_results.csv"
    anomaly_data = []
    for var, result in report.anomaly_results.items():
        anomaly_data.append({
            'variable': var,
            'method': result.method.value,
            'total_anomalies': result.total_anomalies,
            'anomaly_rate': result.anomaly_rate,
            'threshold': result.threshold
        })
    
    pd.DataFrame(anomaly_data).to_csv(anomaly_file, index=False, encoding='utf-8-sig')
    
    logger.info(f"åˆ†æç»“æœå·²ä¿å­˜åˆ° {output_dir} ç›®å½•")

def main():
    """
    ä¸»å‡½æ•°ï¼šæ‰§è¡Œå®Œæ•´çš„å†å²æ°”å€™æ¨¡å¼åˆ†æ
    """
    print("ğŸŒ å†å²æ°”å€™æ¨¡å¼è¯†åˆ«ä¸åˆ†æç³»ç»Ÿ")
    print("=" * 50)
    print("åˆ©ç”¨AIæŠ€æœ¯è¾…åŠ©ç ”ç©¶äººå‘˜è¯†åˆ«å†å²æ°”å€™æ¨¡å¼ã€è¶‹åŠ¿å’Œå¼‚å¸¸äº‹ä»¶")
    print()
    
    try:
        # 1. åŠ è½½æ•°æ®
        data = load_climate_data()
        print(f"âœ… æˆåŠŸåŠ è½½ {len(data)} æ¡æ°”å€™æ•°æ®è®°å½•")
        print(f"ğŸ“Š æ•°æ®å˜é‡: {list(data.columns)}")
        print(f"ğŸ“… æ—¶é—´èŒƒå›´: {data['time'].min()} è‡³ {data['time'].max()}")
        print()
        
        # 2. åˆå§‹åŒ–åˆ†æå™¨
        analyzer = HistoricalClimateAnalyzer()
        
        # 3. è¶‹åŠ¿åˆ†æä¸å‘¨æœŸæ€§æ£€æµ‹
        trend_results, seasonality_results = analyze_trends_and_seasonality(analyzer, data)
        
        # 4. å¼‚å¸¸äº‹ä»¶æ£€æµ‹å’Œæ¨¡å¼è¯†åˆ«
        anomaly_results, pattern_results = detect_anomalies_and_patterns(analyzer, data)
        
        # 5. æç«¯äº‹ä»¶åˆ†æ
        extreme_results = analyze_extreme_events(analyzer, data)
        
        # 6. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        report = generate_comprehensive_report(analyzer, data)
        
        # 7. åˆ›å»ºå¯è§†åŒ–
        create_visualizations(data, trend_results, anomaly_results, extreme_results)
        
        # 8. ä¿å­˜ç»“æœ
        save_results_to_files(report)
        
        print("\nğŸ‰ å†å²æ°”å€™æ¨¡å¼åˆ†æå®Œæˆï¼")
        print("ğŸ“ ç»“æœæ–‡ä»¶å·²ä¿å­˜åˆ° outputs/climate_analysis/ ç›®å½•")
        print("ğŸ“Š å¯è§†åŒ–å›¾è¡¨å·²ç”Ÿæˆå¹¶æ˜¾ç¤º")
        
    except Exception as e:
        logger.error(f"åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()