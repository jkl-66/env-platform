import os
import sys
import subprocess
import platform
import argparse
import logging
from pathlib import Path
import json
import urllib.request
import shutil

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("deploy_image_generation")

# å®šä¹‰å¸¸é‡
FOOOCUS_REPO = "https://github.com/lllyasviel/Fooocus.git"
COMFYUI_REPO = "https://github.com/comfyanonymous/ComfyUI.git"

# æœ¬åœ°æ¨¡å‹é…ç½®
LOCAL_MODELS = {
    "stable_diffusion_v1_5": {
        "name": "Stable Diffusion 3.5 Large",
        "model_id": "stabilityai/stable-diffusion-3.5-large",
        "size": "çº¦8GB",
        "description": "æœ€æ–°çš„Stable Diffusion 3.5æ¨¡å‹ï¼Œå›¾åƒè´¨é‡å’Œç»†èŠ‚è¡¨ç°ä¼˜ç§€"
    },
    "stable_diffusion_v2_1": {
        "name": "Stable Diffusion v2.1",
        "model_id": "stabilityai/stable-diffusion-2-1",
        "size": "çº¦5GB",
        "description": "æ”¹è¿›ç‰ˆæœ¬ï¼Œå›¾åƒè´¨é‡æ›´é«˜"
    },
    "stable_diffusion_xl": {
        "name": "Stable Diffusion XL",
        "model_id": "stabilityai/stable-diffusion-xl-base-1.0",
        "size": "çº¦7GB",
        "description": "æœ€æ–°ç‰ˆæœ¬ï¼Œæ”¯æŒæ›´é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆ"
    }
}


def check_gpu():
    """æ£€æŸ¥GPUæ˜¯å¦å¯ç”¨"""
    try:
        import torch
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            logger.info(f"æ£€æµ‹åˆ°GPU: {gpu_name} ({gpu_memory:.1f}GBæ˜¾å­˜)")
            return True, gpu_memory
        else:
            logger.warning("æœªæ£€æµ‹åˆ°GPUï¼Œå›¾åƒç”Ÿæˆé€Ÿåº¦å¯èƒ½è¾ƒæ…¢")
            return False, 0
    except ImportError:
        logger.warning("æœªå®‰è£…PyTorchï¼Œæ— æ³•æ£€æµ‹GPU")
        return False, 0


def check_disk_space(path, required_gb=10):
    """æ£€æŸ¥ç£ç›˜ç©ºé—´"""
    try:
        total, used, free = shutil.disk_usage(path)
        free_gb = free / (1024**3)
        if free_gb >= required_gb:
            logger.info(f"ç£ç›˜ç©ºé—´å……è¶³: {free_gb:.1f}GB å¯ç”¨")
            return True
        else:
            logger.warning(f"ç£ç›˜ç©ºé—´ä¸è¶³: éœ€è¦{required_gb}GBï¼Œä»…æœ‰{free_gb:.1f}GBå¯ç”¨")
            return False
    except Exception as e:
        logger.error(f"æ£€æŸ¥ç£ç›˜ç©ºé—´å¤±è´¥: {e}")
        return False


def install_local_dependencies():
    """å®‰è£…æœ¬åœ°æ¨¡å‹è¿è¡Œæ‰€éœ€çš„ä¾èµ–"""
    dependencies = [
        "torch",
        "torchvision", 
        "diffusers>=0.21.0",
        "transformers>=4.21.0",
        "accelerate",
        "safetensors",
        "pillow",
        "numpy"
    ]
    
    logger.info("å®‰è£…æœ¬åœ°æ¨¡å‹è¿è¡Œä¾èµ–...")
    try:
        for dep in dependencies:
            logger.info(f"å®‰è£… {dep}...")
            subprocess.check_call([sys.executable, "-m", "pip", "install", dep, "--upgrade"])
        logger.info("ä¾èµ–å®‰è£…å®Œæˆ")
        return True
    except subprocess.CalledProcessError as e:
        logger.error(f"ä¾èµ–å®‰è£…å¤±è´¥: {e}")
        return False


def download_local_model(model_key, cache_dir="./models"):
    """ä¸‹è½½æœ¬åœ°æ¨¡å‹"""
    if model_key not in LOCAL_MODELS:
        logger.error(f"æœªçŸ¥æ¨¡å‹: {model_key}")
        return False
    
    model_info = LOCAL_MODELS[model_key]
    model_id = model_info["model_id"]
    
    logger.info(f"å¼€å§‹ä¸‹è½½æ¨¡å‹: {model_info['name']} ({model_info['size']})")
    logger.info(f"æ¨¡å‹æè¿°: {model_info['description']}")
    
    try:
        # ä½¿ç”¨diffusersåº“ä¸‹è½½æ¨¡å‹
        from diffusers import StableDiffusionPipeline
        
        # åˆ›å»ºç¼“å­˜ç›®å½•
        os.makedirs(cache_dir, exist_ok=True)
        
        # ä¸‹è½½æ¨¡å‹
        logger.info("æ­£åœ¨ä¸‹è½½æ¨¡å‹æ–‡ä»¶ï¼Œè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ...")
        pipeline = StableDiffusionPipeline.from_pretrained(
            model_id,
            cache_dir=cache_dir,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
        )
        
        # ä¿å­˜åˆ°æœ¬åœ°
        local_path = os.path.join(cache_dir, model_key)
        pipeline.save_pretrained(local_path)
        
        logger.info(f"æ¨¡å‹ä¸‹è½½å®Œæˆï¼Œä¿å­˜åˆ°: {local_path}")
        return local_path
        
    except Exception as e:
        logger.error(f"æ¨¡å‹ä¸‹è½½å¤±è´¥: {e}")
        return False


def check_python_version():
    """æ£€æŸ¥Pythonç‰ˆæœ¬"""
    version = sys.version_info
    if version.major == 3 and version.minor >= 8:
        logger.info(f"Pythonç‰ˆæœ¬å…¼å®¹: {version.major}.{version.minor}.{version.micro}")
        return True
    else:
        logger.error(f"Pythonç‰ˆæœ¬ä¸å…¼å®¹: {version.major}.{version.minor}.{version.micro}ï¼Œéœ€è¦Python 3.8+")
        return False


def install_requirements(requirements_file):
    """å®‰è£…ä¾èµ–"""
    try:
        subprocess.check_call([sys.executable, "-m", "pip", "install", "-r", requirements_file])
        logger.info("ä¾èµ–å®‰è£…æˆåŠŸ")
        return True
    except subprocess.CalledProcessError as e:
        logger.error(f"ä¾èµ–å®‰è£…å¤±è´¥: {e}")
        return False


def clone_repository(repo_url, target_dir):
    """å…‹éš†ä»“åº“"""
    if os.path.exists(target_dir):
        logger.info(f"ç›®å½•å·²å­˜åœ¨: {target_dir}ï¼Œè·³è¿‡å…‹éš†")
        return True
    
    try:
        subprocess.check_call(["git", "clone", repo_url, target_dir])
        logger.info(f"ä»“åº“å…‹éš†æˆåŠŸ: {repo_url} -> {target_dir}")
        return True
    except subprocess.CalledProcessError as e:
        logger.error(f"ä»“åº“å…‹éš†å¤±è´¥: {e}")
        return False


def deploy_fooocus(install_dir, launch=True):
    """éƒ¨ç½²Fooocus"""
    install_dir = os.path.abspath(install_dir)
    
    # å…‹éš†ä»“åº“
    if not clone_repository(FOOOCUS_REPO, install_dir):
        return False
    
    # å®‰è£…ä¾èµ–
    os.chdir(install_dir)
    if platform.system() == "Windows":
        setup_script = os.path.join(install_dir, "install.bat")
        if os.path.exists(setup_script):
            try:
                logger.info("è¿è¡ŒFooocuså®‰è£…è„šæœ¬...")
                subprocess.check_call([setup_script])
                logger.info("Fooocuså®‰è£…æˆåŠŸ")
            except subprocess.CalledProcessError as e:
                logger.error(f"Fooocuså®‰è£…è„šæœ¬è¿è¡Œå¤±è´¥: {e}")
                return False
    else:  # Linux/Mac
        setup_script = os.path.join(install_dir, "install.sh")
        if os.path.exists(setup_script):
            try:
                logger.info("è¿è¡ŒFooocuså®‰è£…è„šæœ¬...")
                subprocess.check_call(["bash", setup_script])
                logger.info("Fooocuså®‰è£…æˆåŠŸ")
            except subprocess.CalledProcessError as e:
                logger.error(f"Fooocuså®‰è£…è„šæœ¬è¿è¡Œå¤±è´¥: {e}")
                return False
    
    # å¯åŠ¨Fooocus
    if launch:
        launch_fooocus(install_dir)
    
    return True


def launch_fooocus(install_dir):
    """å¯åŠ¨Fooocus"""
    os.chdir(install_dir)
    if platform.system() == "Windows":
        launch_script = os.path.join(install_dir, "run.bat")
    else:  # Linux/Mac
        launch_script = os.path.join(install_dir, "run.sh")
    
    if os.path.exists(launch_script):
        try:
            logger.info("å¯åŠ¨Fooocus...")
            if platform.system() == "Windows":
                subprocess.Popen([launch_script])
            else:
                subprocess.Popen(["bash", launch_script])
            logger.info("Fooocuså·²å¯åŠ¨ï¼Œè¯·åœ¨æµè§ˆå™¨ä¸­è®¿é—®: http://127.0.0.1:7865")
            return True
        except subprocess.CalledProcessError as e:
            logger.error(f"Fooocuså¯åŠ¨å¤±è´¥: {e}")
            return False
    else:
        logger.error(f"å¯åŠ¨è„šæœ¬ä¸å­˜åœ¨: {launch_script}")
        return False


def deploy_comfyui(install_dir, launch=True):
    """éƒ¨ç½²ComfyUI"""
    install_dir = os.path.abspath(install_dir)
    
    # å…‹éš†ä»“åº“
    if not clone_repository(COMFYUI_REPO, install_dir):
        return False
    
    # å®‰è£…ä¾èµ–
    os.chdir(install_dir)
    requirements_file = os.path.join(install_dir, "requirements.txt")
    if os.path.exists(requirements_file):
        if not install_requirements(requirements_file):
            return False
    
    # å¯åŠ¨ComfyUI
    if launch:
        launch_comfyui(install_dir)
    
    return True


def launch_comfyui(install_dir):
    """å¯åŠ¨ComfyUI"""
    os.chdir(install_dir)
    try:
        logger.info("å¯åŠ¨ComfyUI...")
        if platform.system() == "Windows":
            subprocess.Popen([sys.executable, "main.py"])
        else:
            subprocess.Popen(["python", "main.py"])
        logger.info("ComfyUIå·²å¯åŠ¨ï¼Œè¯·åœ¨æµè§ˆå™¨ä¸­è®¿é—®: http://127.0.0.1:8188")
        return True
    except subprocess.CalledProcessError as e:
        logger.error(f"ComfyUIå¯åŠ¨å¤±è´¥: {e}")
        return False


def deploy_local_model(model_key="stable_diffusion_v1_5", cache_dir="./models"):
    """éƒ¨ç½²æœ¬åœ°æ¨¡å‹ï¼ˆæ¨èæ–¹å¼ï¼‰"""
    logger.info("=== éƒ¨ç½²æœ¬åœ°å›¾åƒç”Ÿæˆæ¨¡å‹ ===")
    
    # æ£€æŸ¥ç¯å¢ƒ
    if not check_python_version():
        return False
    
    has_gpu, gpu_memory = check_gpu()
    
    # æ ¹æ®GPUæƒ…å†µæ¨èæ¨¡å‹
    if has_gpu and gpu_memory >= 8:
        recommended_model = "stable_diffusion_xl"
        logger.info("æ£€æµ‹åˆ°é«˜æ€§èƒ½GPUï¼Œæ¨èä½¿ç”¨Stable Diffusion XL")
    elif has_gpu and gpu_memory >= 6:
        recommended_model = "stable_diffusion_v2_1"
        logger.info("æ£€æµ‹åˆ°ä¸­ç­‰æ€§èƒ½GPUï¼Œæ¨èä½¿ç”¨Stable Diffusion v2.1")
    else:
        recommended_model = "stable_diffusion_v1_5"
        logger.info("æ¨èä½¿ç”¨Stable Diffusion v1.5ï¼ˆå…¼å®¹æ€§æœ€å¥½ï¼‰")
    
    # å¦‚æœç”¨æˆ·æ²¡æœ‰æŒ‡å®šæ¨¡å‹ï¼Œä½¿ç”¨æ¨èæ¨¡å‹
    if model_key == "stable_diffusion_v1_5" and recommended_model != "stable_diffusion_v1_5":
        logger.info(f"è‡ªåŠ¨é€‰æ‹©æ¨èæ¨¡å‹: {recommended_model}")
        model_key = recommended_model
    
    # æ£€æŸ¥ç£ç›˜ç©ºé—´
    required_space = 10 if model_key == "stable_diffusion_xl" else 8
    if not check_disk_space(cache_dir, required_space):
        logger.error("ç£ç›˜ç©ºé—´ä¸è¶³ï¼Œæ— æ³•ä¸‹è½½æ¨¡å‹")
        return False
    
    # å®‰è£…ä¾èµ–
    if not install_local_dependencies():
        return False
    
    # ä¸‹è½½æ¨¡å‹
    model_path = download_local_model(model_key, cache_dir)
    if not model_path:
        return False
    
    # åˆ›å»ºé…ç½®æ–‡ä»¶
    config = {
        "model_type": "local",
        "model_key": model_key,
        "model_path": model_path,
        "device": "cuda" if has_gpu else "cpu",
        "torch_dtype": "float16" if has_gpu else "float32"
    }
    
    config_path = os.path.join(cache_dir, "model_config.json")
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    
    logger.info(f"æœ¬åœ°æ¨¡å‹éƒ¨ç½²æˆåŠŸï¼é…ç½®æ–‡ä»¶: {config_path}")
    
    # æ‰“å°ä½¿ç”¨æŒ‡å—
    print("\n=== æœ¬åœ°æ¨¡å‹ä½¿ç”¨æŒ‡å— ===")
    print("\n1. ç›´æ¥ä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼ˆæ¨èï¼‰:")
    print("```python")
    print("from models.ecology_image_generator import EcologyImageGenerator")
    print("")
    print("# åˆ›å»ºç”Ÿæˆå™¨å®ä¾‹ï¼ˆè‡ªåŠ¨ä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼‰")
    print("generator = EcologyImageGenerator()")
    print("generator.build_model()  # åŠ è½½æœ¬åœ°æ¨¡å‹")
    print("")
    print("# ç”Ÿæˆå›¾åƒ")
    print("result = generator.generate_from_text('å·¥ä¸šæ±¡æŸ“å¯¼è‡´çš„æ²³æµæ±¡æŸ“åœºæ™¯', style='realistic')")
    print("```")
    
    print("\n2. æ‰‹åŠ¨æŒ‡å®šæ¨¡å‹è·¯å¾„:")
    print("```python")
    print("from models.ecology_image_generator import EcologyImageGenerator")
    print("")
    print(f"# ä½¿ç”¨æŒ‡å®šçš„æœ¬åœ°æ¨¡å‹")
    print(f"generator = EcologyImageGenerator(local_model_path='{model_path}')")
    print("result = generator.generate_from_text('æ£®æ—ç ä¼è­¦ç¤ºå›¾åƒ')")
    print("```")
    
    print("\n3. ä¼˜åŠ¿:")
    print("   âœ… å®Œå…¨æœ¬åœ°è¿è¡Œï¼Œæ— éœ€ç½‘ç»œè¿æ¥")
    print("   âœ… æ•°æ®éšç§å®‰å…¨")
    print("   âœ… ç”Ÿæˆé€Ÿåº¦å¿«ï¼ˆç‰¹åˆ«æ˜¯æœ‰GPUæ—¶ï¼‰")
    print("   âœ… æ— APIè°ƒç”¨é™åˆ¶")
    print("   âœ… å¯ç¦»çº¿ä½¿ç”¨")
    
    return True


def list_available_models():
    """åˆ—å‡ºå¯ç”¨çš„æœ¬åœ°æ¨¡å‹"""
    print("\n=== å¯ç”¨çš„æœ¬åœ°æ¨¡å‹ ===")
    for key, info in LOCAL_MODELS.items():
        print(f"\n{key}:")
        print(f"  åç§°: {info['name']}")
        print(f"  å¤§å°: {info['size']}")
        print(f"  æè¿°: {info['description']}")


def main():
    parser = argparse.ArgumentParser(description="éƒ¨ç½²å›¾åƒç”Ÿæˆå·¥å…·")
    parser.add_argument("--mode", type=str, choices=["local", "api"], default="local",
                        help="éƒ¨ç½²æ¨¡å¼: local(æœ¬åœ°æ¨¡å‹,æ¨è) æˆ– api(APIæœåŠ¡)")
    parser.add_argument("--tool", type=str, choices=["fooocus", "comfyui"], default="fooocus",
                        help="APIæ¨¡å¼ä¸‹è¦éƒ¨ç½²çš„å·¥å…·: fooocus æˆ– comfyui")
    parser.add_argument("--model", type=str, choices=list(LOCAL_MODELS.keys()), 
                        default="stable_diffusion_v1_5",
                        help="æœ¬åœ°æ¨¡å¼ä¸‹è¦ä½¿ç”¨çš„æ¨¡å‹")
    parser.add_argument("--install-dir", type=str, default="./image_generation",
                        help="å®‰è£…ç›®å½•")
    parser.add_argument("--cache-dir", type=str, default="./models",
                        help="æ¨¡å‹ç¼“å­˜ç›®å½•")
    parser.add_argument("--no-launch", action="store_true",
                        help="APIæ¨¡å¼ä¸‹å®‰è£…åä¸å¯åŠ¨")
    parser.add_argument("--list-models", action="store_true",
                        help="åˆ—å‡ºå¯ç”¨çš„æœ¬åœ°æ¨¡å‹")
    
    args = parser.parse_args()
    
    if args.list_models:
        list_available_models()
        return
    
    if args.mode == "local":
        # æœ¬åœ°æ¨¡å‹éƒ¨ç½²ï¼ˆæ¨èï¼‰
        success = deploy_local_model(args.model, args.cache_dir)
        if success:
            logger.info("ğŸ‰ æœ¬åœ°æ¨¡å‹éƒ¨ç½²æˆåŠŸï¼")
            print("\nğŸ’¡ æç¤º: æœ¬åœ°æ¨¡å‹æ˜¯æ¨èçš„éƒ¨ç½²æ–¹å¼ï¼Œå…·æœ‰æ›´å¥½çš„éšç§æ€§å’Œç¨³å®šæ€§")
        else:
            logger.error("âŒ æœ¬åœ°æ¨¡å‹éƒ¨ç½²å¤±è´¥")
            sys.exit(1)
    
    else:  # APIæ¨¡å¼
        logger.info("ä½¿ç”¨APIæ¨¡å¼éƒ¨ç½²ï¼ˆéœ€è¦é¢å¤–çš„æœåŠ¡å™¨èµ„æºï¼‰")
        
        # æ£€æŸ¥ç¯å¢ƒ
        if not check_python_version():
            sys.exit(1)
        
        check_gpu()  # åªæ˜¯æ£€æŸ¥ï¼Œä¸é˜»æ­¢å®‰è£…
        
        # åˆ›å»ºå®‰è£…ç›®å½•
        install_dir = os.path.abspath(args.install_dir)
        os.makedirs(install_dir, exist_ok=True)
        
        # éƒ¨ç½²é€‰æ‹©çš„å·¥å…·
        if args.tool == "fooocus":
            success = deploy_fooocus(os.path.join(install_dir, "fooocus"), not args.no_launch)
        else:  # comfyui
            success = deploy_comfyui(os.path.join(install_dir, "comfyui"), not args.no_launch)
        
        if success:
            logger.info(f"{args.tool.capitalize()} APIæœåŠ¡éƒ¨ç½²æˆåŠŸ!")
            
            # æ‰“å°é›†æˆæŒ‡å—
            print("\n=== APIæ¨¡å¼é›†æˆæŒ‡å— ===")
            print("```python")
            if args.tool == "fooocus":
                print("from models.ecology_image_generator import EcologyImageGenerator")
                print("")
                print("# åˆ›å»ºç”Ÿæˆå™¨å®ä¾‹å¹¶é…ç½®API")
                print("generator = EcologyImageGenerator(api_url='http://127.0.0.1:7865', api_type='fooocus')")
                print("")
                print("# ç”Ÿæˆå›¾åƒ")
                print("result = generator.generate_from_text('å·¥ä¸šæ±¡æŸ“å¯¼è‡´çš„æ²³æµæ±¡æŸ“åœºæ™¯', style='realistic')")
            else:
                print("from models.ecology_image_generator import EcologyImageGenerator")
                print("")
                print("# åˆ›å»ºç”Ÿæˆå™¨å®ä¾‹å¹¶é…ç½®API")
                print("generator = EcologyImageGenerator(api_url='http://127.0.0.1:8188', api_type='comfyui')")
                print("")
                print("# ç”Ÿæˆå›¾åƒ")
                print("result = generator.generate_from_text('å·¥ä¸šæ±¡æŸ“å¯¼è‡´çš„æ²³æµæ±¡æŸ“åœºæ™¯', style='realistic')")
            print("```")
            
            print("\nğŸ’¡ æç¤º: å¦‚æœä¸éœ€è¦Webç•Œé¢ï¼Œå»ºè®®ä½¿ç”¨ --mode local è¿›è¡Œæœ¬åœ°éƒ¨ç½²")
        else:
            logger.error(f"{args.tool.capitalize()} éƒ¨ç½²å¤±è´¥!")
            sys.exit(1)


if __name__ == "__main__":
    main()