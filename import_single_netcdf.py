#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å•ä¸ªNetCDFæ–‡ä»¶å¯¼å…¥æµ‹è¯•è„šæœ¬
"""

import asyncio
import pandas as pd
import numpy as np
from pathlib import Path
import sys
import os
from datetime import datetime
from typing import Dict, List, Tuple, Any
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# å°è¯•å¯¼å…¥é¡¹ç›®æ¨¡å—
try:
    from src.data_processing.data_storage import DataStorage
    from src.utils.logger import get_logger
    from src.utils.config import get_settings
    USE_PROJECT_MODULES = True
except ImportError as e:
    print(f"è­¦å‘Š: æ— æ³•å¯¼å…¥é¡¹ç›®æ¨¡å— ({e})ï¼Œå°†ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬")
    USE_PROJECT_MODULES = False
    
    # ç®€åŒ–çš„æ—¥å¿—è®°å½•å™¨
    import logging
    def get_logger(name):
        logger = logging.getLogger(name)
        logger.setLevel(logging.INFO)
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        return logger
    
    # ç®€åŒ–çš„è®¾ç½®
    class SimpleSettings:
        def __init__(self):
            self.database_url = "sqlite:///climate_data.db"
    
    def get_settings():
        return SimpleSettings()

logger = get_logger(__name__)
settings = get_settings()

class NetCDFImporter:
    """NetCDFæ–‡ä»¶å¯¼å…¥å™¨"""
    
    def __init__(self):
        if USE_PROJECT_MODULES:
            self.data_storage = DataStorage()
        else:
            self.data_storage = None
            # åˆ›å»ºè¾“å‡ºç›®å½•
            self.output_dir = Path('outputs') / 'netcdf_import'
            self.output_dir.mkdir(parents=True, exist_ok=True)
    
    async def initialize(self):
        """åˆå§‹åŒ–æ•°æ®å­˜å‚¨"""
        if USE_PROJECT_MODULES and self.data_storage:
            await self.data_storage.initialize()
            logger.info("æ•°æ®å­˜å‚¨åˆå§‹åŒ–å®Œæˆ")
        else:
            logger.info("ä½¿ç”¨ç®€åŒ–æ¨¡å¼ï¼Œè·³è¿‡æ•°æ®å­˜å‚¨åˆå§‹åŒ–")
    
    def load_netcdf_file(self, file_path: Path) -> pd.DataFrame:
        """åŠ è½½NetCDFæ–‡ä»¶"""
        logger.info(f"æ­£åœ¨åŠ è½½NetCDFæ–‡ä»¶: {file_path}")
        
        try:
            import xarray as xr
            
            # æ‰“å¼€NetCDFæ–‡ä»¶
            ds = xr.open_dataset(file_path)
            
            logger.info(f"NetCDFæ–‡ä»¶ä¿¡æ¯:")
            logger.info(f"  ç»´åº¦: {dict(ds.dims)}")
            logger.info(f"  å˜é‡: {list(ds.data_vars)}")
            logger.info(f"  åæ ‡: {list(ds.coords)}")
            
            # è½¬æ¢ä¸ºDataFrame
            df = ds.to_dataframe().reset_index()
            
            # ç§»é™¤NaNå€¼
            df = df.dropna()
            
            # æ·»åŠ æ–‡ä»¶ä¿¡æ¯
            df['source_file'] = file_path.name
            df['dataset_type'] = self._extract_dataset_type(file_path.name)
            df['year'] = self._extract_year(file_path.name)
            
            logger.info(f"NetCDFæ–‡ä»¶åŠ è½½å®Œæˆ: {file_path.name}")
            logger.info(f"æ•°æ®å½¢çŠ¶: {df.shape}")
            logger.info(f"åˆ—å: {list(df.columns)}")
            
            # æ˜¾ç¤ºæ•°æ®æ ·æœ¬
            if not df.empty:
                logger.info(f"æ•°æ®æ ·æœ¬:")
                logger.info(f"{df.head()}")
            
            return df
            
        except ImportError:
            logger.error("éœ€è¦å®‰è£…xarrayåº“æ¥å¤„ç†NetCDFæ–‡ä»¶")
            logger.info("è¯·è¿è¡Œ: pip install xarray netcdf4")
            return pd.DataFrame()
        except Exception as e:
            logger.error(f"åŠ è½½NetCDFæ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return pd.DataFrame()
    
    def _extract_dataset_type(self, filename: str) -> str:
        """ä»æ–‡ä»¶åæå–æ•°æ®é›†ç±»å‹"""
        if 'hot-dry-windy' in filename:
            return 'hot-dry-windy'
        elif 'hot-dry' in filename:
            return 'hot-dry'
        elif 'hot-wet' in filename:
            return 'hot-wet'
        elif 'wet-windy' in filename:
            return 'wet-windy'
        else:
            return 'unknown'
    
    def _extract_year(self, filename: str) -> int:
        """ä»æ–‡ä»¶åæå–å¹´ä»½"""
        import re
        match = re.search(r'_(\d{4})_', filename)
        if match:
            return int(match.group(1))
        return None
    
    async def import_netcdf_file(self, file_path: Path) -> str:
        """å¯¼å…¥NetCDFæ–‡ä»¶åˆ°æ•°æ®åº“"""
        logger.info(f"å¼€å§‹å¯¼å…¥NetCDFæ–‡ä»¶: {file_path}")
        
        # åŠ è½½æ•°æ®
        data = self.load_netcdf_file(file_path)
        
        if data.empty:
            logger.warning(f"æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡å¯¼å…¥: {file_path}")
            return None
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        dataset_type = self._extract_dataset_type(file_path.name)
        year = self._extract_year(file_path.name)
        
        if USE_PROJECT_MODULES and self.data_storage:
            # ä½¿ç”¨å®Œæ•´çš„æ•°æ®å­˜å‚¨ç³»ç»Ÿ
            filename = f"climate_netcdf_{dataset_type}_{year}_{timestamp}"
            
            # ä¿å­˜ä¸ºparquetæ ¼å¼
            saved_file_path = self.data_storage.save_dataframe(
                data, 
                filename, 
                data_category="processed", 
                format="parquet"
            )
            
            # åˆ›å»ºæ•°æ®è®°å½•
            record_id = await self.data_storage.save_data_record(
                source=f"Global Climate NetCDF - {dataset_type}",
                data_type="extreme_events",
                location="Global",
                start_time=datetime(year, 1, 1) if year else datetime(1951, 1, 1),
                end_time=datetime(year, 12, 31) if year else datetime(2022, 12, 31),
                file_path=saved_file_path,
                file_format="parquet",
                file_size=os.path.getsize(saved_file_path),
                variables=list(data.columns),
                data_metadata={
                    "description": f"å…¨çƒ0.5Â°æç«¯æ°”å€™äº‹ä»¶æ•°æ® - {dataset_type} ({year}å¹´)",
                    "resolution": "0.5åº¦",
                    "year": year,
                    "data_type": "extreme_climate_events",
                    "event_type": dataset_type,
                    "original_file": str(file_path),
                    "data_shape": data.shape
                }
            )
            
            logger.info(f"NetCDFæ–‡ä»¶å·²å¯¼å…¥æ•°æ®åº“ï¼Œè®°å½•ID: {record_id}")
            return record_id
        else:
            # ç®€åŒ–æ¨¡å¼ï¼šç›´æ¥ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶
            filename = f"climate_netcdf_{dataset_type}_{year}_{timestamp}.parquet"
            saved_file_path = self.output_dir / filename
            
            data.to_parquet(saved_file_path)
            
            # ä¿å­˜å…ƒæ•°æ®
            metadata = {
                "source": f"Global Climate NetCDF - {dataset_type}",
                "data_type": "extreme_events",
                "location": "Global",
                "year": year,
                "file_path": str(saved_file_path),
                "file_format": "parquet",
                "file_size": os.path.getsize(saved_file_path),
                "variables": list(data.columns),
                "description": f"å…¨çƒ0.5Â°æç«¯æ°”å€™äº‹ä»¶æ•°æ® - {dataset_type} ({year}å¹´)",
                "resolution": "0.5åº¦",
                "event_type": dataset_type,
                "original_file": str(file_path),
                "data_shape": list(data.shape)
            }
            
            metadata_path = self.output_dir / f"metadata_{dataset_type}_{year}_{timestamp}.json"
            import json
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            logger.info(f"NetCDFæ–‡ä»¶å·²ä¿å­˜åˆ°æœ¬åœ°: {saved_file_path}")
            logger.info(f"å…ƒæ•°æ®å·²ä¿å­˜: {metadata_path}")
            return f"local_file_{dataset_type}_{year}_{timestamp}"

async def main():
    """ä¸»å‡½æ•°"""
    # æµ‹è¯•å•ä¸ªNetCDFæ–‡ä»¶
    test_file = r"D:\ç”¨æˆ·\jin\æ¡Œé¢\å…¨çƒ0.5Â°é€å¹´å¤åˆæç«¯æ°”å€™äº‹ä»¶æ•°æ®é›†ï¼ˆ1951-2022å¹´ï¼‰-æ•°æ®å®ä½“\global_hot-dry_[tasmax_p_95_all_7_1]-[pr_p_5_+0_0_1]_1_1~12\global_hot-dry_1951_1~12.nc"
    
    print("ğŸš€ å¼€å§‹NetCDFæ–‡ä»¶å¯¼å…¥æµ‹è¯•...")
    print(f"ğŸ“‚ æµ‹è¯•æ–‡ä»¶: {test_file}")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(test_file).exists():
        print(f"âŒ é”™è¯¯: æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
        return
    
    # åˆ›å»ºå¯¼å…¥å™¨å¹¶è¿è¡Œå¯¼å…¥
    importer = NetCDFImporter()
    
    try:
        await importer.initialize()
        
        record_id = await importer.import_netcdf_file(Path(test_file))
        
        if record_id:
            print(f"\nâœ… NetCDFæ–‡ä»¶å¯¼å…¥æˆåŠŸ!")
            print(f"ğŸ—„ï¸ è®°å½•ID: {record_id}")
        else:
            print(f"\nâŒ NetCDFæ–‡ä»¶å¯¼å…¥å¤±è´¥")
        
    except Exception as e:
        print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        logger.error(f"å¯¼å…¥å¤±è´¥: {e}", exc_info=True)
    finally:
        if USE_PROJECT_MODULES and importer.data_storage:
            await importer.data_storage.close()

if __name__ == "__main__":
    asyncio.run(main())