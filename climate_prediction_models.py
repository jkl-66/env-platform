#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºäºXGBoostå’ŒTransformerçš„æ°”å€™é¢„æµ‹æ¨¡å‹
åŠŸèƒ½ï¼šä»MySQLæ•°æ®åº“è¯»å–æ°”è±¡æ•°æ®ï¼Œä½¿ç”¨XGBoostå’ŒTransformeræ¨¡å‹è¿›è¡Œæ°”å€™é¢„æµ‹
ä½œè€…ï¼šAIåŠ©æ‰‹
æ—¥æœŸï¼š2024
"""

import os
import sys
import json
import logging
import warnings
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any, Union
import pickle

warnings.filterwarnings('ignore')

# æ•°æ®å¤„ç†åº“
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# XGBoost
import xgboost as xgb

# PyTorchå’ŒTransformer
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, TensorDataset
from torch.nn import TransformerEncoder, TransformerEncoderLayer

# æ•°æ®åº“è¿æ¥
import pymysql
from sqlalchemy import create_engine, text
from sqlalchemy.exc import SQLAlchemyError

# å¯è§†åŒ–
import matplotlib.pyplot as plt
import seaborn as sns
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('climate_prediction.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class WeatherDataLoader:
    """
    æ°”è±¡æ•°æ®åŠ è½½å™¨ - ä»MySQLæ•°æ®åº“åŠ è½½æ•°æ®
    """
    
    def __init__(self, mysql_config: Dict[str, str]):
        """
        åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        
        Args:
            mysql_config: MySQLæ•°æ®åº“è¿æ¥é…ç½®
        """
        self.mysql_config = mysql_config
        self.engine = None
        self._init_database_connection()
    
    def _init_database_connection(self):
        """
        åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
        """
        try:
            connection_string = (
                f"mysql+pymysql://{self.mysql_config['user']}:"
                f"{self.mysql_config['password']}@{self.mysql_config['host']}:"
                f"{self.mysql_config['port']}/{self.mysql_config['database']}"
            )
            
            self.engine = create_engine(
                connection_string,
                pool_pre_ping=True,
                pool_recycle=3600,
                echo=False
            )
            
            # æµ‹è¯•è¿æ¥
            with self.engine.connect() as conn:
                conn.execute(text("SELECT 1"))
                
            logger.info("âœ… MySQLæ•°æ®åº“è¿æ¥æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ MySQLæ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            raise
    
    def load_temperature_data(self, 
                            lat_range: Tuple[float, float] = (30.67, 31.88),  # ä¸Šæµ·åœ°åŒºçº¬åº¦èŒƒå›´
                            lon_range: Tuple[float, float] = (120.87, 122.2),  # ä¸Šæµ·åœ°åŒºç»åº¦èŒƒå›´
                            start_date: Optional[str] = None,
                            end_date: Optional[str] = None) -> pd.DataFrame:
        """
        ä»æ•°æ®åº“åŠ è½½æŒ‡å®šåŒºåŸŸçš„æ¸©åº¦æ•°æ®
        
        Args:
            lat_range: çº¬åº¦èŒƒå›´ (min_lat, max_lat)
            lon_range: ç»åº¦èŒƒå›´ (min_lon, max_lon)
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            
        Returns:
            pd.DataFrame: æ¸©åº¦æ•°æ®
        """
        logger.info(f"ğŸŒ¡ï¸ æ­£åœ¨åŠ è½½æ¸©åº¦æ•°æ®...")
        logger.info(f"ğŸ“ çº¬åº¦èŒƒå›´: {lat_range[0]}Â° - {lat_range[1]}Â°")
        logger.info(f"ğŸ“ ç»åº¦èŒƒå›´: {lon_range[0]}Â° - {lon_range[1]}Â°")
        
        # æ„å»ºæŸ¥è¯¢SQL
        conditions = [
            "variable = 't2m'",  # 2ç±³æ°”æ¸©
            f"latitude BETWEEN {lat_range[0]} AND {lat_range[1]}",
            f"longitude BETWEEN {lon_range[0]} AND {lon_range[1]}"
        ]
        
        if start_date:
            conditions.append(f"time >= '{start_date}'")
        if end_date:
            conditions.append(f"time <= '{end_date}'")
        
        where_clause = " AND ".join(conditions)
        
        sql = f"""
        SELECT time, latitude, longitude, value as temperature
        FROM grib_weather_data
        WHERE {where_clause}
        ORDER BY time, latitude, longitude
        """
        
        try:
            with self.engine.connect() as conn:
                df = pd.read_sql(sql, conn)
            
            if df.empty:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ¸©åº¦æ•°æ®ï¼Œç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®")
                df = self._generate_mock_temperature_data(lat_range, lon_range)
            
            # æ•°æ®é¢„å¤„ç†
            df['time'] = pd.to_datetime(df['time'])
            df['temperature_celsius'] = df['temperature'] - 273.15  # è½¬æ¢ä¸ºæ‘„æ°åº¦
            
            logger.info(f"âœ… åŠ è½½å®Œæˆï¼Œå…± {len(df)} æ¡è®°å½•")
            logger.info(f"ğŸ“Š æ—¶é—´èŒƒå›´: {df['time'].min()} - {df['time'].max()}")
            logger.info(f"ğŸ“Š æ¸©åº¦èŒƒå›´: {df['temperature_celsius'].min():.1f}Â°C - {df['temperature_celsius'].max():.1f}Â°C")
            
            return df
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            logger.info("ğŸ”„ ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ä½œä¸ºæ›¿ä»£")
            return self._generate_mock_temperature_data(lat_range, lon_range)
    
    def _generate_mock_temperature_data(self, 
                                      lat_range: Tuple[float, float],
                                      lon_range: Tuple[float, float]) -> pd.DataFrame:
        """
        ç”Ÿæˆæ¨¡æ‹Ÿæ¸©åº¦æ•°æ®
        """
        logger.info("ğŸ”„ æ­£åœ¨ç”Ÿæˆæ¨¡æ‹Ÿæ¸©åº¦æ•°æ®...")
        
        # æ—¶é—´èŒƒå›´ï¼šæœ€è¿‘3å¹´
        start_date = datetime(2022, 1, 1)
        end_date = datetime(2024, 12, 31)
        date_range = pd.date_range(start_date, end_date, freq='D')
        
        # ç©ºé—´ç½‘æ ¼
        lat_points = np.linspace(lat_range[0], lat_range[1], 10)
        lon_points = np.linspace(lon_range[0], lon_range[1], 10)
        
        data = []
        np.random.seed(42)
        
        for date in date_range:
            for lat in lat_points:
                for lon in lon_points:
                    # åŸºç¡€æ¸©åº¦ï¼ˆè€ƒè™‘å­£èŠ‚æ€§ï¼‰
                    day_of_year = date.dayofyear
                    seasonal_temp = 15 + 10 * np.sin(2 * np.pi * (day_of_year - 80) / 365)
                    
                    # æ·»åŠ éšæœºå™ªå£°
                    noise = np.random.normal(0, 2)
                    temperature_k = seasonal_temp + 273.15 + noise
                    
                    data.append({
                        'time': date,
                        'latitude': lat,
                        'longitude': lon,
                        'temperature': temperature_k,
                        'temperature_celsius': temperature_k - 273.15
                    })
        
        df = pd.DataFrame(data)
        logger.info(f"âœ… ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®å®Œæˆï¼Œå…± {len(df)} æ¡è®°å½•")
        
        return df

class WeatherTransformer(nn.Module):
    """
    åŸºäºTransformerçš„æ°”å€™é¢„æµ‹æ¨¡å‹
    """
    
    def __init__(self, 
                 input_dim: int = 10,
                 d_model: int = 128,
                 nhead: int = 8,
                 num_layers: int = 6,
                 dim_feedforward: int = 512,
                 dropout: float = 0.1,
                 max_seq_length: int = 365):
        """
        åˆå§‹åŒ–Transformeræ¨¡å‹
        
        Args:
            input_dim: è¾“å…¥ç‰¹å¾ç»´åº¦
            d_model: æ¨¡å‹ç»´åº¦
            nhead: æ³¨æ„åŠ›å¤´æ•°
            num_layers: ç¼–ç å™¨å±‚æ•°
            dim_feedforward: å‰é¦ˆç½‘ç»œç»´åº¦
            dropout: Dropoutç‡
            max_seq_length: æœ€å¤§åºåˆ—é•¿åº¦
        """
        super(WeatherTransformer, self).__init__()
        
        self.d_model = d_model
        self.max_seq_length = max_seq_length
        
        # è¾“å…¥æŠ•å½±å±‚
        self.input_projection = nn.Linear(input_dim, d_model)
        
        # ä½ç½®ç¼–ç 
        self.positional_encoding = self._create_positional_encoding(max_seq_length, d_model)
        
        # Transformerç¼–ç å™¨
        encoder_layer = TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=dim_feedforward,
            dropout=dropout,
            batch_first=True
        )
        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers)
        
        # è¾“å‡ºå±‚
        self.output_projection = nn.Sequential(
            nn.Linear(d_model, dim_feedforward // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(dim_feedforward // 2, 1)
        )
        
        # Dropout
        self.dropout = nn.Dropout(dropout)
    
    def _create_positional_encoding(self, max_len: int, d_model: int) -> torch.Tensor:
        """
        åˆ›å»ºä½ç½®ç¼–ç 
        """
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-np.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        return pe.unsqueeze(0)  # [1, max_len, d_model]
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥å¼ é‡ [batch_size, seq_len, input_dim]
            
        Returns:
            torch.Tensor: è¾“å‡ºå¼ é‡ [batch_size, seq_len, 1]
        """
        batch_size, seq_len, _ = x.shape
        
        # è¾“å…¥æŠ•å½±
        x = self.input_projection(x)  # [batch_size, seq_len, d_model]
        
        # æ·»åŠ ä½ç½®ç¼–ç 
        if seq_len <= self.max_seq_length:
            pos_encoding = self.positional_encoding[:, :seq_len, :].to(x.device)
            x = x + pos_encoding
        
        # Dropout
        x = self.dropout(x)
        
        # Transformerç¼–ç 
        x = self.transformer_encoder(x)  # [batch_size, seq_len, d_model]
        
        # è¾“å‡ºæŠ•å½±
        output = self.output_projection(x)  # [batch_size, seq_len, 1]
        
        return output

class ClimatePredictionSystem:
    """
    æ°”å€™é¢„æµ‹ç³»ç»Ÿ - ç»“åˆXGBoostå’ŒTransformeræ¨¡å‹
    """
    
    def __init__(self, mysql_config: Dict[str, str]):
        """
        åˆå§‹åŒ–é¢„æµ‹ç³»ç»Ÿ
        
        Args:
            mysql_config: MySQLæ•°æ®åº“é…ç½®
        """
        self.mysql_config = mysql_config
        self.data_loader = WeatherDataLoader(mysql_config)
        
        # æ¨¡å‹
        self.xgb_model = None
        self.transformer_model = None
        
        # æ•°æ®é¢„å¤„ç†å™¨
        self.scaler = StandardScaler()
        self.feature_scaler = MinMaxScaler()
        
        # è®¾å¤‡
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        logger.info(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # æ•°æ®
        self.raw_data = None
        self.processed_data = None
        self.features = None
        self.targets = None
    
    def load_and_prepare_data(self, 
                            lat_range: Tuple[float, float] = (30.67, 31.88),
                            lon_range: Tuple[float, float] = (120.87, 122.2),
                            sequence_length: int = 30) -> bool:
        """
        åŠ è½½å’Œå‡†å¤‡è®­ç»ƒæ•°æ®
        
        Args:
            lat_range: çº¬åº¦èŒƒå›´
            lon_range: ç»åº¦èŒƒå›´
            sequence_length: åºåˆ—é•¿åº¦
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        try:
            logger.info("ğŸ“Š æ­£åœ¨åŠ è½½å’Œå‡†å¤‡æ•°æ®...")
            
            # åŠ è½½åŸå§‹æ•°æ®
            self.raw_data = self.data_loader.load_temperature_data(
                lat_range=lat_range,
                lon_range=lon_range
            )
            
            if self.raw_data.empty:
                logger.error("âŒ æœªèƒ½åŠ è½½æ•°æ®")
                return False
            
            # æ•°æ®èšåˆ - æŒ‰æ—¥æœŸè®¡ç®—å¹³å‡æ¸©åº¦
            daily_data = self.raw_data.groupby('time').agg({
                'temperature_celsius': 'mean',
                'latitude': 'mean',
                'longitude': 'mean'
            }).reset_index()
            
            daily_data = daily_data.sort_values('time').reset_index(drop=True)
            
            # åˆ›å»ºç‰¹å¾
            self.processed_data = self._create_features(daily_data)
            
            # åˆ›å»ºåºåˆ—æ•°æ®
            self.features, self.targets = self._create_sequences(
                self.processed_data, sequence_length
            )
            
            logger.info(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ")
            logger.info(f"ğŸ“Š ç‰¹å¾å½¢çŠ¶: {self.features.shape}")
            logger.info(f"ğŸ“Š ç›®æ ‡å½¢çŠ¶: {self.targets.shape}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®å‡†å¤‡å¤±è´¥: {e}")
            return False
    
    def _create_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        åˆ›å»ºç‰¹å¾å·¥ç¨‹
        """
        logger.info("ğŸ”§ æ­£åœ¨åˆ›å»ºç‰¹å¾...")
        
        df = data.copy()
        
        # æ—¶é—´ç‰¹å¾
        df['year'] = df['time'].dt.year
        df['month'] = df['time'].dt.month
        df['day'] = df['time'].dt.day
        df['day_of_year'] = df['time'].dt.dayofyear
        df['week_of_year'] = df['time'].dt.isocalendar().week
        df['season'] = df['month'].map({12: 0, 1: 0, 2: 0,  # å†¬å­£
                                       3: 1, 4: 1, 5: 1,   # æ˜¥å­£
                                       6: 2, 7: 2, 8: 2,   # å¤å­£
                                       9: 3, 10: 3, 11: 3}) # ç§‹å­£
        
        # å‘¨æœŸæ€§ç‰¹å¾
        df['sin_day_of_year'] = np.sin(2 * np.pi * df['day_of_year'] / 365.25)
        df['cos_day_of_year'] = np.cos(2 * np.pi * df['day_of_year'] / 365.25)
        df['sin_month'] = np.sin(2 * np.pi * df['month'] / 12)
        df['cos_month'] = np.cos(2 * np.pi * df['month'] / 12)
        
        # æ»åç‰¹å¾
        for lag in [1, 2, 3, 7, 14, 30]:
            df[f'temp_lag_{lag}'] = df['temperature_celsius'].shift(lag)
        
        # æ»‘åŠ¨çª—å£ç»Ÿè®¡ç‰¹å¾
        for window in [3, 7, 14, 30]:
            df[f'temp_mean_{window}d'] = df['temperature_celsius'].rolling(window=window).mean()
            df[f'temp_std_{window}d'] = df['temperature_celsius'].rolling(window=window).std()
            df[f'temp_min_{window}d'] = df['temperature_celsius'].rolling(window=window).min()
            df[f'temp_max_{window}d'] = df['temperature_celsius'].rolling(window=window).max()
        
        # æ¸©åº¦å˜åŒ–ç‰¹å¾
        df['temp_diff_1d'] = df['temperature_celsius'].diff(1)
        df['temp_diff_7d'] = df['temperature_celsius'].diff(7)
        
        # åˆ é™¤åŒ…å«NaNçš„è¡Œ
        df = df.dropna().reset_index(drop=True)
        
        logger.info(f"âœ… ç‰¹å¾åˆ›å»ºå®Œæˆï¼Œå…± {len(df)} æ¡è®°å½•ï¼Œ{len(df.columns)} ä¸ªç‰¹å¾")
        
        return df
    
    def _create_sequences(self, data: pd.DataFrame, sequence_length: int) -> Tuple[np.ndarray, np.ndarray]:
        """
        åˆ›å»ºåºåˆ—æ•°æ®
        """
        logger.info(f"ğŸ”§ æ­£åœ¨åˆ›å»ºåºåˆ—æ•°æ® (åºåˆ—é•¿åº¦: {sequence_length})...")
        
        # é€‰æ‹©ç‰¹å¾åˆ—ï¼ˆæ’é™¤æ—¶é—´å’Œç›®æ ‡å˜é‡ï¼‰
        feature_cols = [col for col in data.columns 
                       if col not in ['time', 'temperature_celsius']]
        
        # å‡†å¤‡æ•°æ®
        feature_data = data[feature_cols].values
        target_data = data['temperature_celsius'].values
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        feature_data = self.feature_scaler.fit_transform(feature_data)
        
        # åˆ›å»ºåºåˆ—
        X, y = [], []
        
        for i in range(sequence_length, len(data)):
            X.append(feature_data[i-sequence_length:i])
            y.append(target_data[i])
        
        X = np.array(X)
        y = np.array(y)
        
        logger.info(f"âœ… åºåˆ—æ•°æ®åˆ›å»ºå®Œæˆ")
        logger.info(f"ğŸ“Š è¾“å…¥åºåˆ—å½¢çŠ¶: {X.shape}")
        logger.info(f"ğŸ“Š ç›®æ ‡åºåˆ—å½¢çŠ¶: {y.shape}")
        
        return X, y
    
    def train_xgboost_model(self, test_size: float = 0.2) -> Dict[str, float]:
        """
        è®­ç»ƒXGBoostæ¨¡å‹
        
        Args:
            test_size: æµ‹è¯•é›†æ¯”ä¾‹
            
        Returns:
            Dict[str, float]: è¯„ä¼°æŒ‡æ ‡
        """
        logger.info("ğŸš€ å¼€å§‹è®­ç»ƒXGBoostæ¨¡å‹...")
        
        if self.features is None or self.targets is None:
            raise ValueError("âŒ è¯·å…ˆåŠ è½½å’Œå‡†å¤‡æ•°æ®")
        
        # å°†åºåˆ—æ•°æ®å±•å¹³ç”¨äºXGBoost
        X_flat = self.features.reshape(self.features.shape[0], -1)
        y = self.targets
        
        # åˆ†å‰²æ•°æ®é›†
        X_train, X_test, y_train, y_test = train_test_split(
            X_flat, y, test_size=test_size, random_state=42, shuffle=False
        )
        
        # æ ‡å‡†åŒ–ç›®æ ‡å˜é‡
        y_train_scaled = self.scaler.fit_transform(y_train.reshape(-1, 1)).ravel()
        y_test_scaled = self.scaler.transform(y_test.reshape(-1, 1)).ravel()
        
        # åˆ›å»ºXGBoostæ¨¡å‹
        self.xgb_model = xgb.XGBRegressor(
            n_estimators=1000,
            max_depth=8,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            n_jobs=-1,
            early_stopping_rounds=50,
            eval_metric='rmse'
        )
        
        # è®­ç»ƒæ¨¡å‹
        self.xgb_model.fit(
            X_train, y_train_scaled,
            eval_set=[(X_test, y_test_scaled)],
            verbose=100
        )
        
        # é¢„æµ‹
        y_pred_scaled = self.xgb_model.predict(X_test)
        y_pred = self.scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        metrics = {
            'mae': mean_absolute_error(y_test, y_pred),
            'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),
            'r2': r2_score(y_test, y_pred)
        }
        
        logger.info(f"âœ… XGBoostæ¨¡å‹è®­ç»ƒå®Œæˆ")
        logger.info(f"ğŸ“Š MAE: {metrics['mae']:.4f}")
        logger.info(f"ğŸ“Š RMSE: {metrics['rmse']:.4f}")
        logger.info(f"ğŸ“Š RÂ²: {metrics['r2']:.4f}")
        
        return metrics
    
    def train_transformer_model(self, 
                              test_size: float = 0.2,
                              batch_size: int = 32,
                              epochs: int = 100,
                              learning_rate: float = 0.001) -> Dict[str, float]:
        """
        è®­ç»ƒTransformeræ¨¡å‹
        
        Args:
            test_size: æµ‹è¯•é›†æ¯”ä¾‹
            batch_size: æ‰¹æ¬¡å¤§å°
            epochs: è®­ç»ƒè½®æ•°
            learning_rate: å­¦ä¹ ç‡
            
        Returns:
            Dict[str, float]: è¯„ä¼°æŒ‡æ ‡
        """
        logger.info("ğŸš€ å¼€å§‹è®­ç»ƒTransformeræ¨¡å‹...")
        
        if self.features is None or self.targets is None:
            raise ValueError("âŒ è¯·å…ˆåŠ è½½å’Œå‡†å¤‡æ•°æ®")
        
        # å‡†å¤‡æ•°æ®
        X = self.features
        y = self.targets
        
        # åˆ†å‰²æ•°æ®é›†
        split_idx = int(len(X) * (1 - test_size))
        X_train, X_test = X[:split_idx], X[split_idx:]
        y_train, y_test = y[:split_idx], y[split_idx:]
        
        # æ ‡å‡†åŒ–ç›®æ ‡å˜é‡
        y_train_scaled = self.scaler.fit_transform(y_train.reshape(-1, 1)).ravel()
        y_test_scaled = self.scaler.transform(y_test.reshape(-1, 1)).ravel()
        
        # è½¬æ¢ä¸ºPyTorchå¼ é‡
        X_train_tensor = torch.FloatTensor(X_train).to(self.device)
        y_train_tensor = torch.FloatTensor(y_train_scaled).to(self.device)
        X_test_tensor = torch.FloatTensor(X_test).to(self.device)
        y_test_tensor = torch.FloatTensor(y_test_scaled).to(self.device)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        
        # åˆ›å»ºæ¨¡å‹
        input_dim = X.shape[2]
        self.transformer_model = WeatherTransformer(
            input_dim=input_dim,
            d_model=128,
            nhead=8,
            num_layers=6,
            dim_feedforward=512,
            dropout=0.1,
            max_seq_length=X.shape[1]
        ).to(self.device)
        
        # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        optimizer = optim.AdamW(self.transformer_model.parameters(), lr=learning_rate)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.5, patience=10, verbose=True
        )
        criterion = nn.MSELoss()
        
        # è®­ç»ƒå¾ªç¯
        best_loss = float('inf')
        patience_counter = 0
        patience = 20
        
        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            self.transformer_model.train()
            train_loss = 0.0
            
            for batch_X, batch_y in train_loader:
                optimizer.zero_grad()
                
                # å‰å‘ä¼ æ’­
                outputs = self.transformer_model(batch_X)
                outputs = outputs[:, -1, 0]  # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
                
                # è®¡ç®—æŸå¤±
                loss = criterion(outputs, batch_y)
                
                # åå‘ä¼ æ’­
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.transformer_model.parameters(), 1.0)
                optimizer.step()
                
                train_loss += loss.item()
            
            # éªŒè¯é˜¶æ®µ
            self.transformer_model.eval()
            with torch.no_grad():
                val_outputs = self.transformer_model(X_test_tensor)
                val_outputs = val_outputs[:, -1, 0]
                val_loss = criterion(val_outputs, y_test_tensor).item()
            
            # å­¦ä¹ ç‡è°ƒåº¦
            scheduler.step(val_loss)
            
            # æ—©åœæ£€æŸ¥
            if val_loss < best_loss:
                best_loss = val_loss
                patience_counter = 0
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                torch.save(self.transformer_model.state_dict(), 'best_transformer_model.pth')
            else:
                patience_counter += 1
            
            if epoch % 10 == 0:
                logger.info(f"Epoch {epoch}/{epochs}, Train Loss: {train_loss/len(train_loader):.6f}, Val Loss: {val_loss:.6f}")
            
            if patience_counter >= patience:
                logger.info(f"æ—©åœäºç¬¬ {epoch} è½®")
                break
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        self.transformer_model.load_state_dict(torch.load('best_transformer_model.pth'))
        
        # æœ€ç»ˆè¯„ä¼°
        self.transformer_model.eval()
        with torch.no_grad():
            y_pred_scaled = self.transformer_model(X_test_tensor)[:, -1, 0].cpu().numpy()
            y_pred = self.scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        metrics = {
            'mae': mean_absolute_error(y_test, y_pred),
            'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),
            'r2': r2_score(y_test, y_pred)
        }
        
        logger.info(f"âœ… Transformeræ¨¡å‹è®­ç»ƒå®Œæˆ")
        logger.info(f"ğŸ“Š MAE: {metrics['mae']:.4f}")
        logger.info(f"ğŸ“Š RMSE: {metrics['rmse']:.4f}")
        logger.info(f"ğŸ“Š RÂ²: {metrics['r2']:.4f}")
        
        return metrics
    
    def ensemble_predict(self, X: np.ndarray, weights: Tuple[float, float] = (0.5, 0.5)) -> np.ndarray:
        """
        é›†æˆé¢„æµ‹ - ç»“åˆXGBoostå’ŒTransformerçš„é¢„æµ‹ç»“æœ
        
        Args:
            X: è¾“å…¥ç‰¹å¾
            weights: æ¨¡å‹æƒé‡ (xgb_weight, transformer_weight)
            
        Returns:
            np.ndarray: é¢„æµ‹ç»“æœ
        """
        if self.xgb_model is None or self.transformer_model is None:
            raise ValueError("âŒ è¯·å…ˆè®­ç»ƒæ¨¡å‹")
        
        # XGBoosté¢„æµ‹
        X_flat = X.reshape(X.shape[0], -1)
        xgb_pred_scaled = self.xgb_model.predict(X_flat)
        xgb_pred = self.scaler.inverse_transform(xgb_pred_scaled.reshape(-1, 1)).ravel()
        
        # Transformeré¢„æµ‹
        self.transformer_model.eval()
        with torch.no_grad():
            X_tensor = torch.FloatTensor(X).to(self.device)
            transformer_pred_scaled = self.transformer_model(X_tensor)[:, -1, 0].cpu().numpy()
            transformer_pred = self.scaler.inverse_transform(transformer_pred_scaled.reshape(-1, 1)).ravel()
        
        # åŠ æƒå¹³å‡
        ensemble_pred = weights[0] * xgb_pred + weights[1] * transformer_pred
        
        return ensemble_pred
    
    def predict_future(self, days: int = 30) -> pd.DataFrame:
        """
        é¢„æµ‹æœªæ¥æ°”æ¸©
        
        Args:
            days: é¢„æµ‹å¤©æ•°
            
        Returns:
            pd.DataFrame: é¢„æµ‹ç»“æœ
        """
        logger.info(f"ğŸ”® æ­£åœ¨é¢„æµ‹æœªæ¥ {days} å¤©çš„æ°”æ¸©...")
        
        if self.processed_data is None:
            raise ValueError("âŒ è¯·å…ˆåŠ è½½å’Œå‡†å¤‡æ•°æ®")
        
        # è·å–æœ€åçš„åºåˆ—æ•°æ®
        last_sequence = self.features[-1:]
        
        predictions = []
        current_sequence = last_sequence.copy()
        
        # è·å–æœ€åçš„æ—¥æœŸ
        last_date = self.processed_data['time'].iloc[-1]
        
        for i in range(days):
            # é¢„æµ‹ä¸‹ä¸€å¤©
            pred = self.ensemble_predict(current_sequence)
            predictions.append(pred[0])
            
            # æ›´æ–°åºåˆ—ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…åº”ç”¨ä¸­éœ€è¦æ›´å¤æ‚çš„ç‰¹å¾æ›´æ–°ï¼‰
            # è¿™é‡Œæˆ‘ä»¬åªæ˜¯ç®€å•åœ°æ»šåŠ¨åºåˆ—
            new_features = current_sequence[0, -1:].copy()
            current_sequence = np.concatenate([
                current_sequence[:, 1:],
                new_features.reshape(1, 1, -1)
            ], axis=1)
        
        # åˆ›å»ºé¢„æµ‹ç»“æœDataFrame
        future_dates = pd.date_range(
            start=last_date + timedelta(days=1),
            periods=days,
            freq='D'
        )
        
        results = pd.DataFrame({
            'date': future_dates,
            'predicted_temperature': predictions
        })
        
        logger.info(f"âœ… æœªæ¥æ°”æ¸©é¢„æµ‹å®Œæˆ")
        
        return results
    
    def create_visualizations(self, predictions: pd.DataFrame = None):
        """
        åˆ›å»ºå¯è§†åŒ–å›¾è¡¨
        
        Args:
            predictions: é¢„æµ‹ç»“æœ
        """
        logger.info("ğŸ“Š æ­£åœ¨åˆ›å»ºå¯è§†åŒ–å›¾è¡¨...")
        
        # è®¾ç½®å›¾è¡¨æ ·å¼
        plt.style.use('seaborn-v0_8')
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('æ°”å€™é¢„æµ‹æ¨¡å‹åˆ†æç»“æœ', fontsize=16, fontweight='bold')
        
        # 1. å†å²æ¸©åº¦è¶‹åŠ¿
        if self.processed_data is not None:
            axes[0, 0].plot(self.processed_data['time'], self.processed_data['temperature_celsius'], 
                           alpha=0.7, linewidth=1, label='å†å²æ°”æ¸©')
            axes[0, 0].set_title('å†å²æ°”æ¸©è¶‹åŠ¿')
            axes[0, 0].set_xlabel('æ—¶é—´')
            axes[0, 0].set_ylabel('æ¸©åº¦ (Â°C)')
            axes[0, 0].legend()
            axes[0, 0].grid(True, alpha=0.3)
        
        # 2. é¢„æµ‹ç»“æœ
        if predictions is not None:
            axes[0, 1].plot(predictions['date'], predictions['predicted_temperature'], 
                           'r-', linewidth=2, marker='o', markersize=4, label='é¢„æµ‹æ°”æ¸©')
            axes[0, 1].set_title('æœªæ¥æ°”æ¸©é¢„æµ‹')
            axes[0, 1].set_xlabel('æ—¶é—´')
            axes[0, 1].set_ylabel('æ¸©åº¦ (Â°C)')
            axes[0, 1].legend()
            axes[0, 1].grid(True, alpha=0.3)
            
            # æ—‹è½¬xè½´æ ‡ç­¾
            for tick in axes[0, 1].get_xticklabels():
                tick.set_rotation(45)
        
        # 3. æ¸©åº¦åˆ†å¸ƒ
        if self.processed_data is not None:
            axes[1, 0].hist(self.processed_data['temperature_celsius'], bins=50, 
                           alpha=0.7, color='skyblue', edgecolor='black')
            axes[1, 0].set_title('å†å²æ°”æ¸©åˆ†å¸ƒ')
            axes[1, 0].set_xlabel('æ¸©åº¦ (Â°C)')
            axes[1, 0].set_ylabel('é¢‘æ¬¡')
            axes[1, 0].grid(True, alpha=0.3)
        
        # 4. å­£èŠ‚æ€§åˆ†æ
        if self.processed_data is not None:
            monthly_temp = self.processed_data.groupby('month')['temperature_celsius'].mean()
            axes[1, 1].bar(monthly_temp.index, monthly_temp.values, 
                          color='lightcoral', alpha=0.8, edgecolor='black')
            axes[1, 1].set_title('æœˆå¹³å‡æ°”æ¸©')
            axes[1, 1].set_xlabel('æœˆä»½')
            axes[1, 1].set_ylabel('å¹³å‡æ¸©åº¦ (Â°C)')
            axes[1, 1].set_xticks(range(1, 13))
            axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        output_path = f"outputs/climate_prediction_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
        os.makedirs('outputs', exist_ok=True)
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        logger.info(f"ğŸ“Š å›¾è¡¨å·²ä¿å­˜è‡³: {output_path}")
        
        plt.show()
    
    def save_models(self, model_dir: str = 'models/climate_prediction'):
        """
        ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹
        
        Args:
            model_dir: æ¨¡å‹ä¿å­˜ç›®å½•
        """
        os.makedirs(model_dir, exist_ok=True)
        
        # ä¿å­˜XGBoostæ¨¡å‹
        if self.xgb_model is not None:
            xgb_path = os.path.join(model_dir, 'xgboost_model.pkl')
            with open(xgb_path, 'wb') as f:
                pickle.dump(self.xgb_model, f)
            logger.info(f"âœ… XGBoostæ¨¡å‹å·²ä¿å­˜è‡³: {xgb_path}")
        
        # ä¿å­˜Transformeræ¨¡å‹
        if self.transformer_model is not None:
            transformer_path = os.path.join(model_dir, 'transformer_model.pth')
            torch.save(self.transformer_model.state_dict(), transformer_path)
            logger.info(f"âœ… Transformeræ¨¡å‹å·²ä¿å­˜è‡³: {transformer_path}")
        
        # ä¿å­˜é¢„å¤„ç†å™¨
        scaler_path = os.path.join(model_dir, 'scalers.pkl')
        with open(scaler_path, 'wb') as f:
            pickle.dump({
                'target_scaler': self.scaler,
                'feature_scaler': self.feature_scaler
            }, f)
        logger.info(f"âœ… é¢„å¤„ç†å™¨å·²ä¿å­˜è‡³: {scaler_path}")
    
    def load_models(self, model_dir: str = 'models/climate_prediction'):
        """
        åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
        
        Args:
            model_dir: æ¨¡å‹ä¿å­˜ç›®å½•
        """
        # åŠ è½½XGBoostæ¨¡å‹
        xgb_path = os.path.join(model_dir, 'xgboost_model.pkl')
        if os.path.exists(xgb_path):
            with open(xgb_path, 'rb') as f:
                self.xgb_model = pickle.load(f)
            logger.info(f"âœ… XGBoostæ¨¡å‹å·²åŠ è½½")
        
        # åŠ è½½é¢„å¤„ç†å™¨
        scaler_path = os.path.join(model_dir, 'scalers.pkl')
        if os.path.exists(scaler_path):
            with open(scaler_path, 'rb') as f:
                scalers = pickle.load(f)
                self.scaler = scalers['target_scaler']
                self.feature_scaler = scalers['feature_scaler']
            logger.info(f"âœ… é¢„å¤„ç†å™¨å·²åŠ è½½")
        
        logger.info(f"ğŸ“ æ¨¡å‹åŠ è½½å®Œæˆ")


def main():
    """
    ä¸»å‡½æ•° - æ¼”ç¤ºå®Œæ•´çš„æ°”å€™é¢„æµ‹æµç¨‹
    """
    logger.info("ğŸŒŸ å¼€å§‹æ°”å€™é¢„æµ‹ç³»ç»Ÿæ¼”ç¤º")
    
    try:
        # 1. åˆå§‹åŒ–é¢„æµ‹ç³»ç»Ÿ
        prediction_system = ClimatePredictionSystem()
        
        # 2. åŠ è½½æ•°æ®
        logger.info("ğŸ“Š æ­£åœ¨åŠ è½½æ°”è±¡æ•°æ®...")
        prediction_system.load_data_from_mysql(
            lat_range=(30.67, 31.88),  # åŒ—çº¬30Â°40â€²è‡³31Â°53â€²
            lon_range=(120.87, 122.20),  # ä¸œç»120Â°52â€²è‡³122Â°12â€²
            start_date='2020-01-01',
            end_date='2024-12-31'
        )
        
        # 3. å‡†å¤‡è®­ç»ƒæ•°æ®
        logger.info("ğŸ”§ æ­£åœ¨å‡†å¤‡è®­ç»ƒæ•°æ®...")
        prediction_system.prepare_features()
        X, y = prediction_system.create_sequences(sequence_length=30, target_days=1)
        
        # 4. è®­ç»ƒXGBoostæ¨¡å‹
        logger.info("ğŸš€ å¼€å§‹è®­ç»ƒXGBoostæ¨¡å‹...")
        xgb_metrics = prediction_system.train_xgboost_model(test_size=0.2)
        
        # 5. è®­ç»ƒTransformeræ¨¡å‹
        logger.info("ğŸš€ å¼€å§‹è®­ç»ƒTransformeræ¨¡å‹...")
        transformer_metrics = prediction_system.train_transformer_model(
            test_size=0.2,
            batch_size=32,
            epochs=50,
            learning_rate=0.001
        )
        
        # 6. æ¨¡å‹æ€§èƒ½å¯¹æ¯”
        logger.info("ğŸ“Š æ¨¡å‹æ€§èƒ½å¯¹æ¯”:")
        logger.info(f"XGBoost - MAE: {xgb_metrics['mae']:.4f}, RMSE: {xgb_metrics['rmse']:.4f}, RÂ²: {xgb_metrics['r2']:.4f}")
        logger.info(f"Transformer - MAE: {transformer_metrics['mae']:.4f}, RMSE: {transformer_metrics['rmse']:.4f}, RÂ²: {transformer_metrics['r2']:.4f}")
        
        # 7. é¢„æµ‹æœªæ¥æ°”æ¸©
        logger.info("ğŸ”® æ­£åœ¨é¢„æµ‹æœªæ¥30å¤©æ°”æ¸©...")
        future_predictions = prediction_system.predict_future(days=30)
        
        # 8. åˆ›å»ºå¯è§†åŒ–
        logger.info("ğŸ“Š æ­£åœ¨åˆ›å»ºå¯è§†åŒ–å›¾è¡¨...")
        prediction_system.create_visualizations(future_predictions)
        
        # 9. ä¿å­˜æ¨¡å‹
        logger.info("ğŸ’¾ æ­£åœ¨ä¿å­˜æ¨¡å‹...")
        prediction_system.save_models()
        
        # 10. ä¿å­˜é¢„æµ‹ç»“æœ
        output_path = f"outputs/future_predictions_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        os.makedirs('outputs', exist_ok=True)
        future_predictions.to_csv(output_path, index=False, encoding='utf-8-sig')
        logger.info(f"ğŸ“„ é¢„æµ‹ç»“æœå·²ä¿å­˜è‡³: {output_path}")
        
        # 11. æ˜¾ç¤ºé¢„æµ‹ç»“æœæ‘˜è¦
        logger.info("ğŸ“‹ é¢„æµ‹ç»“æœæ‘˜è¦:")
        logger.info(f"é¢„æµ‹æœŸé—´: {future_predictions['date'].min()} è‡³ {future_predictions['date'].max()}")
        logger.info(f"å¹³å‡é¢„æµ‹æ¸©åº¦: {future_predictions['predicted_temperature'].mean():.2f}Â°C")
        logger.info(f"æœ€é«˜é¢„æµ‹æ¸©åº¦: {future_predictions['predicted_temperature'].max():.2f}Â°C")
        logger.info(f"æœ€ä½é¢„æµ‹æ¸©åº¦: {future_predictions['predicted_temperature'].min():.2f}Â°C")
        
        logger.info("âœ… æ°”å€™é¢„æµ‹ç³»ç»Ÿæ¼”ç¤ºå®Œæˆ!")
        
    except Exception as e:
        logger.error(f"âŒ ç³»ç»Ÿè¿è¡Œå‡ºé”™: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())


if __name__ == "__main__":
    main()