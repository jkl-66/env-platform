# çœŸå®AIå›¾åƒç”Ÿæˆé…ç½®æŒ‡å—

## æ¦‚è¿°

å½“å‰ç³»ç»Ÿä½¿ç”¨ç¤ºä¾‹å›¾åƒæ•°æ®æ¥æ¨¡æ‹ŸAIå›¾åƒç”ŸæˆåŠŸèƒ½ã€‚è¦è·å¾—çœŸæ­£çš„AIç”Ÿæˆå›¾åƒä½œå“ï¼Œéœ€è¦é…ç½®å®é™…çš„å›¾åƒç”Ÿæˆæ¨¡å‹ã€‚æœ¬æ–‡æ¡£æä¾›äº†è¯¦ç»†çš„é…ç½®æ­¥éª¤ã€‚

## ğŸ¯ ç›®æ ‡

å°†ç³»ç»Ÿä»ç”Ÿæˆç¤ºä¾‹å›¾åƒæ•°æ®å‡çº§ä¸ºç”ŸæˆçœŸå®çš„AIå›¾åƒä½œå“ï¼ŒåŒ…æ‹¬ï¼š
- ç¯å¢ƒè­¦ç¤ºåœºæ™¯å›¾åƒ
- ç”Ÿæ€ç³»ç»Ÿå¯è§†åŒ–
- æ°”å€™å˜åŒ–å½±å“å›¾åƒ
- æ±¡æŸ“åœºæ™¯å›¾åƒ

## ğŸ”§ é…ç½®é€‰é¡¹

### é€‰é¡¹1: Stable Diffusion (æ¨è)

**ä¼˜ç‚¹:**
- å¼€æºå…è´¹
- é«˜è´¨é‡å›¾åƒç”Ÿæˆ
- æ”¯æŒè‡ªå®šä¹‰æç¤ºè¯
- æ´»è·ƒçš„ç¤¾åŒºæ”¯æŒ

**è¦æ±‚:**
- GPU: 4GB+ VRAM (æ¨è8GB+)
- RAM: 8GB+
- å­˜å‚¨: 10GB+ å¯ç”¨ç©ºé—´

**å®‰è£…æ­¥éª¤:**

```bash
# 1. å®‰è£…æ ¸å¿ƒä¾èµ–
pip install diffusers>=0.21.0
pip install transformers>=4.25.0
pip install accelerate>=0.20.0
pip install safetensors>=0.3.0

# 2. å®‰è£…GPUåŠ é€Ÿ (å¯é€‰ï¼Œä½†å¼ºçƒˆæ¨è)
pip install xformers  # ä»…é™Linux/Windows

# 3. å®‰è£…PyTorch (å¦‚æœå°šæœªå®‰è£…)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### é€‰é¡¹2: DALL-E API

**ä¼˜ç‚¹:**
- æ— éœ€æœ¬åœ°GPU
- é«˜è´¨é‡å›¾åƒ
- ç®€å•é›†æˆ

**ç¼ºç‚¹:**
- éœ€è¦ä»˜è´¹API
- ä¾èµ–ç½‘ç»œè¿æ¥
- æœ‰ä½¿ç”¨é™åˆ¶

**å®‰è£…æ­¥éª¤:**

```bash
pip install openai>=1.0.0
```

### é€‰é¡¹3: æœ¬åœ°GANæ¨¡å‹

**ä¼˜ç‚¹:**
- å®Œå…¨æœ¬åœ°åŒ–
- å¯è‡ªå®šä¹‰è®­ç»ƒ

**ç¼ºç‚¹:**
- éœ€è¦å¤§é‡è®­ç»ƒæ•°æ®
- è®­ç»ƒæ—¶é—´é•¿
- æŠ€æœ¯é—¨æ§›é«˜

## ğŸ› ï¸ ä»£ç ä¿®æ”¹

### 1. ä¿®æ”¹ EcologyImageGenerator ç±»

åœ¨ `src/models/ecology_image_generator.py` ä¸­ä¿®æ”¹ `_build_model` æ–¹æ³•ï¼š

```python
def _build_model(self):
    """æ„å»ºå›¾åƒç”Ÿæˆæ¨¡å‹"""
    try:
        # é€‰é¡¹1: Stable Diffusion
        from diffusers import StableDiffusionPipeline
        import torch
        
        # æ£€æŸ¥è®¾å¤‡
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åŠ è½½æ¨¡å‹
        model_id = "runwayml/stable-diffusion-v1-5"
        
        self.diffusion_pipeline = StableDiffusionPipeline.from_pretrained(
            model_id,
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
            safety_checker=None,
            requires_safety_checker=False
        )
        
        self.diffusion_pipeline = self.diffusion_pipeline.to(self.device)
        
        # å†…å­˜ä¼˜åŒ–
        if self.device == "cuda":
            self.diffusion_pipeline.enable_attention_slicing()
            self.diffusion_pipeline.enable_memory_efficient_attention()
            
            # å¦‚æœæ˜¾å­˜ä¸è¶³ï¼Œå¯ç”¨CPUå¸è½½
            # self.diffusion_pipeline.enable_sequential_cpu_offload()
        
        logger.info("Stable Diffusionæ¨¡å‹åŠ è½½æˆåŠŸ")
        
    except ImportError as e:
        logger.warning(f"æ— æ³•å¯¼å…¥diffusers: {e}")
        logger.warning("å°†ä½¿ç”¨ç¤ºä¾‹å›¾åƒç”Ÿæˆ")
        self.diffusion_pipeline = None
        
    except Exception as e:
        logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        self.diffusion_pipeline = None
```

### 2. ä¿®æ”¹ _generate_with_diffusion æ–¹æ³•

```python
def _generate_with_diffusion(self, prompt, conditions=None):
    """ä½¿ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆå›¾åƒ"""
    try:
        if self.diffusion_pipeline is None:
            logger.warning("æ‰©æ•£æ¨¡å‹æœªåŠ è½½ï¼Œä½¿ç”¨ç¤ºä¾‹å›¾åƒ")
            return self._create_example_warning_image(prompt, "medium")
        
        # æ„å»ºå®Œæ•´æç¤ºè¯
        if conditions:
            full_prompt = self._conditions_to_prompt(conditions)
        else:
            full_prompt = prompt
        
        # å¢å¼ºç¯å¢ƒä¸»é¢˜æç¤ºè¯
        enhanced_prompt = self._enhance_environmental_prompt(full_prompt)
        
        logger.info(f"ç”Ÿæˆå›¾åƒï¼Œæç¤ºè¯: {enhanced_prompt}")
        
        # ç”Ÿæˆå‚æ•°
        generation_params = {
            "prompt": enhanced_prompt,
            "num_inference_steps": 50,  # æ¨ç†æ­¥æ•°ï¼Œè¶Šé«˜è´¨é‡è¶Šå¥½ä½†é€Ÿåº¦è¶Šæ…¢
            "guidance_scale": 7.5,      # æç¤ºè¯å¼•å¯¼å¼ºåº¦
            "width": 512,               # å›¾åƒå®½åº¦
            "height": 512,              # å›¾åƒé«˜åº¦
            "num_images_per_prompt": 1,  # ç”Ÿæˆå›¾åƒæ•°é‡
        }
        
        # ç”Ÿæˆå›¾åƒ
        with torch.no_grad():
            result = self.diffusion_pipeline(**generation_params)
            
        # è·å–ç”Ÿæˆçš„å›¾åƒ
        generated_image = result.images[0]
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        import numpy as np
        image_array = np.array(generated_image)
        
        logger.info(f"å›¾åƒç”ŸæˆæˆåŠŸï¼Œå°ºå¯¸: {image_array.shape}")
        return image_array
        
    except Exception as e:
        logger.error(f"å›¾åƒç”Ÿæˆå¤±è´¥: {e}")
        # å›é€€åˆ°ç¤ºä¾‹å›¾åƒ
        return self._create_example_warning_image(prompt, "medium")

def _enhance_environmental_prompt(self, prompt):
    """å¢å¼ºç¯å¢ƒä¸»é¢˜çš„æç¤ºè¯"""
    # ç¯å¢ƒè‰ºæœ¯é£æ ¼å…³é”®è¯
    style_keywords = [
        "environmental art",
        "nature photography",
        "dramatic lighting",
        "high quality",
        "detailed",
        "realistic"
    ]
    
    # æ£€æµ‹ç¯å¢ƒä¸»é¢˜å¹¶æ·»åŠ ç›¸åº”å…³é”®è¯
    environmental_themes = {
        "pollution": "polluted environment, smog, industrial waste",
        "climate": "climate change effects, extreme weather",
        "wildlife": "endangered wildlife, natural habitat",
        "forest": "deforestation, forest destruction",
        "ocean": "ocean pollution, marine life threat",
        "glacier": "melting glaciers, ice caps, global warming"
    }
    
    enhanced = prompt
    
    # æ·»åŠ ä¸»é¢˜å…³é”®è¯
    for theme, keywords in environmental_themes.items():
        if theme in prompt.lower():
            enhanced += f", {keywords}"
    
    # æ·»åŠ é£æ ¼å…³é”®è¯
    enhanced += f", {', '.join(style_keywords)}"
    
    return enhanced
```

### 3. æ·»åŠ å›¾åƒä¿å­˜åŠŸèƒ½

```python
def _save_generated_image(self, image_array, filename):
    """ä¿å­˜ç”Ÿæˆçš„å›¾åƒ"""
    try:
        from PIL import Image
        import numpy as np
        
        # ç¡®ä¿å›¾åƒæ•°æ®æ ¼å¼æ­£ç¡®
        if image_array.dtype != np.uint8:
            image_array = (image_array * 255).astype(np.uint8)
        
        # åˆ›å»ºPILå›¾åƒ
        if len(image_array.shape) == 3:
            image = Image.fromarray(image_array, 'RGB')
        else:
            image = Image.fromarray(image_array, 'L')
        
        # ä¿å­˜å›¾åƒ
        image.save(filename, 'PNG', quality=95)
        logger.info(f"å›¾åƒå·²ä¿å­˜: {filename}")
        
        return filename
        
    except Exception as e:
        logger.error(f"å›¾åƒä¿å­˜å¤±è´¥: {e}")
        return None
```

## ğŸš€ ä½¿ç”¨ç¤ºä¾‹

### åŸºæœ¬ä½¿ç”¨

```python
from src.models.ecology_image_generator import EcologyImageGenerator

# åˆ›å»ºç”Ÿæˆå™¨å®ä¾‹
generator = EcologyImageGenerator()

# ä»è‡ªç„¶è¯­è¨€ç”Ÿæˆå›¾åƒ
result = generator.generate_from_text(
    "ä¸¥é‡çš„ç©ºæ°”æ±¡æŸ“ï¼ŒåŸå¸‚è¢«é›¾éœ¾ç¬¼ç½©ï¼Œèƒ½è§åº¦æä½"
)

# æ£€æŸ¥ç»“æœ
if result['success']:
    print(f"å›¾åƒç”ŸæˆæˆåŠŸ: {result['image_path']}")
    print(f"è­¦ç¤ºç­‰çº§: {result['warning_level']}")
else:
    print(f"ç”Ÿæˆå¤±è´¥: {result['error']}")
```

### é«˜çº§é…ç½®

```python
# è‡ªå®šä¹‰ç”Ÿæˆå‚æ•°
generator = EcologyImageGenerator(
    model_config={
        'model_id': 'stabilityai/stable-diffusion-2-1',  # ä½¿ç”¨æ›´æ–°çš„æ¨¡å‹
        'inference_steps': 100,  # æ›´é«˜è´¨é‡
        'guidance_scale': 10.0,  # æ›´å¼ºçš„æç¤ºè¯å¼•å¯¼
        'width': 768,
        'height': 768
    }
)
```

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **GPUå†…å­˜ä¸è¶³**
   ```python
   # å¯ç”¨CPUå¸è½½
   pipeline.enable_sequential_cpu_offload()
   
   # æˆ–ä½¿ç”¨æ›´å°çš„å›¾åƒå°ºå¯¸
   width=256, height=256
   ```

2. **æ¨¡å‹ä¸‹è½½å¤±è´¥**
   ```bash
   # è®¾ç½®Hugging Faceé•œåƒ
   export HF_ENDPOINT=https://hf-mirror.com
   ```

3. **ç”Ÿæˆé€Ÿåº¦æ…¢**
   ```python
   # å‡å°‘æ¨ç†æ­¥æ•°
   num_inference_steps=20
   
   # å¯ç”¨xformersåŠ é€Ÿ
   pipeline.enable_xformers_memory_efficient_attention()
   ```

### æ€§èƒ½ä¼˜åŒ–

1. **å†…å­˜ä¼˜åŒ–**
   - å¯ç”¨attention slicing
   - ä½¿ç”¨CPUå¸è½½
   - å‡å°æ‰¹æ¬¡å¤§å°

2. **é€Ÿåº¦ä¼˜åŒ–**
   - ä½¿ç”¨xformers
   - å‡å°‘æ¨ç†æ­¥æ•°
   - ä½¿ç”¨è¾ƒå°çš„å›¾åƒå°ºå¯¸

3. **è´¨é‡ä¼˜åŒ–**
   - å¢åŠ æ¨ç†æ­¥æ•°
   - è°ƒæ•´guidance scale
   - ä½¿ç”¨æ›´å¥½çš„æ¨¡å‹

## ğŸ“Š æ€§èƒ½åŸºå‡†

| é…ç½® | ç”Ÿæˆæ—¶é—´ | å†…å­˜ä½¿ç”¨ | å›¾åƒè´¨é‡ |
|------|----------|----------|----------|
| RTX 3060 (12GB) | 15-30ç§’ | 6-8GB | é«˜ |
| RTX 4070 (12GB) | 10-20ç§’ | 5-7GB | é«˜ |
| CPU (16GB RAM) | 2-5åˆ†é’Ÿ | 4-6GB | ä¸­ç­‰ |

## ğŸ” å®‰å…¨æ³¨æ„äº‹é¡¹

1. **å†…å®¹è¿‡æ»¤**: é»˜è®¤å¯ç”¨å®‰å…¨æ£€æŸ¥å™¨ï¼Œé˜²æ­¢ç”Ÿæˆä¸å½“å†…å®¹
2. **èµ„æºé™åˆ¶**: è®¾ç½®åˆç†çš„ç”Ÿæˆå‚æ•°ï¼Œé¿å…èµ„æºè€—å°½
3. **æ¨¡å‹æ¥æº**: ä»…ä½¿ç”¨å¯ä¿¡çš„é¢„è®­ç»ƒæ¨¡å‹

## ğŸ“š å‚è€ƒèµ„æº

- [Diffusers å®˜æ–¹æ–‡æ¡£](https://huggingface.co/docs/diffusers)
- [Stable Diffusion æ¨¡å‹åº“](https://huggingface.co/runwayml/stable-diffusion-v1-5)
- [ä¼˜åŒ–æŒ‡å—](https://huggingface.co/docs/diffusers/optimization/fp16)
- [æ•…éšœæ’é™¤](https://huggingface.co/docs/diffusers/troubleshooting)

## ğŸ¯ ä¸‹ä¸€æ­¥

é…ç½®å®Œæˆåï¼Œæ‚¨å¯ä»¥ï¼š

1. è¿è¡Œ `scripts/setup_real_image_generation.py` æ£€æŸ¥é…ç½®
2. æµ‹è¯•å›¾åƒç”ŸæˆåŠŸèƒ½
3. æ ¹æ®éœ€è¦è°ƒæ•´ç”Ÿæˆå‚æ•°
4. æ¢ç´¢æ›´å¤šæ¨¡å‹å’Œé£æ ¼

---

**æ³¨æ„**: é¦–æ¬¡è¿è¡Œæ—¶ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹æ–‡ä»¶ï¼ˆçº¦4-6GBï¼‰ï¼Œè¯·ç¡®ä¿ç½‘ç»œè¿æ¥ç¨³å®šã€‚