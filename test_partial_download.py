#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•éƒ¨åˆ†ä¸‹è½½çš„æ¨¡å‹æ–‡ä»¶
æ£€æŸ¥å½“å‰ç¼“å­˜çŠ¶æ€å¹¶å°è¯•ä½¿ç”¨å¯ç”¨çš„æ¨¡å‹
"""

import os
import sys
from pathlib import Path

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
os.environ['HF_HUB_CACHE'] = str(Path.cwd() / 'cache' / 'huggingface')
os.environ['TRANSFORMERS_CACHE'] = str(Path.cwd() / 'cache' / 'huggingface')
os.environ['HF_HOME'] = str(Path.cwd() / 'cache' / 'huggingface')

print("ğŸ” æ£€æŸ¥å½“å‰æ¨¡å‹ä¸‹è½½çŠ¶æ€")
print("=" * 50)
print(f"ğŸ“ ç¼“å­˜ç›®å½•: {os.environ['HF_HUB_CACHE']}")
print()

def check_cache_status():
    """æ£€æŸ¥ç¼“å­˜ç›®å½•çŠ¶æ€"""
    cache_dir = Path(os.environ['HF_HUB_CACHE'])
    
    if not cache_dir.exists():
        print("âŒ ç¼“å­˜ç›®å½•ä¸å­˜åœ¨")
        return
    
    print("ğŸ“‚ ç¼“å­˜ç›®å½•å†…å®¹:")
    
    # æ£€æŸ¥ hub ç›®å½•
    hub_dir = cache_dir / 'hub'
    if hub_dir.exists():
        print(f"  ğŸ“ hub/ ç›®å½•å­˜åœ¨")
        
        # åˆ—å‡ºæ‰€æœ‰æ¨¡å‹ç›®å½•
        model_dirs = [d for d in hub_dir.iterdir() if d.is_dir() and d.name.startswith('models--')]
        
        for model_dir in model_dirs:
            model_name = model_dir.name.replace('models--', '').replace('--', '/')
            print(f"    ğŸ¤– æ¨¡å‹: {model_name}")
            
            # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
            snapshots_dir = model_dir / 'snapshots'
            if snapshots_dir.exists():
                snapshot_dirs = list(snapshots_dir.iterdir())
                if snapshot_dirs:
                    latest_snapshot = snapshot_dirs[0]  # å–ç¬¬ä¸€ä¸ªå¿«ç…§
                    files = list(latest_snapshot.iterdir())
                    total_size = sum(f.stat().st_size for f in files if f.is_file())
                    print(f"      ğŸ“„ æ–‡ä»¶æ•°é‡: {len(files)}")
                    print(f"      ğŸ’¾ æ€»å¤§å°: {total_size / (1024*1024):.1f} MB")
                    
                    # æ£€æŸ¥å…³é”®æ–‡ä»¶
                    key_files = ['config.json', 'model.safetensors', 'pytorch_model.bin']
                    for key_file in key_files:
                        file_path = latest_snapshot / key_file
                        if file_path.exists():
                            size_mb = file_path.stat().st_size / (1024*1024)
                            print(f"        âœ… {key_file}: {size_mb:.1f} MB")
                        else:
                            print(f"        âŒ {key_file}: ç¼ºå¤±")
    else:
        print("  âŒ hub/ ç›®å½•ä¸å­˜åœ¨")

def test_model_loading():
    """æµ‹è¯•æ¨¡å‹åŠ è½½"""
    print("\nğŸ§ª æµ‹è¯•æ¨¡å‹åŠ è½½")
    print("-" * 30)
    
    try:
        from diffusers import StableDiffusionPipeline
        import torch
        print("âœ… ä¾èµ–åº“å¯¼å…¥æˆåŠŸ")
        
        # å°è¯•åŠ è½½æ¨¡å‹ï¼ˆä»…æœ¬åœ°æ–‡ä»¶ï¼‰
        models_to_test = [
            "stabilityai/stable-diffusion-3.5-large",
        "stabilityai/stable-diffusion-3-medium-diffusers",
        "stabilityai/stable-diffusion-2-1"
        ]
        
        for model_id in models_to_test:
            print(f"\nğŸ”§ æµ‹è¯•æ¨¡å‹: {model_id}")
            try:
                pipeline = StableDiffusionPipeline.from_pretrained(
                    model_id,
                    cache_dir=os.environ['HF_HUB_CACHE'],
                    torch_dtype=torch.float32,
                    device_map="cpu",
                    local_files_only=True  # ä»…ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
                )
                print(f"âœ… {model_id} åŠ è½½æˆåŠŸï¼")
                
                # ç®€å•æµ‹è¯•ç”Ÿæˆ
                print("ğŸ¨ æµ‹è¯•å›¾åƒç”Ÿæˆ...")
                with torch.no_grad():
                    image = pipeline(
                        "a simple test image",
                        num_inference_steps=1,  # æœ€å°‘æ­¥æ•°
                        guidance_scale=1.0,
                        height=64,  # å°å°ºå¯¸
                        width=64
                    ).images[0]
                print("âœ… å›¾åƒç”Ÿæˆæµ‹è¯•æˆåŠŸï¼")
                return True
                
            except Exception as e:
                print(f"âŒ {model_id} åŠ è½½å¤±è´¥: {str(e)[:100]}...")
                continue
        
        print("âŒ æ‰€æœ‰æ¨¡å‹åŠ è½½å¤±è´¥")
        return False
        
    except ImportError as e:
        print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
        return False

def test_alternative_approach():
    """æµ‹è¯•æ›¿ä»£æ–¹æ¡ˆ"""
    print("\nğŸ”„ æµ‹è¯•æ›¿ä»£æ–¹æ¡ˆ")
    print("-" * 30)
    
    try:
        # å°è¯•ä½¿ç”¨æ›´è½»é‡çº§çš„æ–¹æ³•
        from transformers import pipeline
        
        # æµ‹è¯•æ–‡æœ¬ç”Ÿæˆï¼ˆæ›´å°çš„æ¨¡å‹ï¼‰
        print("ğŸ“ æµ‹è¯•æ–‡æœ¬ç”Ÿæˆç®¡é“...")
        text_generator = pipeline(
            "text-generation",
            model="gpt2",  # æ›´å°çš„æ¨¡å‹
            cache_dir=os.environ['HF_HUB_CACHE']
        )
        
        result = text_generator("Environmental protection is", max_length=30, num_return_sequences=1)
        print(f"âœ… æ–‡æœ¬ç”ŸæˆæˆåŠŸ: {result[0]['generated_text']}")
        return True
        
    except Exception as e:
        print(f"âŒ æ›¿ä»£æ–¹æ¡ˆå¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    # æ£€æŸ¥ç¼“å­˜çŠ¶æ€
    check_cache_status()
    
    # æµ‹è¯•æ¨¡å‹åŠ è½½
    model_success = test_model_loading()
    
    # å¦‚æœæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°è¯•æ›¿ä»£æ–¹æ¡ˆ
    if not model_success:
        alt_success = test_alternative_approach()
        if alt_success:
            print("\nğŸ’¡ å»ºè®®: å¯ä»¥ä½¿ç”¨æ–‡æœ¬ç”Ÿæˆç­‰æ›¿ä»£åŠŸèƒ½")
    
    print("\nğŸ“Š æµ‹è¯•æ€»ç»“")
    print("=" * 30)
    if model_success:
        print("âœ… Stable Diffusion æ¨¡å‹å¯ç”¨")
        print("ğŸ‰ å¯ä»¥è¿›è¡Œå›¾åƒç”Ÿæˆ")
    else:
        print("âŒ Stable Diffusion æ¨¡å‹æš‚ä¸å¯ç”¨")
        print("ğŸ’¡ å»ºè®®ç»§ç»­ç­‰å¾…ä¸‹è½½å®Œæˆæˆ–æ£€æŸ¥ç½‘ç»œè¿æ¥")
        print("ğŸ”„ å½“å‰ä¸‹è½½å¯èƒ½ä»åœ¨è¿›è¡Œä¸­")

if __name__ == "__main__":
    main()